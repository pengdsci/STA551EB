% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
%
\documentclass[
]{book}
\usepackage{amsmath,amssymb}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math} % this also loads fontspec
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
\usepackage{lmodern}
\ifPDFTeX\else
  % xetex/luatex font selection
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{248,248,248}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.94,0.16,0.16}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\BuiltInTok}[1]{#1}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.64,0.00,0.00}{\textbf{#1}}}
\newcommand{\ExtensionTok}[1]{#1}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\ImportTok}[1]{#1}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{#1}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.81,0.36,0.00}{\textbf{#1}}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\RegionMarkerTok}[1]{#1}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.81,0.36,0.00}{\textbf{#1}}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{5}
\usepackage{booktabs}
\usepackage{amsthm}
\makeatletter
\def\thm@space@setup{%
  \thm@preskip=8pt plus 2pt minus 4pt
  \thm@postskip=\thm@preskip
}
\makeatother
\ifLuaTeX
  \usepackage{selnolig}  % disable illegal ligatures
\fi
\usepackage[]{natbib}
\bibliographystyle{apalike}
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same}
\hypersetup{
  pdftitle={STA551 E-Pack: Foundations of Data Science},
  pdfauthor={Cheng Peng},
  hidelinks,
  pdfcreator={LaTeX via pandoc}}

\title{STA551 E-Pack: Foundations of Data Science}
\author{Cheng Peng}
\date{West Chester University}

\begin{document}
\maketitle

{
\setcounter{tocdepth}{1}
\tableofcontents
}
\hypertarget{introduction}{%
\chapter{Introduction}\label{introduction}}

This \emph{E-coursepack} is a self-contained homegrown eBook that contains all topics covered in current STA551 at WCU.

Data science is a young discipline, but it is becoming more and more significant in both academia and industry. It is not a sub-field of any existing well-developed disciplines. Its foundation is built on theories and techniques and drawn from applied statistics and mathematics, machine learning, database and information technology, communication and related domain fields. Proficiency in programming with languages such as Python, SQL, R, etc. is essential to perform any data science tasks.

\textbf{Data science has an interdisciplinary nature, collectively uses existing tool from different fields, and creates new knowledge and tools to overcome new challenges }. There are debates on the definition of data science and validity of naming the emerging interdisciplinary field as \textbf{data science}.

\hypertarget{the-origin-of-data-science}{%
\section{The Origin of Data Science}\label{the-origin-of-data-science}}

The idea of expanding the horizon of classical statistics can be traced back to John Tukey's paper \emph{The future of data analysis} (1962), Although not mentioning the term \emph{data science}, Tukey urged statisticians to reduce their focus on statistical theory and engage with the entire data-analysis process: procedures for analyzing data, techniques for interpreting the results of such procedures, ways of planning the gathering of data to make its analysis easier, more precise or more accurate, and all the machinery and results of (mathematical) statistics which apply to analyzing data: These are foundational components of present new field of \textbf{data science}.

John Tukey was a chemist, topologist, educator, consultant, information scientist, researcher, statistician, data analyst, and corporate executive. At the end of World War II, he began a joint industrial and academic career at Bell Telephone Laboratories and at Princeton University (1946-1985) before he retired.

30 years after the Tukey's prophesied ``new science'', John Chambers, Tukey's colleague at Bell and a co-inventor of the S language (R and S-Plus are both implementations of S), published an article \emph{Greater or lesser statistics: a choice for future research} in which he thought statistics research was on crossroad and need to rethink whether the field needs to be expanded. Chambers came up with two contrast views of statistics: \texttt{Lesser\ Statistics} (the classical statistics built based on the probability theory) and \texttt{Greater\ Statistics} (the data-driven research which is the part of Tukey's unnamed \emph{New Science}). He thinks the if statisticians remain aloof, others will act. Statistics will lose; in addition, I believe science and society will lose also since statistician's mental attitude at its best provides qualities likely to be missing otherwise.

The term \textbf{data science} was first used in scientific community in 1968 by Peter Naur (an astronomer and computer scientist/professor), in an unpublished text in which he defined \textbf{data science} to be \emph{`The science of dealing with data, once they have been established, while the relation of the data to what they represent is delegated to other fields and sciences.'}.Apparently, Naur's ``data science'' was not meant a discipline.

Jeff Wu, an established statistics professor, first used the term \textbf{data science} in a lecture given in 1985 as an alternative of statistics. Later, in an inaugural lecture at the University Michigan in 1997, he called for statistics to be renamed data science and statisticians to be renamed data scientists.

William Cleveland, a statistics professor (at Purdue, 2004 - present) and industrial statistician and researcher (at Bell Labs, 1972-2003), defined \textbf{data science} as an expanded technical area in the field of statistics. According to Cleveland's proposal, the technical areas of \textbf{data science} consist of 6 components with corresponding suggested allocations apply to universities' curriculum.

\begin{itemize}
\item
  (25\%) \textbf{Multidisciplinary Investigations}: data analysis collaborations in a collection of subject matter areas.
\item
  (20\%) \textbf{Models and Methods for Data}: statistical models; methods of model building; methods of estimation and distribution based on probabilistic inference.
\item
  (15\%) \textbf{Computing with Data}: hardware systems; software systems; computational algorithms.
\item
  (15\%) \textbf{Pedagogy}: curriculum planning and approaches to teaching for elementary school, secondary school, college, graduate school, continuing education, and corporate training.
\item
  (5\%) \textbf{Tool Evaluation}: surveys of tools in use in practice, surveys of perceived needs for new tools, and studies of the processes for developing new tools.
\item
  (20\%) \textbf{Theory}: foundations of data science; general approaches to models and methods, computing with data, teaching, and tool evaluation; mathematical investigations of models and methods, computing with data, teaching, and evaluation.
\end{itemize}

Cleveland's \textbf{data science curriculum} proposal has a balanced components of classical statistical and mathematical foundations for data science, computational and software tools and technologies, and domain knowledge. This article as republished in 2014 in Wiley's journal \emph{Statistical Analysis and Data Mining} together with his another article in which he listed the 6 key technical areas of data science are those that have an impact on how a data analyst analyses data in practice:

\begin{itemize}
\tightlist
\item
  Statistical theory;
\item
  Statistical models;
\item
  Statistical and machine learning methods;
\item
  Algorithms for statistical and machine learning methods, and optimization;
\item
  Computational environments for data analysis;
\item
  Live analyses of data where results are judged by the findings, not the methodology and systems that where used.
\end{itemize}

By that time many universities and colleges including some key universities such as started offered undergraduate majors in data science and master's program in data sciences. These college degree programs in data science are usually houses in mathematics and statistics, computer science and engineering, or business and information systems departments. Some of the key universities expand their statistics departments and rename it as \emph{Statistics and Data Science}. All these movements in academia indicate that the Tukey's prophesied \texttt{new\ science} has been growing to the field of \textbf{data science}.

\hypertarget{the-coverage-of-the-first-data-science-course}{%
\section{The Coverage of the First Data Science Course}\label{the-coverage-of-the-first-data-science-course}}

The technical foundation of data science is built partly on applied statistics and mathematics, computer science and information system, and domain knowledge. Many universities started DS programs at graduate level degrees or certificates from different traditional disciplines using the existing resources and faculty expertise. Because of these limitations, different programs deliver slightly different curricula with different emphases but teach commonalities to build DS foundation.

For the first DS course, different programs also deliver the content in different ways depending on faculty expertise. Most of the current data science foundation courses are methodological emphasis.

We designed this course as a project-based course. We teach the \textbf{data science process} not the isolated models or algorithms.

\begin{figure}

{\centering \includegraphics[width=0.9\linewidth]{img01/w01-DSProcess} 

}

\caption{Data science process workflow}\label{fig:unnamed-chunk-3}
\end{figure}

We will guide students to complete an end-to-end data science project that involves question formulation, data extraction and transformation, identification of models and algorithms, model deployment and monitoring, and effective communication. The final DS consulting project will be conducted in a workplace like environment and with real-world problems and using data sources. Through doing the project, students will

\begin{itemize}
\tightlist
\item
  understand the big picture of DS and its capacity of solving practical problems;\\
\item
  sharpen their to skills in programming and information extraction and transforming;
\item
  enhance their understanding and effective utilization of models and algorithms;
\item
  improve their effective communications skills, particularly the information visualization;
\item
  learn how to extract \textbf{actionable} information and effectively implement the DS product.
\end{itemize}

\hypertarget{tentative-topics}{%
\section{Tentative Topics}\label{tentative-topics}}

We will cover the following major topics in this course.

\begin{itemize}
\tightlist
\item
  Data science process
\item
  Formulating analytic question from business questions
\item
  Data source identification, collection, and processing
\item
  EDA and Visualization in basic feature engineering
\item
  Statistical models for data science
\item
  Performance measures in predictive analytics
\item
  Training, testing and cross validation - data-driven methods
\item
  Survey of supervised machine learning algorithms and models
\item
  Unsupervised algorithms
\item
  Algorithm-based Feature engineering methods
\item
  Model / algorithm deployment and updating (including real-time predictive analytics)
\item
  Four-four workshop on a real-world data science consulting team project.
\end{itemize}

\hypertarget{data-science---a-big-picture}{%
\chapter{Data Science - A Big Picture}\label{data-science---a-big-picture}}

This note introduces the big picture of data science and tools and infrastructure needed for perform data science tasks. These include technical tools such as models and algorithms and data storage and extraction tools, etc.

\hypertarget{data-science-process}{%
\section{Data Science Process}\label{data-science-process}}

Although there are different versions of definition of data science, but the fundamental technical components have been well established for the field by the academic community. There is a consensus that the four pillars of data science are depicted in the following figure.

\begin{figure}

{\centering \includegraphics[width=0.9\linewidth]{img01/w01-4pillarsDS} 

}

\caption{Four pillars of data science.}\label{fig:unnamed-chunk-4}
\end{figure}

This is pretty much consistent with Tukey's \emph{new science} and Cleveland's proposal of college DS curriculum.

\hypertarget{what-is-data}{%
\subsection{What is Data?}\label{what-is-data}}

According to the definition in \emph{Wikipedia},

\begin{quote}
Data is a set of values of qualitative or quantitative variables; restated, pieces of data are individual pieces of information. Data is measured, collected, and reported, and analyzed, whereupon it can be visualized using graphs or images. Data as a general concept refers to the fact that some existing information or knowledge is represented or coded in some form suitable for better usage or processing.
\end{quote}

The data world of data science, \textbf{data is all recordable information} such as images, audios, and videos.

\hypertarget{data-storage-and-retrieval}{%
\subsection{Data Storage and Retrieval}\label{data-storage-and-retrieval}}

Once a business question is translated to an analytic questions, the next step is to identify data sources. This includes finding data sources in existing data warehouses in an organization or collecting data based on protocols or using external data sources. This stage involves data storage and retrieval. The following figure shows the simplest architecture of a data science professional will work with.

\begin{figure}

{\centering \includegraphics[width=0.9\linewidth]{img01/w01-StorageRetrieval} 

}

\caption{Basic architechute associated with data storage and retrieval.}\label{fig:unnamed-chunk-5}
\end{figure}

\hypertarget{different-da-roles-in-industry}{%
\subsection{Different DA Roles in Industry}\label{different-da-roles-in-industry}}

There are different types of data science roles in industry. Depending on the specific job role and organizations, Different job titles were used for different job role.

\textbf{Mathematical and Statistical analysis - statistician}: This role uses Statistics Methods such as
Descriptive statistics, EDA, regression analysis, time series and nonparametric analysis, etc. Sometimes, mathematical methods such mixture modeling and various optimization algorithms are also used when performing certain tasks.

\textbf{Machine Learning Algorithms - Machine Learning Engineer}: This role uses machine learning algorithms routinely in the work. Typical algorithms include \emph{classification algorithms} (Rule-based, Tree-based methods, Kernel method-KNN, Naive Bayes, Support vector machine-SVM, Neural networks, etc.), Clustering Algorithms (K-mean, Hierarchical clustering) and Anomaly detection algorithms, etc.

\textbf{Programming - Software Engineer}: Database query languages (SQL and NoSQL), Script languages (such as Python, R, Julia, SAS, etc.), Data wrangling and cleaning using different software tools, creating visuals via coding, Basic software development skills (writing APIs), Writing industry standard production code.

\textbf{Technology - IT and Data Engineer}: This role uses database technology, big data technology, data science platforms, software programs, collaboration tools, communication tools.

\textbf{Business Intelligence - Business analyst}: This role requires a business mind set, project management skills and communication skills.

\hypertarget{cloud-computing}{%
\subsection{Cloud Computing}\label{cloud-computing}}

\textbf{Cloud computing} is the on-demand availability of application and infrastructure computing resources such as servers, storage, databases, networking, software, analytics, and intelligence --- over the internet, without direct active management by the customer.

\begin{figure}

{\centering \includegraphics[width=0.9\linewidth]{img01/w01-CloudCompArchitecture} 

}

\caption{The architecture of cloud computing.}\label{fig:unnamed-chunk-6}
\end{figure}

The key characteristics of cloud computing are

\begin{itemize}
\item
  \textbf{On-demand self-services}: Users monitor and manage computing resources as necessary without the assistance of human administrators.
\item
  \textbf{Broad network access}: A wide range of hardware and established networks are usually used to deliver computing services.
\item
  \textbf{Rapid elasticity}: Resources for the computing services can be scaled up and down quickly as required.
\item
  \textbf{Resource pooling}: Ad hoc sharing of networks, servers, storage, apps, and services by several users and applications.
\item
  \textbf{Resilient computing}: computing services are ensured with high uptime and dependability.
\item
  \textbf{Flexible pricing structures}: including pay-per-use, subscription-based, and spot pricing.
\item
  \textbf{Security}: To safeguard the privacy of sensitive data and their users' data.
\item
  \textbf{Automation}: Cloud computing services feature a high level of automation with little to no manual input.
\end{itemize}

\hypertarget{from-business-questions-to-analytic-question}{%
\section{From Business Questions to Analytic Question}\label{from-business-questions-to-analytic-question}}

Business questions (if any) are usually ambiguous. Quite often in practice, there is no business question but a description of business issues or goals. As an example, let's assume a credit card company has been suffering fraud loss and decided to create a fraud detection team to reduce the current fraud loss.

\hypertarget{business-goal}{%
\subsection{Business Goal}\label{business-goal}}

\textbf{Business Goal}: \emph{The business goal of the executive team} is to cut the current fraud loss by half within one year. The business goal is clear and achievable based on the current fraud loss in the credit card industry. The question is that the business goal does not operational information for the analytic and data science team.

In order to appropriately formulate the corresponding analytic question, we need to understand the business logic of this business goal - how credit card transactions are processed?

\begin{figure}

{\centering \includegraphics[width=0.99\linewidth]{img01/w01-CreditCardProcessing} 

}

\caption{Credit card transaction processing workflow.}\label{fig:unnamed-chunk-7}
\end{figure}

The electronic credit card payment process may be difficult to understand at first. The diagram above illustrates the workflow of payment process.

\textbf{\color{red}1}. The \textbf{cardholder} swipes the card at the merchant POS in exchange for goods or services.

\textbf{\color{red}2}. The \textbf{merchant} sends a request for payment authorization to their \textbf{payment processor}.

\textbf{\color{red}3}. The \textbf{payment processor} submits transactions to the appropriate card association, eventually reaching the \textbf{issuing bank}.

\textbf{\color{red}4}. \textbf{Authorization} requests are made to the issuing bank, including parameters such as CVV (card verification value), expiration date, etc. to validate the incoming transaction request.

\textbf{\color{red}5}. The \textbf{issuing bank} approves or declines the request. The transaction can be declined in case of insufficient funds.

\textbf{\color{red}6}. The \textbf{issuing bank} then sends the approval (or denial) statement back along the line to the \textbf{card association}, \textbf{merchant bank}, and finally to the \textbf{merchant}.

\textbf{\color{blue}7}. \textbf{Merchants} send batches of authorized transactions to their \textbf{payment processor}.

\textbf{\color{blue}8}. The \textbf{payment processor} passes transaction details to the card associations that communicate the appropriate debits with the \textbf{issuing bank} in their network.

\textbf{\color{blue}9}. The \textbf{issuing bank} charges the cardholder's account for the amount of the transactions,

\textbf{\color{blue}10}. The \textbf{issuing bank} then transfers the appropriate amount for the transactions to the \textbf{merchant bank}, minus the interchange fees.

\textbf{\color{blue}11}. The \textbf{merchant bank} deposits funds into the merchant account.

If a fraudulent transaction was detected during the authorization process (the above steps 1- 6), there will be zero loss to the company. If the fraudulent transaction escaped from the authorization process, the company would lose at least one transaction, in general more if there is no way to identify them.

\hypertarget{analytic-question}{%
\subsection{Analytic Question}\label{analytic-question}}

\textbf{Analytic Question}: \emph{The high-level analytic question} is how to identify fraudulent transactions and stop them? - This high-level analytic question is not operational since we cannot be based on this question to identify data. We still need to drill down to gather some granular information in order to develop actionable plans for data collection and analysis planning.

\begin{itemize}
\item
  There are different types of fraud, different fraud has different patterns that require different pieces of information to identify.

  \begin{itemize}
  \tightlist
  \item
    Identity theft fraud?
  \item
    Lost card fraud?
  \item
    Application fraud?
  \item
    Account takeover fraud?
  \end{itemize}
\item
  Fraud interception at authorization process - proactive action for zero loss.
\item
  If the initial fraudulent transaction escaped from the fraud check during the authorization process, fraudsters would continue stealing, what analytic action we should take.
\end{itemize}

Based on the above sub-analytic questions, we make analytic plans and create a set of sub-tasks to identify data sources and models and algorithms to build a detection system. This means, we will not work with one model or algorithm, we need a set of potentially very different models and algorithms to tackle the seemingly simple business question.

\textbf{Types of Models/Algorithms and Required Data Sources}: We will use two of the above sub-analysis questions to illustrate the potential models/algorithms and relevant information needed to build identification systems.

\begin{itemize}
\item
  \textbf{Proactive Fraud Detection}: This detection is most favorable is it intercepts the fraud before the transaction is complete (i.e., the transaction will be declined). Some of the information such as geolocation of the merchant site, time and previous transaction site and time, card verification value (CVV), expiration, credit limit, etc. in the initial fraud check. This types of information can be used to develop business rules (expert system). Of cause, we can also build predictive models using this information.
\item
  \textbf{Application Fraud}: A fraudster can use fake information to apply for credit and then use the credit and discard the card. The first transaction is not easy to catch, what analytic models and algorithms can do is to detect the card and suspend it as soon as possible. The more effective way to stop application fraud is to build models to prevent potential application fraud during card application. The information needed to build the model can be extracted from the application and personal credit information and other relevant information from third-party.
\item
  \textbf{Identity Theft Fraud}: Identity fraud is common where criminals make purchases or obtain cash advances in the victim's name. This can be with an existing account, via theft of victim's physical credit card or victim's account numbers and PINs, or by opening new credit card accounts in victim's name. Because the fraudsters information is not available, we can use customers' proxy information on spending patterns to build models.
\end{itemize}

The types of algorithms and models for fraud identification will be discussed in more detail in the subsequent chapters.

\hypertarget{concepts-of-relational-databases-and-sql}{%
\section{Concepts of Relational Databases and SQL}\label{concepts-of-relational-databases-and-sql}}

Every data science professional should have basic knowledge of \textbf{databases} and \textbf{data warehouse} to accomplish all data science and other analytic projects.

\hypertarget{what-is-database}{%
\subsection{What is Database?}\label{what-is-database}}

A \textbf{database} is a collection of \emph{related data} set collected from the real world. It is designed to be built and populated with data for a specific task. It is also a building block of data solutions. A database

\begin{figure}

{\centering \includegraphics[width=0.99\linewidth]{img01/w01-relationalTables} 

}

\caption{Relational datatables in a relational database.}\label{fig:unnamed-chunk-8}
\end{figure}

\begin{itemize}
\tightlist
\item
  offers the security of data and its access;
\item
  offers a variety of techniques to store and retrieve data;
\item
  acts as an efficient handler to balance the requirement of multiple applications using the same data;
\item
  A DBMS offers integrity constraints to get a high level of protection to prevent access to prohibited data;
\item
  allows users to access concurrent data in such a way that only a single user can access the same data at a time.
\end{itemize}

\hypertarget{what-is-a-data-warehouse}{%
\subsection{What is a Data Warehouse?}\label{what-is-a-data-warehouse}}

A \textbf{data warehouse} is an information system which stores historical and commutative data from single or multiple sources. It is designed to \emph{analyze, report, and integrate transaction data} from different sources.

A basic structure of an organization's data warehouse is depicted in the following.

\begin{figure}

{\centering \includegraphics[width=0.9\linewidth]{img01/w01-simpleDWStructure} 

}

\caption{Basic structure of data warehouse.}\label{fig:unnamed-chunk-9}
\end{figure}

A data warehouse.

\begin{itemize}
\tightlist
\item
  helps business users to access critical data from some sources all in one place;
\item
  provides consistent information on various cross-functional activities;
\item
  helps you to integrate many sources of data to reduce stress on the production system.
\item
  helps you to reduce TAT (total turnaround time) for analysis and reporting.
\item
  helps users to access critical data from different sources in a single place so, it saves user's time of retrieving data information from multiple sources. You can also access data from the cloud easily.
\item
  allows you to store a large amount of historical data to analyze different periods and trends to make future predictions.
\item
  enhances the value of operational business applications and customer relationship management systems
\item
  separates analytics processing from transactional databases, improving the performance of both systems
\item
  provides more accurate reports.
\end{itemize}

\hypertarget{difference-between-database-and-data-warehouse}{%
\subsection{Difference between Database and Data Warehouse}\label{difference-between-database-and-data-warehouse}}

Both databases and data warehouses use relational data structures. But they are different. A data warehouse exists as a layer on top of another database or databases (usually OLTP databases). The following table lists the difference between two systems.

\begin{figure}

{\centering \includegraphics[width=0.99\linewidth]{img01/w01-databaseVSdatawarehouse} 

}

\caption{Comparison between database and data warehouse.}\label{fig:unnamed-chunk-10}
\end{figure}

\hypertarget{database-management-system-dbms}{%
\subsection{Database Management System (DBMS)}\label{database-management-system-dbms}}

\textbf{Database Management Systems (DBMS)} are software systems used to store, retrieve, and run queries on data. DBMS manages the data, the database engine, and the database schema, allowing for data to be manipulated or extracted by users and other programs. This helps provide data security, data integrity, concurrency, and uniform data administration procedures.

The following figure depicts the structure of a simple DBMS.

\begin{figure}

{\centering \includegraphics[width=0.99\linewidth]{img01/w01-DBNS-sturcture} 

}

\caption{Comparison between database and data warehouse.}\label{fig:unnamed-chunk-11}
\end{figure}

\textbf{Structured query language (SQL)} is a programming language for storing and processing information in a relational database. A relational database stores information in tabular form, with rows and columns representing different data attributes and the various relationships between the data values. We can use SQL statements to store, update, remove, search, and retrieve information from the database. We can also use SQL to maintain and optimize database performance.

\hypertarget{some-definitions-and-notations-of-relational-tables}{%
\subsection{Some Definitions and Notations of Relational Tables}\label{some-definitions-and-notations-of-relational-tables}}

\begin{itemize}
\tightlist
\item
  \textbf{Names Associated with Relational Tables}
\end{itemize}

\textbf{Name} -- each relation in a relational database should have a name that is unique among other relations.

\textbf{Attribute} -- each column in a relation.

\textbf{The degree of the relation} -- the total number of attributes for a relation.

\textbf{Tuple} -- each row in a relation.

\textbf{The cardinality of the relation} -- the total number of rows in a relation.

\begin{itemize}
\tightlist
\item
  \textbf{Operations among Relational Tables}
\end{itemize}

In a relational database, we can define several \textbf{operations} to create new relations out of the existing ones.

\begin{itemize}
\item
  \textbf{Basic operations} are

  \begin{itemize}
  \tightlist
  \item
    Unary Operation: Insert, Delete, Update, Select, Project,
  \item
    Binary Operation: Join, Union, Intersection, Difference.
  \end{itemize}
\end{itemize}

\hypertarget{running-sql-in-sas-and-r}{%
\chapter{Running SQL in SAS and R}\label{running-sql-in-sas-and-r}}

Although R and Python has different data wrangling libraries we can use to connect to databases for information extraction. Structural Query Language (SQL) is native to different DBMS. Some degree of proficiency in SQL crucial for every data professional.

This note use SAS PROX SQL and several R packages to run SQL in SAS and R.

\hypertarget{running-sql-in-sas}{%
\section{Running SQL in SAS}\label{running-sql-in-sas}}

The PROC SQL in SAS is powerful. We can run authentic SQL script with in SAS SQL. One can use free SAS Studio via SAS on-Demand. We will use three relational data tables in the following SAS code (URLs are also in the SAS code).

In order to make the code workable in SAS environment, we simply copy and paste the code to include it in this note. NOte that, SAS converted imported relational tables to SAS data sets. The resulting tables in PROC SAS are still SAS tables stored in the designated library. If a library reference is not given, the resulting SAS data set will be saved in the temporary library.

\hypertarget{loading-data}{%
\subsection{Loading Data}\label{loading-data}}

\begin{verbatim}
LIBNAME sql "/home/u50445699/STA551";

filename dat01 url 'https://pengdsci.github.io/datasets/AnimalSurvey/plots.csv';
filename dat02 url 'https://pengdsci.github.io/datasets/AnimalSurvey/species.csv';
filename dat03 url 'https://pengdsci.github.io/datasets/AnimalSurvey/surveys.csv';

*write a macro to load multiple data files;
%MACRO IMPORTCSV(in_csv, out_sas);
PROC IMPORT DATAFILE=&in_csv 
   DBMS=csv 
   OUT=sql.&out_sas 
   REPLACE;
RUN;
%MEND;

*call macros to load data files;
%IMPORTCSV(dat01, plots);
%IMPORTCSV(dat02, species);
%IMPORTCSV(dat03, survey);
\end{verbatim}

\hypertarget{basic-sql-syntax-and-clauses}{%
\subsection{Basic SQL Syntax and Clauses}\label{basic-sql-syntax-and-clauses}}

\#\#\#\#. First query with clause SELECT

(a). Create a table view

(b). Create a new table (SAS data set)

\begin{verbatim}
*Table view;   
PROC SQL;
/** The actual SQL code starts here **/
SELECT year
FROM sql.survey;
/***/
QUIT;


* create a new table;
PROC SQL;

CREATE TABLE sql.YMD AS
SELECT year, 
       month, 
       day
FROM sql.survey;

QUIT;

* create a new table: select all variables;
PROC SQL;

CREATE TABLE sql.survey_all AS
SELECT *
FROM sql.survey;

QUIT;
\end{verbatim}

\textbf{CAUTION}: SQL uses ``CREATE VIEW name\_of\_table AS'' to
create a view table that is saved in the database and
can be used to look at, filter, and even update information.

\hypertarget{select-unique-values}{%
\subsubsection{Select unique values}\label{select-unique-values}}

\begin{verbatim}
* one variable;
PROC SQL;
SELECT DISTINCT year
FROM sql.survey;
QUIT;

* two variable;
PROC SQL;
SELECT DISTINCT year, 
                species_id
FROM sql.survey;
QUIT;
\end{verbatim}

\hypertarget{calculated-values}{%
\subsubsection{Calculated values}\label{calculated-values}}

\begin{verbatim}
* Create a view;
PROC SQL;
SELECT year, 
       month, 
       day, 
       INPUT(weight, best.)/1000.0 AS wgt
FROM sql.survey;
QUIT;

* Create a table and define a new variable
  based on the calculated values;
PROC SQL;
CREATE TABLE sql.Add_new_var AS
SELECT year, 
       month, 
       day, 
       INPUT(weight, best.)/1000.0 AS wt_kilo
FROM sql.survey;
QUIT;


proc contents data = sql.surveys; run;

                         
PROC SQL;
SELECT plot_id, 
       species_id, 
       sex, 
       weight, 
       ROUND(INPUT(weight, best.)/ 1000.0, 0.01) /* ROUND() is a SAS function! */
FROM sql.survey;
QUIT;
\end{verbatim}

\hypertarget{filtering}{%
\subsubsection{Filtering}\label{filtering}}

\begin{verbatim}
* subsetting by filtering - WHERE statement;
* Single condition;
PROC SQL;
SELECT *
FROM sql.survey
WHERE species_id='DM';
QUIT;


* Multiple conditions: AND;
PROC SQL;
SELECT *
FROM sql.survey
WHERE (year >= 2000) AND (species_id = 'DM');
QUIT;

* Multiple conditions: OR;
PROC SQL;
SELECT *
FROM sql.survey
WHERE (species_id = 'DM') OR (species_id = 'DO') OR (species_id = 'DS');
QUIT;
\end{verbatim}

\hypertarget{special-keywords-simplify-where-statement}{%
\subsubsection{Special Keywords simplify WHERE statement}\label{special-keywords-simplify-where-statement}}

\begin{verbatim}
* use of keyword IN;
PROC SQL;
SELECT *
FROM sql.survey
WHERE (year >= 2000) AND (species_id IN ('DM', 'DO', 'DS'));
QUIT;
\end{verbatim}

\hypertarget{sorting-variables}{%
\subsubsection{Sorting Variables}\label{sorting-variables}}

\begin{verbatim}
* Ascending ordering - default;
PROC SQL;
SELECT *
FROM sql.species
ORDER BY taxa ASC;
QUIT;

* descending ordering;
PROC SQL;
SELECT *
FROM sql.species
ORDER BY taxa DESC;
QUIT;

* sorting multiple variables- nest sorting;
PROC SQL;
SELECT *
FROM sql.species
ORDER BY genus ASC, species ASC;
QUIT;
\end{verbatim}

\hypertarget{order-of-execution}{%
\subsubsection{Order of Execution}\label{order-of-execution}}

\begin{verbatim}
*Clauses are written in a fixed order: SELECT, FROM, WHERE, then ORDER BY. ;
PROC SQL;
SELECT genus, species
FROM sql.species
WHERE taxa = 'Bird'
ORDER BY species_id ASC;
QUIT;
\end{verbatim}

\hypertarget{summary-statistics-with-groups}{%
\subsubsection{Summary Statistics with groups}\label{summary-statistics-with-groups}}

\begin{verbatim}
*no group - Using the wildcard simply counts the number of records (rows);
PROC SQL;
SELECT COUNT(*)
FROM sql.survey;
QUIT;

* calculate the sum of a numerical variable;
PROC SQL;
SELECT COUNT(*) ,
       SUM(INPUT(weight, best.))
FROM sql.survey;
QUIT;

* calculate the sum of a numerical variable - adding names of summary statistics;
PROC SQL;
SELECT COUNT(*) AS sample_size,
       SUM(weight) AS total_weight
FROM sql.survey;
QUIT;

*summary statistics within subgroups - GROUP BY clause;
* This is equalent to a univeriable frequency table;
PROC SQL;
SELECT species_id, 
       COUNT(*)
FROM sql.survey
GROUP BY species_id;
QUIT;
\end{verbatim}

\hypertarget{having-clause-based-on-aggregated-variables}{%
\subsubsection{HAVING clause based on aggregated variables}\label{having-clause-based-on-aggregated-variables}}

\begin{verbatim}
* conditioning on species size;
PROC SQL;
SELECT species_id, 
       COUNT(species_id)
FROM sql.survey
GROUP BY species_id
HAVING COUNT(species_id) > 10;
QUIT;

* conditioning on species size - a better code;
PROC SQL;
SELECT species_id, 
       COUNT(species_id) AS species_size
FROM sql.survey
GROUP BY species_id
HAVING species_size > 10;
QUIT;
\end{verbatim}

\hypertarget{ordering-aggregated-results.}{%
\subsubsection{Ordering aggregated results.}\label{ordering-aggregated-results.}}

\begin{verbatim}
* sorting aggregated variables -- This DOES NOT work!
* Summary functions are restricted to the SELECT and HAVING clauses only;
PROC SQL;
SELECT species_id, 
       COUNT(*) 
FROM sql.survey
GROUP BY species_id
ORDER BY COUNT(species_id);
QUIT;

* sorting aggregated variables;
* use the new name in the ORDER BY clause;
PROC SQL;
SELECT species_id AS subtotal, 
       COUNT(*) 
FROM sql.survey
GROUP BY species_id
ORDER BY subtotal;
QUIT;
\end{verbatim}

\hypertarget{null---working-with-missing-values}{%
\subsubsection{NULL - Working with Missing values}\label{null---working-with-missing-values}}

\begin{verbatim}
* keyword IS;
PROC SQL;
SELECT *
FROM sql.survey
WHERE species_id IS NULL;
QUIT;

* keyword IS NOT;
PROC SQL;
SELECT *
FROM sql.survey
WHERE species_id IS NOT NULL;
QUIT;

* for non-missing values, we use IS/IS NOT or =/!=;
* CAUTION: "=" and "==" both work in SQL, However, "==" does NOT in SAS.;
PROC SQL;
SELECT SUM(INPUT(weight, best.)), 
       COUNT(*), 
       SUM(INPUT(weight, best.))/COUNT(*)
FROM sql.survey
WHERE species_id = 'PE';
QUIT;

* for non-missing values, we use IS/IS NOT or =/!=;
* CAUTION: "=" and "==" both work in SQL, However, "==" does NOT in SAS.;
* != works in SQL, but not in SAS. ^= works in SAS;
PROC SQL;
SELECT SUM(INPUT(weight, best.)), 
       COUNT(*), 
       SUM(INPUT(weight, best.))/COUNT(*)
FROM sql.survey
WHERE species_id ^= 'PE';
QUIT;
\end{verbatim}

\hypertarget{working-with-multiple-tables-join}{%
\subsubsection{Working with multiple tables: JOIN}\label{working-with-multiple-tables-join}}

\begin{verbatim}
*INNER JOIN;
*Need to use ALIAS to rename/name the data set since
 the files are stored in the SAS permanent libaray;
PROC SQL;
SELECT *
FROM sql.survey AS surveys
JOIN sql.species AS species
ON surveys.species_id = species.species_id;
QUIT;


* we can simply rename the tables as A and B using alias;
PROC SQL;
CREATE TABLE sql.INNERJOIN AS
SELECT *
FROM sql.survey AS A
JOIN sql.species AS B
ON A.species_id = B.species_id;
QUIT;

*left join;
PROC SQL;
CREATE TABLE sql.LEFTJOIN AS
SELECT *
FROM sql.survey AS A
LEFT JOIN sql.species AS B
ON A.species_id = B.species_id;
QUIT;

*right join;
PROC SQL;
CREATE TABLE sql.RIGHTJOIN AS
SELECT *
FROM sql.survey AS A
RIGHT JOIN sql.species AS B
ON A.species_id = B.species_id;
QUIT;

*full join;
PROC SQL;
CREATE TABLE sql.FULLTJOIN AS
SELECT *
FROM sql.survey AS A
FULL JOIN sql.species AS B
ON A.species_id = B.species_id;
QUIT;

* We can select some variables from individual tables
* then join the two sub tables.;
* CAUTION: We will NOT select any variables in species table 
*  to include in the new table;
PROC SQL;
SELECT A.species_id, 
       A.sex, 
       AVG(INPUT(a.hindfoot_length, best.)) as mean_foot_length  
FROM sql.survey  AS A
JOIN sql.species AS B
ON A.species_id=B.species_id 
WHERE taxa = 'Rodent' AND A.sex IS NOT NULL 
GROUP BY A.species_id, A.sex;
QUIT;
\end{verbatim}

\hypertarget{creatng-new-variables}{%
\subsubsection{Creatng New Variables}\label{creatng-new-variables}}

The following code defines new variables using string functions in SQL. This method is sometimes used to define composite keys

\begin{verbatim}
PROC SQL;
SELECT *, 
       species_id||'-'||sex AS newKey
FROM sql.surveys;
QUIT;
\end{verbatim}

\hypertarget{nest-queries}{%
\subsubsection{Nest Queries}\label{nest-queries}}

\begin{verbatim}
* nest queries;
* The SELECT ... FROM in the denominator is self-contained
* It is NOT affected by GROUP BY statement. The COUNT() function
* returns the total size of the data;
PROC SQL;
SELECT B.taxa, 
       100.0*COUNT(*)/(SELECT COUNT(*) FROM sql.survey)  AS Percentage
FROM sql.survey AS A
JOIN sql.species AS B
ON A.species_id = B.species_id 
GROUP BY taxa;
QUIT;
\end{verbatim}

\hypertarget{running-sas-in-r}{%
\section{Running SAS in R}\label{running-sas-in-r}}

To run SQL clauses in R, we need to use several R libraries (installed and loaded in the above R setup code chunk). There are different ways to run SQL query in R. We only introduce one methods that is close to the authentic SQL code that can be run a DBMS.

\hypertarget{connect-r-to-existing-database}{%
\subsection{Connect R to Existing Database}\label{connect-r-to-existing-database}}

If there is an existing database, the following code connects R to the database.

\begin{verbatim}
con <- DBI::dbConnect(drv = odbc::odbc(),
                      Driver = "driver_name",
                      Server = "server_url",
                      Database = "database_name",
                      user = "user", #optional
                      password = "password") #optional
\end{verbatim}

This section shows the three basic steps to run SQL in R using R Markdown starting with a set of relational tables.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Load relational data tables as usual to R.
\item
  Create a SQLite (relational) database that contain these relational table.
\item
  Create R code chunk and connect to the created database using Chunk options.
\end{enumerate}

\hypertarget{create-sqlite-database-with-r}{%
\subsection{Create SQLite Database with R}\label{create-sqlite-database-with-r}}

If modeling requires a data set that contains information from multiple relational data tables, we need to perform data management to aggregate the required information from different data tables. We can load the different data sets in different formats using appropriate R functions.

As an example, We use three ecological survey data sets to create a database.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#Load the sample data}
\NormalTok{plots }\OtherTok{\textless{}{-}} \FunctionTok{read.csv}\NormalTok{(}\StringTok{"https://pengdsci.github.io/datasets/AnimalSurvey/plots.csv"}\NormalTok{)}
\NormalTok{species }\OtherTok{\textless{}{-}} \FunctionTok{read.csv}\NormalTok{(}\StringTok{"https://pengdsci.github.io/datasets/AnimalSurvey/species.csv"}\NormalTok{)}
\NormalTok{surveys }\OtherTok{\textless{}{-}} \FunctionTok{read.csv}\NormalTok{(}\StringTok{"https://pengdsci.github.io/datasets/AnimalSurvey/surveys.csv"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Next, we create a SQLit database using several R libraries.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#Create database}
\NormalTok{con }\OtherTok{\textless{}{-}} \FunctionTok{dbConnect}\NormalTok{(}\AttributeTok{drv =} \FunctionTok{SQLite}\NormalTok{(),}
                 \AttributeTok{dbname =} \StringTok{":memory:"}\NormalTok{)}

\CommentTok{\#store sample data in database}
\FunctionTok{dbWriteTable}\NormalTok{(}\AttributeTok{conn =}\NormalTok{ con, }
             \AttributeTok{name =} \StringTok{"plots"}\NormalTok{,}
             \AttributeTok{value =}\NormalTok{ plots)}

\FunctionTok{dbWriteTable}\NormalTok{(}\AttributeTok{conn =}\NormalTok{ con, }
             \AttributeTok{name =} \StringTok{"species"}\NormalTok{,}
             \AttributeTok{value =}\NormalTok{ species)}

\FunctionTok{dbWriteTable}\NormalTok{(}\AttributeTok{conn =}\NormalTok{ con, }
             \AttributeTok{name =} \StringTok{"surveys"}\NormalTok{,}
             \AttributeTok{value =}\NormalTok{ surveys)}
 
\CommentTok{\#remove the local data from the environment}
\FunctionTok{rm}\NormalTok{(plots, species, surveys)}
\end{Highlighting}
\end{Shaded}

We can use table view function \texttt{tbl()} to explore the information of relational data tables in the database. Note that, we

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{tbl}\NormalTok{(}\AttributeTok{src =}\NormalTok{ con, }\CommentTok{\#the source if the database connection profile}
    \FunctionTok{c}\NormalTok{(}\StringTok{"surveys"}\NormalTok{)) }\CommentTok{\#the name of the table to preview}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # Source:   table<surveys> [?? x 10]
## # Database: sqlite 3.41.2 [:memory:]
##        X record_id month   day  year plot_id species_id sex   hindfoot_length weight
##    <int>     <int> <int> <int> <int>   <int> <chr>      <chr>           <int>  <int>
##  1     1         1     7    16  1977       2 NL         M                  32     NA
##  2     2         2     7    16  1977       3 NL         M                  33     NA
##  3     3         3     7    16  1977       2 DM         F                  37     NA
##  4     4         4     7    16  1977       7 DM         M                  36     NA
##  5     5         5     7    16  1977       3 DM         M                  35     NA
##  6     6         6     7    16  1977       1 PF         M                  14     NA
##  7     7         7     7    16  1977       2 PE         F                  NA     NA
##  8     8         8     7    16  1977       1 DM         M                  37     NA
##  9     9         9     7    16  1977       1 DM         F                  34     NA
## 10    10        10     7    16  1977       6 PF         F                  20     NA
## # i more rows
\end{verbatim}

\hypertarget{running-sql-queries-in-r-code-chunks}{%
\subsection{Running SQL Queries in R Code chunks}\label{running-sql-queries-in-r-code-chunks}}

To use SQL in RMarkdown, we need the following chunk options:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  sql
\item
  connection = ``database-name''
\item
  output.var = ``output-dataset-name''
\end{enumerate}

If we create a data view only, we simply ignore option \texttt{output.var\ =}

Following are few examples of SQL queries based on the animal survey data tales in the database.

\hypertarget{subsetting-and-duplicating-data}{%
\subsubsection{Subsetting and Duplicating Data}\label{subsetting-and-duplicating-data}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Extract year, month and day from \texttt{survey} table
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{SELECT} 
\NormalTok{  surveys.}\DataTypeTok{year}\NormalTok{, surveys.}\DataTypeTok{month}\NormalTok{, surveys.}\DataTypeTok{Day}
\KeywordTok{FROM} 
\NormalTok{ surveys }\CommentTok{/* pointer is not needed since it is in the database */}
\KeywordTok{WHERE}
\NormalTok{  surveys.species\_id }\KeywordTok{IN}\NormalTok{ (}\StringTok{\textquotesingle{}NL\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}DM\textquotesingle{}}\NormalTok{) }\KeywordTok{AND}
\NormalTok{  surveys.sex }\OperatorTok{=} \StringTok{\textquotesingle{}M\textquotesingle{}}
\end{Highlighting}
\end{Shaded}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  Duplicate a data and rename it
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{SELECT} 
\NormalTok{  surveys.}\OperatorTok{*}
\KeywordTok{FROM} 
\NormalTok{ surveys}
\end{Highlighting}
\end{Shaded}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{2}
\tightlist
\item
  Create a table view (i.e., no data set will be created and saved)
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{SELECT} 
\NormalTok{  surveys.}\DataTypeTok{year}\NormalTok{, surveys.}\DataTypeTok{month}\NormalTok{, surveys.}\DataTypeTok{Day}
\KeywordTok{FROM} 
\NormalTok{ surveys}
\KeywordTok{WHERE}
\NormalTok{  surveys.species\_id }\OperatorTok{=} \StringTok{\textquotesingle{}NL\textquotesingle{}} \KeywordTok{AND}
\NormalTok{  surveys.sex }\OperatorTok{=} \StringTok{\textquotesingle{}M\textquotesingle{}}
\end{Highlighting}
\end{Shaded}

\hypertarget{define-a-new-variable}{%
\subsubsection{Define A New Variable}\label{define-a-new-variable}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Define a new variable with simple arithmetic operations
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{SELECT} 
\NormalTok{    surveys.plot\_id, }
\NormalTok{    surveys.species\_id, }
\NormalTok{    surveys.sex, }
\NormalTok{    surveys.weight, }
\NormalTok{    surveys.weight}\OperatorTok{/}\DecValTok{100} \KeywordTok{AS}\NormalTok{ wt\_kilo  }\CommentTok{/*should not the pointer in front of the name of the new variable*/} 
\KeywordTok{FROM} 
\NormalTok{   surveys}
\end{Highlighting}
\end{Shaded}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  Define new variables using string functions in SQL
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{SELECT}\NormalTok{ surveys.}\OperatorTok{*}\NormalTok{, }
\NormalTok{       surveys.species\_id}\OperatorTok{||}\StringTok{\textquotesingle{}{-}\textquotesingle{}}\OperatorTok{||}\NormalTok{surveys.sex }\KeywordTok{AS}\NormalTok{ newKey}
\KeywordTok{FROM}\NormalTok{ surveys}
\end{Highlighting}
\end{Shaded}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{2}
\tightlist
\item
  Define new variables with aggregated information
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{SELECT}\NormalTok{ surveys.species\_id, }
       \FunctionTok{COUNT}\NormalTok{(surveys.species\_id) }\KeywordTok{AS}\NormalTok{ species\_ctr}
\KeywordTok{FROM}\NormalTok{ surveys}
\KeywordTok{GROUP} \KeywordTok{BY}\NormalTok{ surveys.species\_id}
\KeywordTok{HAVING}\NormalTok{ species\_ctr }\OperatorTok{\textgreater{}} \DecValTok{10}
\end{Highlighting}
\end{Shaded}

\hypertarget{sorting-variables-1}{%
\subsubsection{Sorting Variables}\label{sorting-variables-1}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Sort data based on the summarized statistics of a variable
\end{enumerate}

Summary functions are restricted to the SELECT and HAVING clauses only;

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{SELECT}\NormalTok{ surveys.species\_id}
\KeywordTok{FROM}\NormalTok{ surveys}
\KeywordTok{GROUP} \KeywordTok{BY}\NormalTok{ surveys.species\_id}
\KeywordTok{ORDER} \KeywordTok{BY} \FunctionTok{COUNT}\NormalTok{(surveys.species\_id);}
\end{Highlighting}
\end{Shaded}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  Sort data based on a new variable defined using summarized statistics of a variable.
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{/* create a table view*/}
\KeywordTok{SELECT}\NormalTok{ surveys.species\_id }\KeywordTok{AS}\NormalTok{ subtotal, }
       \FunctionTok{COUNT}\NormalTok{(}\OperatorTok{*}\NormalTok{) }
\KeywordTok{FROM}\NormalTok{ surveys}
\KeywordTok{GROUP} \KeywordTok{BY}\NormalTok{ surveys.species\_id}
\KeywordTok{ORDER} \KeywordTok{BY}\NormalTok{ subtotal;}
\end{Highlighting}
\end{Shaded}

\hypertarget{join-tables}{%
\subsubsection{Join Tables}\label{join-tables}}

This section introduce commonly used join operation to merge tables using the common key(s).

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Inner Join
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{SELECT} \OperatorTok{*}
\KeywordTok{FROM}\NormalTok{ surveys }\KeywordTok{AS}\NormalTok{ A}
\KeywordTok{JOIN}\NormalTok{ species }\KeywordTok{AS}\NormalTok{ B}
\KeywordTok{ON}\NormalTok{ A.species\_id }\OperatorTok{=}\NormalTok{ B.species\_id;}
\end{Highlighting}
\end{Shaded}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  Left Join
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{SELECT} \OperatorTok{*}
\KeywordTok{FROM}\NormalTok{ surveys }\KeywordTok{AS}\NormalTok{ A}
\KeywordTok{LEFT} \KeywordTok{JOIN}\NormalTok{ species }\KeywordTok{AS}\NormalTok{ B}
\KeywordTok{ON}\NormalTok{ A.species\_id }\OperatorTok{=}\NormalTok{ B.species\_id;}
\end{Highlighting}
\end{Shaded}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{2}
\tightlist
\item
  Right Join
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{SELECT} \OperatorTok{*}
\KeywordTok{FROM}\NormalTok{ surveys }\KeywordTok{AS}\NormalTok{ A}
\KeywordTok{RIGHT} \KeywordTok{JOIN}\NormalTok{ species }\KeywordTok{AS}\NormalTok{ B}
\KeywordTok{ON}\NormalTok{ A.species\_id }\OperatorTok{=}\NormalTok{ B.species\_id;}
\end{Highlighting}
\end{Shaded}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{3}
\tightlist
\item
  Full Join
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{SELECT} \OperatorTok{*}
\KeywordTok{FROM}\NormalTok{ surveys }\KeywordTok{AS}\NormalTok{ A}
\KeywordTok{FULL} \KeywordTok{JOIN}\NormalTok{ species }\KeywordTok{AS}\NormalTok{ B}
\KeywordTok{ON}\NormalTok{ A.species\_id }\OperatorTok{=}\NormalTok{ B.species\_id;}
\end{Highlighting}
\end{Shaded}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{4}
\tightlist
\item
  Join sub-tables
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{SELECT}\NormalTok{ A.species\_id, }
\NormalTok{       A.sex, }
       \FunctionTok{AVG}\NormalTok{(A.weight) }\KeywordTok{as}\NormalTok{ mean\_wgt  }
\KeywordTok{FROM}\NormalTok{ surveys }\KeywordTok{AS}\NormalTok{ A}
\KeywordTok{JOIN}\NormalTok{ species }\KeywordTok{AS}\NormalTok{ B}
\KeywordTok{ON}\NormalTok{ A.species\_id}\OperatorTok{=}\NormalTok{B.species\_id }
\KeywordTok{WHERE}\NormalTok{ taxa }\OperatorTok{=} \StringTok{\textquotesingle{}Rodent\textquotesingle{}} \KeywordTok{AND}\NormalTok{ A.sex }\KeywordTok{IS} \KeywordTok{NOT} \KeywordTok{NULL} 
\KeywordTok{GROUP} \KeywordTok{BY}\NormalTok{ A.species\_id, A.sex; }
\end{Highlighting}
\end{Shaded}

\hypertarget{subqueries}{%
\subsubsection{Subqueries}\label{subqueries}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Sample size
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{SELECT} \FunctionTok{COUNT}\NormalTok{(}\OperatorTok{*}\NormalTok{) }
\KeywordTok{FROM}\NormalTok{ surveys}
\end{Highlighting}
\end{Shaded}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  Relative Frequency with sub-query
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{SELECT}\NormalTok{ B.taxa, }
       \FloatTok{100.0}\OperatorTok{*}\FunctionTok{COUNT}\NormalTok{(}\OperatorTok{*}\NormalTok{)}\OperatorTok{/}\NormalTok{(}\KeywordTok{SELECT} \FunctionTok{COUNT}\NormalTok{(}\OperatorTok{*}\NormalTok{) }\KeywordTok{FROM}\NormalTok{ surveys)  }\KeywordTok{AS}\NormalTok{ Percentage}
\KeywordTok{FROM}\NormalTok{ surveys }\KeywordTok{AS}\NormalTok{ A}
\KeywordTok{JOIN}\NormalTok{ species }\KeywordTok{AS}\NormalTok{ B}
\KeywordTok{ON}\NormalTok{ A.species\_id }\OperatorTok{=}\NormalTok{ B.species\_id }
\KeywordTok{GROUP} \KeywordTok{BY}\NormalTok{ taxa;}
\end{Highlighting}
\end{Shaded}

\hfill\break

\hypertarget{exploratory-data-analysis-eda}{%
\chapter{Exploratory Data Analysis (EDA)}\label{exploratory-data-analysis-eda}}

The US National Institute of Standards and Technology (NIST) defines EDA as:

\begin{quote}
An approach/philosophy for data analysis that employs a variety of techniques (mostly graphical) to maximize insight into a data set, uncover underlying structure, extract important variables, detect outliers and anomalies, test underlying assumptions, develop parsimonious models, and determine optimal factor settings.
\end{quote}

The term EDA was coined by John Tukey in the 1970s. According to Tukey: ``It (EDA) is important to understand what you CAN DO before you learn to measure how WELL you seem to have DONE it\ldots{} Exploratory data analysis can never be the whole story, but nothing else can serve as the foundation stone -- as the first step.

Tukey clearly explains the purpose of EDA. In classical statistics, EDA has been primarily used to inspect the distribution of variables and observe patterns to make hypotheses and test (validating). To be more specific, EDA is for

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  inspecting the distribution of variables,
\item
  detecting (and/or removing) outliers,
\item
  examining the trend of variables
\item
  assess the associations between variables
\end{enumerate}

The general tools used for EDA in classical statistics are numerical descriptive statistics with basic graphics such as histograms and scatter plots, etc. A cautionary note about EDA is its descriptive nature. EDA is NOT an inferential method.

In Data Science, more EDA tools will be used for feature engineering in order to improve the performance of underlying models and algorithms. This note will systematically outline EDA tools and their applications in both classical statistics and data science.

\textbf{Working Data Set}

For convenience, we use a data set to illustrate the concepts and methods as we proceed. The data set can be found at \url{https://pengdsci.github.io/datasets/MelbourneHousingMarket/MelbourneHousing.csv}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{MelbourneHousePrice }\OtherTok{=} \FunctionTok{read.csv}\NormalTok{(}\StringTok{"https://pengdsci.github.io/datasets/MelbourneHousingMarket/MelbourneHousing.csv"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\hypertarget{tools-of-eda-and-applications}{%
\section{Tools of EDA and Applications}\label{tools-of-eda-and-applications}}

This section summarizes the tools of EDA and their applications in both classical statistics and data science.

\hypertarget{descriptive-statistics-approach}{%
\subsection{Descriptive Statistics Approach}\label{descriptive-statistics-approach}}

This approach uses tables and summarized statistics to uncover the pattern in the data. These patterns include the distribution of feature variables, the correlation between variables, missing values proportions, outliers, etc. Measures such five number summary, quartiles, IQR, and standardization of numerical variables.

R has a powerful function \texttt{summary()} that produces summarized descriptive statistics for every variable in the data set.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{summary}\NormalTok{(MelbourneHousePrice)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##     Suburb            Address              Rooms            Type               Price             Method         
##  Length:34857       Length:34857       Min.   : 1.000   Length:34857       Min.   :   85000   Length:34857      
##  Class :character   Class :character   1st Qu.: 2.000   Class :character   1st Qu.:  635000   Class :character  
##  Mode  :character   Mode  :character   Median : 3.000   Mode  :character   Median :  870000   Mode  :character  
##                                        Mean   : 3.031                      Mean   : 1050173                     
##                                        3rd Qu.: 4.000                      3rd Qu.: 1295000                     
##                                        Max.   :16.000                      Max.   :11200000                     
##                                                                            NA's   :7610                         
##    SellerG              Date             Distance           Postcode            Bedroom2         Bathroom     
##  Length:34857       Length:34857       Length:34857       Length:34857       Min.   : 0.000   Min.   : 0.000  
##  Class :character   Class :character   Class :character   Class :character   1st Qu.: 2.000   1st Qu.: 1.000  
##  Mode  :character   Mode  :character   Mode  :character   Mode  :character   Median : 3.000   Median : 2.000  
##                                                                              Mean   : 3.085   Mean   : 1.625  
##                                                                              3rd Qu.: 4.000   3rd Qu.: 2.000  
##                                                                              Max.   :30.000   Max.   :12.000  
##                                                                              NA's   :8217     NA's   :8226    
##       Car            Landsize         BuildingArea       YearBuilt     CouncilArea          Lattitude     
##  Min.   : 0.000   Min.   :     0.0   Min.   :    0.0   Min.   :1196    Length:34857       Min.   :-38.19  
##  1st Qu.: 1.000   1st Qu.:   224.0   1st Qu.:  102.0   1st Qu.:1940    Class :character   1st Qu.:-37.86  
##  Median : 2.000   Median :   521.0   Median :  136.0   Median :1970    Mode  :character   Median :-37.81  
##  Mean   : 1.729   Mean   :   593.6   Mean   :  160.3   Mean   :1965                       Mean   :-37.81  
##  3rd Qu.: 2.000   3rd Qu.:   670.0   3rd Qu.:  188.0   3rd Qu.:2000                       3rd Qu.:-37.75  
##  Max.   :26.000   Max.   :433014.0   Max.   :44515.0   Max.   :2106                       Max.   :-37.39  
##  NA's   :8728     NA's   :11810      NA's   :21115     NA's   :19306                      NA's   :7976    
##    Longtitude     Regionname        Propertycount     
##  Min.   :144.4   Length:34857       Length:34857      
##  1st Qu.:144.9   Class :character   Class :character  
##  Median :145.0   Mode  :character   Mode  :character  
##  Mean   :145.0                                        
##  3rd Qu.:145.1                                        
##  Max.   :145.5                                        
##  NA's   :7976
\end{verbatim}

We observe from the above summary tables that (1) most of the numeric variables have missing values; (2) The distribution of some of these numeric variables is skewed. We will discuss how to use these observations in feature engineering later.

\textbf{Remarks}: Handling missing values in classical statistics is crucial particularly when the sample size is mall. In data science, most of the projects are based on large data sets. Furthermore, the sample is usually not the ransom sample taken from a well-defined population. Therefore, imputing missing values is less important in many data science projects are less important (usually assume missing at random). Next, we delete all records with missing components.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{HousePrice }\OtherTok{=} \FunctionTok{na.omit}\NormalTok{(MelbourneHousePrice)}
\end{Highlighting}
\end{Shaded}

For a categorical variable, we can use a frequency table to display its distribution. For example,

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{table}\NormalTok{(HousePrice}\SpecialCharTok{$}\NormalTok{Bedroom2)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
##    0    1    2    3    4    5    6    7    8    9   10   12 
##    5  348 1965 3837 2183  487   50    5    2    3    1    1
\end{verbatim}

\hypertarget{graphical-approach}{%
\subsection{Graphical Approach}\label{graphical-approach}}

This approach uses basic statistical graphics to visualize the shape of the data to discover the distributional information of variables from the data and the potential relationships between variables. Graphics that are commonly used are histograms, box plots, serial plots, etc.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{par}\NormalTok{(}\AttributeTok{mfrow =} \FunctionTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{))}
\FunctionTok{hist}\NormalTok{(HousePrice}\SpecialCharTok{$}\NormalTok{Price, }\AttributeTok{main =} \StringTok{"Distribution of House Price"}\NormalTok{)}
\NormalTok{Suburb }\OtherTok{=} \FunctionTok{table}\NormalTok{(HousePrice}\SpecialCharTok{$}\NormalTok{Suburb)}
\FunctionTok{barplot}\NormalTok{(Suburb, }\AttributeTok{main=}\StringTok{"Suburb Information"}\NormalTok{)}
\NormalTok{Type }\OtherTok{=} \FunctionTok{table}\NormalTok{(HousePrice}\SpecialCharTok{$}\NormalTok{Type)}
\FunctionTok{pie}\NormalTok{(Type, }\AttributeTok{main=}\StringTok{"Distribution of House Type"}\NormalTok{)}
\NormalTok{den }\OtherTok{\textless{}{-}} \FunctionTok{density}\NormalTok{(HousePrice}\SpecialCharTok{$}\NormalTok{Price)}
\FunctionTok{plot}\NormalTok{(den, }\AttributeTok{frame =} \ConstantTok{FALSE}\NormalTok{, }\AttributeTok{col =} \StringTok{"blue"}\NormalTok{,}\AttributeTok{main =} \StringTok{"Density House Prices"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{STA551EB_files/figure-latex/unnamed-chunk-35-1} \end{center}

We can see We will discuss how to use these observed patterns in feature engineering to yield better results later.

\hypertarget{algorithm-based-method}{%
\subsection{Algorithm-based Method}\label{algorithm-based-method}}

If there exist some groups (data points clustered), we may want to assign an ID for each group to reduce the overall variations of the data. Including this cluster ID will improve the performance of the underlying model. The clustering algorithm uses a lot of computing resources. As an example, we use the well-known iris data set based on the 4 numerical variables.

\begin{verbatim}
iris0 = iris[,-5]
res.hc <- eclust(iris0, "hclust", k = 3)
#fviz_dend(res.hc)              # dendrogam
\end{verbatim}

\begin{verbatim}
 fviz_cluster(res.hc)       # scatter plot
\end{verbatim}

\begin{verbatim}
NewIris = iris
NewIris$Cluster = res.hc$cluster
\end{verbatim}

\hypertarget{visual-techniques-of-eda}{%
\section{Visual Techniques of EDA}\label{visual-techniques-of-eda}}

EDA is particularly effective for low-dimensional data. The following discussion will be based on the number and type of variables.

\hypertarget{univariate-eda}{%
\subsection{Univariate EDA}\label{univariate-eda}}

\begin{itemize}
\tightlist
\item
  Numerical Variable
\end{itemize}

The commonly used visual techniques for numerical variables are histograms, density curves, box-plots, serial plots, etc.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{par}\NormalTok{(}\AttributeTok{mfrow =} \FunctionTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{))}
\FunctionTok{hist}\NormalTok{(HousePrice}\SpecialCharTok{$}\NormalTok{Price, }\AttributeTok{xlab =} \StringTok{"Price"}\NormalTok{, }\AttributeTok{ylab =} \StringTok{"count"}\NormalTok{, }\AttributeTok{main =} \StringTok{"House Prices"}\NormalTok{)}
\NormalTok{den}\OtherTok{=}\FunctionTok{density}\NormalTok{(HousePrice}\SpecialCharTok{$}\NormalTok{Price)}
\FunctionTok{plot}\NormalTok{(den, }\AttributeTok{xlab =} \StringTok{"Price"}\NormalTok{, }\AttributeTok{ylab =} \StringTok{"count"}\NormalTok{, }\AttributeTok{main =} \StringTok{"House Prices"}\NormalTok{)}
\DocumentationTok{\#\#}
\FunctionTok{boxplot}\NormalTok{(HousePrice}\SpecialCharTok{$}\NormalTok{Price, }\AttributeTok{xlab =} \StringTok{"Price"}\NormalTok{, }\AttributeTok{ylab =} \StringTok{"count"}\NormalTok{, }\AttributeTok{main =} \StringTok{"House Prices"}\NormalTok{)}
\DocumentationTok{\#\#}
\CommentTok{\# Get the data points in the form of an R vector.}
\NormalTok{rainfall }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{799}\NormalTok{,}\FloatTok{1174.8}\NormalTok{,}\FloatTok{865.1}\NormalTok{,}\FloatTok{1334.6}\NormalTok{,}\FloatTok{635.4}\NormalTok{,}\FloatTok{918.5}\NormalTok{,}\FloatTok{685.5}\NormalTok{,}\FloatTok{998.6}\NormalTok{,}\FloatTok{784.2}\NormalTok{,}\DecValTok{985}\NormalTok{,}\FloatTok{882.8}\NormalTok{,}\DecValTok{1071}\NormalTok{)}
\CommentTok{\# Convert it to a time series object.}
\NormalTok{rainfall.timeseries }\OtherTok{\textless{}{-}} \FunctionTok{ts}\NormalTok{(rainfall,}\AttributeTok{start =} \FunctionTok{c}\NormalTok{(}\DecValTok{2012}\NormalTok{,}\DecValTok{1}\NormalTok{),}\AttributeTok{frequency =} \DecValTok{12}\NormalTok{)}
\CommentTok{\# Plot a graph of the time series.}
\FunctionTok{plot}\NormalTok{(rainfall.timeseries, }\AttributeTok{ylab =} \StringTok{"Rainfall"}\NormalTok{, }\AttributeTok{main =} \StringTok{"Rainfall Trend"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{STA551EB_files/figure-latex/unnamed-chunk-36-1} \end{center}

One can also create a frequency table to look at the distribution.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{options}\NormalTok{(}\AttributeTok{digits =} \DecValTok{7}\NormalTok{)}
\NormalTok{bound }\OtherTok{=} \FunctionTok{round}\NormalTok{(}\FunctionTok{seq}\NormalTok{(}\DecValTok{100000}\NormalTok{,}\DecValTok{9000000}\NormalTok{, }\AttributeTok{length=}\DecValTok{15}\NormalTok{),}\DecValTok{1}\NormalTok{)}
\FunctionTok{as.data.frame}\NormalTok{(}\FunctionTok{table}\NormalTok{(}\FunctionTok{cut}\NormalTok{(HousePrice}\SpecialCharTok{$}\NormalTok{Price, }\AttributeTok{breaks=}\NormalTok{bound)))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##                   Var1 Freq
## 1     (1e+05,7.36e+05] 3101
## 2  (7.36e+05,1.37e+06] 3669
## 3  (1.37e+06,2.01e+06] 1374
## 4  (2.01e+06,2.64e+06]  442
## 5  (2.64e+06,3.28e+06]  172
## 6  (3.28e+06,3.91e+06]   77
## 7  (3.91e+06,4.55e+06]   24
## 8  (4.55e+06,5.19e+06]   11
## 9  (5.19e+06,5.82e+06]    8
## 10 (5.82e+06,6.46e+06]    5
## 11 (6.46e+06,7.09e+06]    1
## 12 (7.09e+06,7.73e+06]    1
## 13 (7.73e+06,8.36e+06]    1
## 14    (8.36e+06,9e+06]    1
\end{verbatim}

The above frequency table gives a similar distribution as shown in the histogram and the density curve.

\begin{itemize}
\tightlist
\item
  Categorical Variable
\end{itemize}

The commonly used visual techniques for numerical variables are bar charts and pie charts.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{par}\NormalTok{(}\AttributeTok{mfrow=}\FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{))}
\NormalTok{freq.tbl }\OtherTok{=} \FunctionTok{table}\NormalTok{(HousePrice}\SpecialCharTok{$}\NormalTok{Bedroom2)}
\FunctionTok{barplot}\NormalTok{(freq.tbl, }\AttributeTok{xlab=}\StringTok{"Number of Bedrooms"}\NormalTok{, }\AttributeTok{ylab =} \StringTok{"Counts"}\NormalTok{, }\AttributeTok{main=}\StringTok{"Distribution of number of Bedrooms"}\NormalTok{)}
\FunctionTok{pie}\NormalTok{(freq.tbl, }\AttributeTok{xlab=}\StringTok{"Number of Bedrooms"}\NormalTok{, }\AttributeTok{ylab =} \StringTok{"Counts"}\NormalTok{, }\AttributeTok{main=}\StringTok{"Distribution of number of Bedrooms"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{STA551EB_files/figure-latex/unnamed-chunk-38-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{kable}\NormalTok{(}\FunctionTok{as.data.frame}\NormalTok{(}\FunctionTok{table}\NormalTok{(HousePrice}\SpecialCharTok{$}\NormalTok{Bedroom2)))}
\end{Highlighting}
\end{Shaded}

\begin{tabular}{l|r}
\hline
Var1 & Freq\\
\hline
0 & 5\\
\hline
1 & 348\\
\hline
2 & 1965\\
\hline
3 & 3837\\
\hline
4 & 2183\\
\hline
5 & 487\\
\hline
6 & 50\\
\hline
7 & 5\\
\hline
8 & 2\\
\hline
9 & 3\\
\hline
10 & 1\\
\hline
12 & 1\\
\hline
\end{tabular}

\hypertarget{two-variables}{%
\subsection{Two Variables}\label{two-variables}}

Three different cases involve two variables.

\hypertarget{two-numeric-variables}{%
\subsubsection{Two Numeric Variables}\label{two-numeric-variables}}

In the case of two numeric variables, the key interest is to look at the potential association between the two. The most effective visual representation is a scatter plot.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{plot}\NormalTok{(HousePrice}\SpecialCharTok{$}\NormalTok{Price, HousePrice}\SpecialCharTok{$}\NormalTok{BuildingArea)}
\end{Highlighting}
\end{Shaded}

\includegraphics{STA551EB_files/figure-latex/unnamed-chunk-40-1.pdf}

The above scatter plot indicates a linear trend between the house price and the building area.

\hypertarget{two-categorical-variable}{%
\subsubsection{Two Categorical Variable}\label{two-categorical-variable}}

For given two categorical variables, we may be interested in exploring whether they are independent. The two-way table and be used to visualize the potential relationship between the two categorical variables.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ftable}\NormalTok{(HousePrice}\SpecialCharTok{$}\NormalTok{Bathroom, HousePrice}\SpecialCharTok{$}\NormalTok{Bedroom2)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##       0    1    2    3    4    5    6    7    8    9   10   12
##                                                               
## 1     2  345 1667 1921  255   10    1    0    0    0    0    0
## 2     3    2  295 1788 1495  203   13    1    0    0    0    0
## 3     0    1    3  124  394  206   26    2    1    0    0    0
## 4     0    0    0    4   36   45    9    2    1    0    0    0
## 5     0    0    0    0    3   22    0    0    0    0    0    1
## 6     0    0    0    0    0    1    1    0    0    1    0    0
## 7     0    0    0    0    0    0    0    0    0    1    0    0
## 8     0    0    0    0    0    0    0    0    0    1    0    0
## 9     0    0    0    0    0    0    0    0    0    0    1    0
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{chisq.test}\NormalTok{(HousePrice}\SpecialCharTok{$}\NormalTok{Bathroom, HousePrice}\SpecialCharTok{$}\NormalTok{Bedroom2)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Warning in chisq.test(HousePrice$Bathroom, HousePrice$Bedroom2): Chi-squared
\end{verbatim}

\begin{verbatim}
## 
##  Pearson's Chi-squared test
## 
## data:  HousePrice$Bathroom and HousePrice$Bedroom2
## X-squared = 20915, df = 88, p-value < 2.2e-16
\end{verbatim}

Note that \(\chi^2\) test is sometimes used in EDA.

\hypertarget{one-numeric-variable-and-one-categorical-variable}{%
\subsubsection{One Numeric Variable and One Categorical Variable}\label{one-numeric-variable-and-one-categorical-variable}}

From the modeling point of view, there are two different ways to assess the relationship between a categorical variable and a numerical variable. For example, a ridge plot can be used to visualize the distribution of house prices across the Type of houses.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(HousePrice, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x=}\NormalTok{Price}\SpecialCharTok{/}\DecValTok{10000}\NormalTok{,}\AttributeTok{y=}\NormalTok{Type,}\AttributeTok{fill=}\NormalTok{Type))}\SpecialCharTok{+}
  \FunctionTok{geom\_density\_ridges\_gradient}\NormalTok{(}\AttributeTok{scale =} \DecValTok{4}\NormalTok{) }\SpecialCharTok{+} \FunctionTok{theme\_ridges}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{scale\_y\_discrete}\NormalTok{(}\AttributeTok{expand =} \FunctionTok{c}\NormalTok{(}\FloatTok{0.01}\NormalTok{, }\DecValTok{0}\NormalTok{)) }\SpecialCharTok{+}
  \FunctionTok{scale\_x\_continuous}\NormalTok{(}\AttributeTok{expand =} \FunctionTok{c}\NormalTok{(}\FloatTok{0.08}\NormalTok{, }\DecValTok{0}\NormalTok{)) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Prices"}\NormalTok{,}\AttributeTok{y =} \StringTok{"Type"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{ggtitle}\NormalTok{(}\StringTok{"Density estimation of prices given Type"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme}\NormalTok{(}\AttributeTok{plot.title =} \FunctionTok{element\_text}\NormalTok{(}\AttributeTok{hjust =} \FloatTok{0.5}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Picking joint bandwidth of 6.65
\end{verbatim}

\includegraphics{STA551EB_files/figure-latex/unnamed-chunk-43-1.pdf}

The ridge plot is a visual representation of ANOVA.

\hfill\break

\hypertarget{three-or-more-variables}{%
\subsection{Three or More Variables}\label{three-or-more-variables}}

Visualizing the relationship between three or more variables can be challenging. One has to use visual design elements such as line, shape, negative/white space, volume, value, color, and texture --- to represent the values of variables.

\hfill\break

\hypertarget{use-of-colors-movement-and-point-size}{%
\subsubsection{Use of Colors, Movement, and Point-size}\label{use-of-colors-movement-and-point-size}}

In the following example, color, movement, and point size represent \texttt{continent}, \texttt{time}, and \texttt{population\ size}, respectively. Therefore, it represents the complete relationship of 5 variables.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{knitr}\SpecialCharTok{::}\FunctionTok{include\_url}\NormalTok{(}\StringTok{"https://flo.uri.sh/visualisation/11871870/embed?auto=1"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\hfill\break

\hypertarget{pairewised-relationship-between-variables}{%
\subsubsection{Pairewised Relationship Between Variables}\label{pairewised-relationship-between-variables}}

The pair-wise scatter plot \textbf{numerical variables} is the most commonly used in practice. We use the \texttt{Iris} data set as an example to show the pair-wise plot in the following.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{pairs}\NormalTok{(iris[,}\SpecialCharTok{{-}}\DecValTok{5}\NormalTok{])}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{STA551EB_files/figure-latex/unnamed-chunk-45-1} \end{center}

The above enhanced pair-wise scatter plot provides a pair-wise comparison between the four numerical variables across the three species (categorical variable).

\hfill\break

\hypertarget{roles-of-visualization-in-eda}{%
\section{Roles of Visualization in EDA}\label{roles-of-visualization-in-eda}}

\textbf{Information visualization} displays information in a visual format that makes insights easier to understand for human users. The information in data is usually visualized in a pictorial or graphical form such as charts, graphs, lists, maps, and comprehensive dashboards that combine these multiple formats.

\hypertarget{data-visualization}{%
\subsection{Data Visualization}\label{data-visualization}}

The primary objective of \textbf{data visualization} is to clearly communicate what the data says, help explain trends and statistics, and show patterns that would otherwise be impossible to see. \textbf{Data visualization} is used to make consuming, interpreting, and understanding data as simple as possible, and to make it easier to derive insights from data.

\hypertarget{visual-analytics}{%
\subsection{Visual Analytics}\label{visual-analytics}}

Visual analytics is an emerging area in analytics. It is more than visualization. \textbf{Interactive} exploration and \textbf{automatic} visual manipulation play a central role in visual analytics.

Visual analytics does the **heavy lifting*''** with data, by using a variety of tools and technologies --- machine learning and mathematical algorithms, statistical models, cutting-edge software programs, etc --- to identify and reveal patterns and trends. It prepares the data for the process of data visualization, thereby enabling users to examine data, understand what it means, interpret the patterns it highlights, and help them find meaning and gain useful insights from complex data sets.

In other words, using visual analytic methods and techniques can enhance (data) visualization and improve the performance of analysis and modeling. Interactive visualization technology enables the exploration of data via the manipulation of chart images, with the color, brightness, size, shape, and motion of visual objects representing aspects of the data set being analyzed. The following is such an example (\url{https://vizhub.healthdata.org/cod/}).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{knitr}\SpecialCharTok{::}\FunctionTok{include\_app}\NormalTok{(}\StringTok{"https://vizhub.healthdata.org/cod/"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\hypertarget{eda-for-feature-engineering}{%
\chapter{EDA for Feature Engineering}\label{eda-for-feature-engineering}}

Recall the following workflow of the data science project.

\begin{center}\includegraphics[width=0.8\linewidth]{img03/DS-project-workflow} \end{center}

We have introduced the techniques and tools for exploratory data analysis. This note provides an example to show how to use EDA based on the data set used in the example.

\hypertarget{description-of-data}{%
\section{Description of Data}\label{description-of-data}}

A population of women who were at least 21 years old, of Pima Indian heritage, and living near Phoenix, Arizona, was tested for diabetes according to World Health Organization criteria. The data were collected by the US National Institute of Diabetes and Digestive and Kidney. The objective of the data set is to diagnostically predict whether or not a patient has diabetes, based on certain diagnostic measurements included in the data set. Several constraints were placed on the selection of these instances from a larger database.

There are two versions of the data available in the public domain. This case study uses the version that contains the missing values. The total number of records in this data set is 768. The data set consists of 9 variables including the response variable with the name \texttt{diabetes}. Predictor variables include the number of pregnancies the patient has had, their BMI, insulin level, age, and so on. A detailed description of the variables is given below

\texttt{pregnant}: Number of times pregnant

\texttt{glucose}: Plasma glucose concentration 2 hours in an oral glucose tolerance test

\texttt{pressure}: Diastolic blood pressure (mm Hg)

\texttt{triceps}: Triceps skin fold thickness (mm)

\texttt{insulin}: 2-Hour serum insulin (mu U/ml)

\texttt{mass}: Body mass index (weight in kg/(height in m)\^{}2)

\texttt{pedigree}: Diabetes pedigree function

\texttt{age}: Age (years)

\texttt{diabetes}: outcome class variable (`neg' or `pos')

A copy of this publicly available data is stored at \url{https://pengdsci.github.io/datasets/PimaDiabetes/PimaIndiansDiabetes2.csv}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{PimaDiabetes }\OtherTok{=} \FunctionTok{read.csv}\NormalTok{(}\StringTok{"https://pengdsci.github.io/datasets/PimaDiabetes/PimaIndiansDiabetes2.csv"}\NormalTok{)[, }\SpecialCharTok{{-}}\DecValTok{1}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\hfill\break

\hypertarget{eda-for-feature-engineering-1}{%
\section{EDA for Feature Engineering}\label{eda-for-feature-engineering-1}}

We have introduced several techniques and methods in an earlier note. We will use some EDA techniques and methods as needed for this data and subsequent modeling.

We first scan the entire data set and determine the EDA tools to use for feature engineering.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{summary}\NormalTok{(PimaDiabetes)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##     pregnant         glucose         pressure         triceps         insulin            mass          pedigree     
##  Min.   : 0.000   Min.   : 44.0   Min.   : 24.00   Min.   : 7.00   Min.   : 14.00   Min.   :18.20   Min.   :0.0780  
##  1st Qu.: 1.000   1st Qu.: 99.0   1st Qu.: 64.00   1st Qu.:22.00   1st Qu.: 76.25   1st Qu.:27.50   1st Qu.:0.2437  
##  Median : 3.000   Median :117.0   Median : 72.00   Median :29.00   Median :125.00   Median :32.30   Median :0.3725  
##  Mean   : 3.845   Mean   :121.7   Mean   : 72.41   Mean   :29.15   Mean   :155.55   Mean   :32.46   Mean   :0.4719  
##  3rd Qu.: 6.000   3rd Qu.:141.0   3rd Qu.: 80.00   3rd Qu.:36.00   3rd Qu.:190.00   3rd Qu.:36.60   3rd Qu.:0.6262  
##  Max.   :17.000   Max.   :199.0   Max.   :122.00   Max.   :99.00   Max.   :846.00   Max.   :67.10   Max.   :2.4200  
##                   NA's   :5       NA's   :35       NA's   :227     NA's   :374      NA's   :11                      
##       age          diabetes        
##  Min.   :21.00   Length:768        
##  1st Qu.:24.00   Class :character  
##  Median :29.00   Mode  :character  
##  Mean   :33.24                     
##  3rd Qu.:41.00                     
##  Max.   :81.00                     
## 
\end{verbatim}

\hypertarget{missing-values---imputation}{%
\subsection{Missing Values - Imputation}\label{missing-values---imputation}}

The above summary table indicates that feature variables \texttt{glucose}, \texttt{pressure}, \texttt{triceps}, \texttt{insulin}, and \texttt{mass} have missing values. \texttt{insulin} has nearly 50\% missing values. \texttt{triceps} has 227 missing values. The other three variables have a very low percentage of missing values.

\hypertarget{missing-value-vs-no-value}{%
\subsubsection{Missing Value vs No Value}\label{missing-value-vs-no-value}}

Missing value means that the information is available but not collected while no value means that the value does not exist.

Replacing the missing values with proxy values (imputation) or deleting them from the data are the ways of handling missing values. Most software programs \emph{automatically delete all records with missing components} from the data before modeling if the missing value issue is not handled.

\texttt{no-value} should be never imputed in the data processing. The ways of handling \texttt{no\ value} is to either drop all records with \texttt{no\ value} components or the feature variables that have \texttt{no\ values}. The former will change the study population and the latter will lead to a loss of information.

\hfill\break

\hypertarget{glucose-tolerance-glucose-vs-2-hour-serum-insulin-insulin}{%
\subsubsection{\texorpdfstring{Glucose Tolerance (\texttt{glucose}) vs 2-Hour Serum Insulin (\texttt{insulin})}{Glucose Tolerance (glucose) vs 2-Hour Serum Insulin (insulin)}}\label{glucose-tolerance-glucose-vs-2-hour-serum-insulin-insulin}}

Both \texttt{fasting\ insulin\ test} and \texttt{glucose\ tolerance\ test} are used in diabetes diagnosis, therefore, variables \texttt{glucose} and \texttt{insulin} are correlated. Since nearly 50\% of patients did not do the insulin test. Therefore, we can use \texttt{glucose} to impute the missing values in \texttt{insulin}. We first look at the correlation between the two variables based on the complete data.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{par}\NormalTok{(}\AttributeTok{mfrow =} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{))}
\FunctionTok{plot}\NormalTok{(PimaDiabetes}\SpecialCharTok{$}\NormalTok{glucose, PimaDiabetes}\SpecialCharTok{$}\NormalTok{insulin, }\AttributeTok{xlab =} \StringTok{"Glucose Level"}\NormalTok{, }\AttributeTok{ylab =} \StringTok{"Insulin Level"}\NormalTok{)}
\FunctionTok{plot}\NormalTok{(PimaDiabetes}\SpecialCharTok{$}\NormalTok{glucose, }\FunctionTok{log}\NormalTok{(PimaDiabetes}\SpecialCharTok{$}\NormalTok{insulin), }\AttributeTok{xlab =} \StringTok{"Glucose Level"}\NormalTok{, }\AttributeTok{ylab =} \StringTok{"log Insulin Level"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{STA551EB_files/figure-latex/unnamed-chunk-51-1} \end{center}

The scatter plot shows that the logarithm of the insulin level and the glucose level are highly linearly correlated. We can use this relationship to impute the logarithm of insulin level based on the no-missing glucose level. Since we will use this data set to build predictive models, the logarithm of insulin will be used directly in the subsequent models and algorithms.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{impute.insulin.lm }\OtherTok{=} \FunctionTok{lm}\NormalTok{(}\FunctionTok{log}\NormalTok{(insulin[}\SpecialCharTok{{-}}\DecValTok{446}\NormalTok{]) }\SpecialCharTok{\textasciitilde{}}\NormalTok{ glucose[}\SpecialCharTok{{-}}\DecValTok{446}\NormalTok{], }\AttributeTok{data =}\NormalTok{ PimaDiabetes)}
\FunctionTok{summary}\NormalTok{(impute.insulin.lm)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## Call:
## lm(formula = log(insulin[-446]) ~ glucose[-446], data = PimaDiabetes)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -1.87469 -0.31478 -0.02521  0.33826  1.55711 
## 
## Coefficients:
##                Estimate Std. Error t value Pr(>|t|)    
## (Intercept)   3.0588150  0.1095012   27.93   <2e-16 ***
## glucose[-446] 0.0143630  0.0008673   16.56   <2e-16 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 0.5269 on 390 degrees of freedom
##   (375)
## Multiple R-squared:  0.4129, Adjusted R-squared:  0.4114 
## F-statistic: 274.3 on 1 and 390 DF,  p-value: < 2.2e-16
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{par}\NormalTok{(}\AttributeTok{mfrow =} \FunctionTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{))}
\FunctionTok{plot}\NormalTok{(impute.insulin.lm)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{STA551EB_files/figure-latex/unnamed-chunk-53-1} \end{center}

Next, we use the following linear regression to impute the missing values in \texttt{insulin}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{glucose }\OtherTok{=}\NormalTok{ PimaDiabetes}\SpecialCharTok{$}\NormalTok{glucose}
\NormalTok{impute.log.insulin }\OtherTok{=} \FunctionTok{log}\NormalTok{(PimaDiabetes}\SpecialCharTok{$}\NormalTok{insulin)}
\NormalTok{n}\OtherTok{=}\FunctionTok{length}\NormalTok{(impute.log.insulin)}
\ControlFlowTok{for}\NormalTok{ (i }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\NormalTok{n)\{}
  \ControlFlowTok{if}\NormalTok{ (}\FunctionTok{is.na}\NormalTok{(impute.log.insulin[i]) }\SpecialCharTok{==} \ConstantTok{TRUE} \SpecialCharTok{\&\&} \FunctionTok{is.na}\NormalTok{(glucose[i]) }\SpecialCharTok{==} \ConstantTok{FALSE}\NormalTok{) impute.log.insulin[i] }\OtherTok{=} \FunctionTok{sum}\NormalTok{(}\FunctionTok{coef}\NormalTok{(impute.insulin.lm)}\SpecialCharTok{*}\FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,glucose[i])) }\SpecialCharTok{+} \FunctionTok{sample}\NormalTok{(}\FunctionTok{resid}\NormalTok{(impute.insulin.lm),}\DecValTok{1}\NormalTok{)}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

Visual comparison of the distribution between the original \texttt{insulin} and the imputed \texttt{insulin}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{den.orig.insulin }\OtherTok{=} \FunctionTok{density}\NormalTok{(}\FunctionTok{na.omit}\NormalTok{(}\FunctionTok{log}\NormalTok{(PimaDiabetes}\SpecialCharTok{$}\NormalTok{insulin)))}
\NormalTok{den.impute.insulin }\OtherTok{=} \FunctionTok{density}\NormalTok{(}\FunctionTok{na.omit}\NormalTok{(impute.log.insulin))}
\FunctionTok{plot}\NormalTok{(den.impute.insulin, }\AttributeTok{xlab=}\StringTok{"log insulin"}\NormalTok{, }\AttributeTok{main =} \StringTok{"density curve of log of original and imputed insulin"}\NormalTok{,}
     \AttributeTok{col =} \StringTok{"red"}\NormalTok{)}
\FunctionTok{lines}\NormalTok{(den.orig.insulin, }\AttributeTok{col =} \StringTok{"blue"}\NormalTok{)}
\FunctionTok{legend}\NormalTok{(}\StringTok{"topright"}\NormalTok{, }\FunctionTok{c}\NormalTok{(}\StringTok{"original log insulin"}\NormalTok{, }\StringTok{"inputed log insulin"}\NormalTok{), }\AttributeTok{col =} \FunctionTok{c}\NormalTok{(}\StringTok{"blue"}\NormalTok{, }\StringTok{"red"}\NormalTok{), }\AttributeTok{lty =} \FunctionTok{rep}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{), }\AttributeTok{cex =} \FloatTok{0.9}\NormalTok{, }\AttributeTok{bty =} \StringTok{"n"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{STA551EB_files/figure-latex/unnamed-chunk-55-1} \end{center}

The above density curves show that distributions of the imputed log insulin and original log insulin levels are close to each other.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{PimaDiabetes}\SpecialCharTok{$}\NormalTok{impute.log.insulin }\OtherTok{=}\NormalTok{ impute.log.insulin}
\end{Highlighting}
\end{Shaded}

\hfill\break

\hypertarget{triceps-skinfold-thickness-triceps-vs-body-mass-index-mass}{%
\subsubsection{\texorpdfstring{Triceps Skinfold Thickness (\texttt{triceps}) vs Body Mass Index (\texttt{mass})}{Triceps Skinfold Thickness (triceps) vs Body Mass Index (mass)}}\label{triceps-skinfold-thickness-triceps-vs-body-mass-index-mass}}

Clinical variables \texttt{triceps} (triceps skin-fold thickness, see the following figure to see how it is measured) and \texttt{mass} (body mass index) are clinically correlated.

\begin{figure}

{\centering \includegraphics[width=0.3\linewidth]{img03/tricepts} 

}

\caption{Figure 1. Measurement of triceps skinfold using a Lange caliper. With the subject's arm in a relaxed position, the skinfold is picked with thumb and index fingers at the midpoint of the arm.}\label{fig:unnamed-chunk-57}
\end{figure}

\texttt{triceps} has nearly 30\% missing values and \texttt{mass} has a few missing values. We can use the information in \texttt{mass} to impute the missing values in triceps - single imputation with a linear regression model. To perform imputation,

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  fit a linear regression model with \texttt{triceps} being the response and \texttt{mass} as the predictor.
\item
  use the above fitted regression to predict \texttt{triceps} on non-missing \texttt{mass}.
\item
  impute the missing value in \texttt{triceps} with the \emph{predicted} \texttt{triceps}.
\end{enumerate}

Note that in R, records with missing components will be automatically deleted in the modeling process.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{impute.lm }\OtherTok{=} \FunctionTok{lm}\NormalTok{(triceps[}\SpecialCharTok{{-}}\DecValTok{580}\NormalTok{] }\SpecialCharTok{\textasciitilde{}}\NormalTok{ mass[}\SpecialCharTok{{-}}\DecValTok{580}\NormalTok{], }\AttributeTok{data =}\NormalTok{ PimaDiabetes)}
\FunctionTok{summary}\NormalTok{(impute.lm)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## Call:
## lm(formula = triceps[-580] ~ mass[-580], data = PimaDiabetes)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -19.6294  -4.9225  -0.4862   5.0930  21.3029 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(>|t|)    
## (Intercept) -3.34070    1.56901  -2.129   0.0337 *  
## mass[-580]   0.98464    0.04669  21.087   <2e-16 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 7.442 on 536 degrees of freedom
##   (229)
## Multiple R-squared:  0.4534, Adjusted R-squared:  0.4524 
## F-statistic: 444.7 on 1 and 536 DF,  p-value: < 2.2e-16
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{par}\NormalTok{(}\AttributeTok{mfrow =} \FunctionTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{))}
\FunctionTok{plot}\NormalTok{(impute.lm)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{STA551EB_files/figure-latex/unnamed-chunk-59-1} \end{center}

The above fitted regression line will be used to \textbf{impute} the missing values in \texttt{triceps} in the following.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{mass }\OtherTok{=}\NormalTok{ PimaDiabetes}\SpecialCharTok{$}\NormalTok{mass}
\NormalTok{impute.triceps }\OtherTok{=}\NormalTok{ PimaDiabetes}\SpecialCharTok{$}\NormalTok{triceps}
\NormalTok{n}\OtherTok{=}\FunctionTok{length}\NormalTok{(impute.triceps)}
\ControlFlowTok{for}\NormalTok{ (i }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\NormalTok{n)\{}
  \ControlFlowTok{if}\NormalTok{ (}\FunctionTok{is.na}\NormalTok{(impute.triceps[i]) }\SpecialCharTok{==} \ConstantTok{TRUE} \SpecialCharTok{\&\&} \FunctionTok{is.na}\NormalTok{(mass[i]) }\SpecialCharTok{==} \ConstantTok{FALSE}\NormalTok{) impute.triceps[i] }\OtherTok{=} \FunctionTok{sum}\NormalTok{(}\FunctionTok{coef}\NormalTok{(impute.lm)}\SpecialCharTok{*}\FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,mass[i])) }\SpecialCharTok{+} \FunctionTok{sample}\NormalTok{(}\FunctionTok{resid}\NormalTok{(impute.lm),}\DecValTok{1}\NormalTok{)}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{PimaDiabetes}\SpecialCharTok{$}\NormalTok{impute.triceps }\OtherTok{=}\NormalTok{ impute.triceps}
\end{Highlighting}
\end{Shaded}

Next, we check whether the missing values in \texttt{triceps} were appropriately imputed.

We look at the density curves of \texttt{impute.triceps} and the original \texttt{triceps} to see the performance of the imputation and whether a discretization is needed.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{den.tri }\OtherTok{=} \FunctionTok{density}\NormalTok{(}\FunctionTok{na.omit}\NormalTok{(PimaDiabetes}\SpecialCharTok{$}\NormalTok{triceps))}
\NormalTok{den.imput.tri }\OtherTok{=} \FunctionTok{density}\NormalTok{(}\FunctionTok{na.omit}\NormalTok{(PimaDiabetes}\SpecialCharTok{$}\NormalTok{impute.triceps))}
\FunctionTok{plot}\NormalTok{(den.imput.tri, }\AttributeTok{col =} \StringTok{"red"}\NormalTok{, }\AttributeTok{xlab =} \StringTok{"triceps"}\NormalTok{, }\AttributeTok{ylab =} \StringTok{"density"}\NormalTok{, }\AttributeTok{main =} \StringTok{"original triceps vs imputed triceps"}\NormalTok{)}
\FunctionTok{lines}\NormalTok{(den.tri, }\AttributeTok{col =} \StringTok{"blue"}\NormalTok{)}
\FunctionTok{legend}\NormalTok{(}\StringTok{"topright"}\NormalTok{, }\FunctionTok{c}\NormalTok{(}\StringTok{"Inputed Triceps"}\NormalTok{, }\StringTok{"Original Triceps"}\NormalTok{), }\AttributeTok{col=}\FunctionTok{c}\NormalTok{(}\StringTok{"red"}\NormalTok{, }\StringTok{"blue"}\NormalTok{), }\AttributeTok{lty =}\FunctionTok{rep}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{), }\AttributeTok{bty=}\StringTok{"n"}\NormalTok{, }\AttributeTok{cex =} \FloatTok{0.8}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{STA551EB_files/figure-latex/unnamed-chunk-62-1} \end{center}

The above density curves indicate that

\begin{itemize}
\item
  the two distributions are almost identical, and
\item
  both distributions are almost symmetric (except for one outlier in the original data).
\end{itemize}

Since the missing values in \texttt{triceps} were appropriately imputed, we next add the \texttt{impute.triceps} to the original data frame and drop the original \texttt{triceps}.

To close this imputation section, we re-organize the data set by dropping the original variables and keeping the imputed variables. At the same time, we also delete all records with missing components.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{PimaDiabetes }\OtherTok{=} \FunctionTok{na.omit}\NormalTok{(PimaDiabetes[, }\FunctionTok{c}\NormalTok{(}\StringTok{"pregnant"}\NormalTok{, }\StringTok{"glucose"}\NormalTok{, }\StringTok{"pressure"}\NormalTok{, }\StringTok{"mass"}\NormalTok{ , }\StringTok{"pedigree"}\NormalTok{ , }\StringTok{"age"}\NormalTok{ ,}\StringTok{"impute.log.insulin"}\NormalTok{, }\StringTok{"impute.triceps"}\NormalTok{, }\StringTok{"diabetes"}\NormalTok{)])}
\end{Highlighting}
\end{Shaded}

\hfill\break

\hypertarget{assess-distributions}{%
\subsection{Assess Distributions}\label{assess-distributions}}

This subsection focuses on the potential discretization of continuous variables and grouping sparse categories of category variables based on their distribution.

\hypertarget{discretizing-continuous-variables}{%
\subsubsection{Discretizing Continuous Variables}\label{discretizing-continuous-variables}}

The above pairwise scatter plot shows that \texttt{glucose}, \texttt{pressure}(diastolic reading), and \texttt{age} are usually discretized in the clinical study. We will use the clinical standard and practices to discretize these variables

According to Medical News Today (\url{https://www.medicalnewstoday.com/articles/a1c-chart-diabetes-numbers\#a-1-c-chart}). The glucose levels \(< 117\), \([117, 137]\), \(>137\) indicate normal, pre-diabetes, and diabetes.

According to National Diabetes Statistics Report (
\url{https://www.cdc.gov/media/releases/2017/p0718-diabetes-report.html\#:~:text=Rates\%20of\%20diagnosed\%20diabetes\%20increased,older\%2C\%2025\%20percent\%20had\%20diabetes}), rates of diagnosed diabetes increased with age. Among adults ages 18-44, 4 percent had diabetes. Among those ages 45-64 years, 17 percent had diabetes. And among those ages 65 years and older, 25 percent had diabetes.

According to The Seventh Report of the Joint National Committee on Prevention, Detection, Evaluation, and Treatment of High Blood Pressure (2003 Guideline, \url{https://www.nhlbi.nih.gov/files/docs/guidelines/express.pdf}), The normal diastolic pressure is less than 80 mm Hg, at risk diastolic reading is between 80 mm Hg and 90 mm Hg, abnormal (hypertension) diastolic reading is higher than 90 mm Hg.

We will discretize these three variables for future models and algorithms.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{PimaDiabetes}\SpecialCharTok{$}\NormalTok{grp.glucose }\OtherTok{\textless{}{-}} \FunctionTok{ifelse}\NormalTok{(PimaDiabetes}\SpecialCharTok{$}\NormalTok{glucose }\SpecialCharTok{\textless{}} \DecValTok{117}\NormalTok{, }\StringTok{\textquotesingle{}(0, 117)\textquotesingle{}}\NormalTok{,}
               \FunctionTok{ifelse}\NormalTok{(PimaDiabetes}\SpecialCharTok{$}\NormalTok{glucose }\SpecialCharTok{\textgreater{}} \DecValTok{137}\NormalTok{, }\StringTok{\textquotesingle{}\textgreater{} 137\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}[117,137]\textquotesingle{}}\NormalTok{))}

\NormalTok{PimaDiabetes}\SpecialCharTok{$}\NormalTok{grp.diastolic }\OtherTok{\textless{}{-}} \FunctionTok{ifelse}\NormalTok{(PimaDiabetes}\SpecialCharTok{$}\NormalTok{pressure }\SpecialCharTok{\textless{}} \DecValTok{80}\NormalTok{, }\StringTok{\textquotesingle{}(0, 80)\textquotesingle{}}\NormalTok{,}
               \FunctionTok{ifelse}\NormalTok{(PimaDiabetes}\SpecialCharTok{$}\NormalTok{pressure }\SpecialCharTok{\textgreater{}} \DecValTok{90}\NormalTok{, }\StringTok{\textquotesingle{}\textgreater{} 90\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}[80,90]\textquotesingle{}}\NormalTok{))}

\NormalTok{PimaDiabetes}\SpecialCharTok{$}\NormalTok{grp.age }\OtherTok{\textless{}{-}} \FunctionTok{ifelse}\NormalTok{(PimaDiabetes}\SpecialCharTok{$}\NormalTok{age }\SpecialCharTok{\textless{}=} \DecValTok{44}\NormalTok{, }\StringTok{\textquotesingle{}[21, 44]\textquotesingle{}}\NormalTok{,}
               \FunctionTok{ifelse}\NormalTok{(PimaDiabetes}\SpecialCharTok{$}\NormalTok{age }\SpecialCharTok{\textgreater{}=} \DecValTok{65}\NormalTok{, }\StringTok{\textquotesingle{}65+\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}[45, 64]\textquotesingle{}}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\hfill\break

\hypertarget{grouping-sparse-categories}{%
\subsubsection{Grouping Sparse Categories}\label{grouping-sparse-categories}}

The number of times pregnant \texttt{pregnant} is a discrete numerical variable. We could also consider it as an ordinal categorical variable.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pregnancy }\OtherTok{=} \FunctionTok{table}\NormalTok{(PimaDiabetes}\SpecialCharTok{$}\NormalTok{pregnant)}
\FunctionTok{barplot}\NormalTok{(pregnancy, }\AttributeTok{main =} \StringTok{"Distribution of pregnacies"}\NormalTok{, }\AttributeTok{xlab =} \StringTok{"Pregnant Times"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{STA551EB_files/figure-latex/unnamed-chunk-65-1} \end{center}

There are a few sparse categories in the variable, we decide to group this variable in the following:
0, 1, 2, 3-4, 5-7, 8+.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{PimaDiabetes}\SpecialCharTok{$}\NormalTok{grp.pregnant }\OtherTok{\textless{}{-}} \FunctionTok{ifelse}\NormalTok{(PimaDiabetes}\SpecialCharTok{$}\NormalTok{pregnant }\SpecialCharTok{==} \DecValTok{0}\NormalTok{, }\StringTok{\textquotesingle{}0\textquotesingle{}}\NormalTok{,}
                                \FunctionTok{ifelse}\NormalTok{(PimaDiabetes}\SpecialCharTok{$}\NormalTok{pregnant }\SpecialCharTok{==} \DecValTok{1}\NormalTok{, }\StringTok{\textquotesingle{}1\textquotesingle{}}\NormalTok{, }
                                   \FunctionTok{ifelse}\NormalTok{(PimaDiabetes}\SpecialCharTok{$}\NormalTok{pregnant }\SpecialCharTok{==} \DecValTok{2}\NormalTok{, }\StringTok{\textquotesingle{}2\textquotesingle{}}\NormalTok{,}
                                      \FunctionTok{ifelse}\NormalTok{((PimaDiabetes}\SpecialCharTok{$}\NormalTok{pregnant }\SpecialCharTok{==} \DecValTok{3} \SpecialCharTok{|}\NormalTok{ PimaDiabetes}\SpecialCharTok{$}\NormalTok{pregnant }\SpecialCharTok{==} \DecValTok{4}\NormalTok{), }\StringTok{\textquotesingle{}3{-}4\textquotesingle{}}\NormalTok{,}
                                         \FunctionTok{ifelse}\NormalTok{((PimaDiabetes}\SpecialCharTok{$}\NormalTok{pregnant }\SpecialCharTok{==} \DecValTok{5} \SpecialCharTok{|}\NormalTok{ PimaDiabetes}\SpecialCharTok{$}\NormalTok{pregnant }\SpecialCharTok{==} \DecValTok{6} \SpecialCharTok{|}\NormalTok{ PimaDiabetes}\SpecialCharTok{$}\NormalTok{pregnant }\SpecialCharTok{==} \DecValTok{7}\NormalTok{), }\StringTok{\textquotesingle{}5{-}7\textquotesingle{}}\NormalTok{, }
                                            \FunctionTok{ifelse}\NormalTok{(PimaDiabetes}\SpecialCharTok{$}\NormalTok{pregnant }\SpecialCharTok{\textgreater{}=} \DecValTok{8}\NormalTok{, }\StringTok{\textquotesingle{}8+\textquotesingle{}}\NormalTok{, }\StringTok{"NA"}\NormalTok{))))))}
\end{Highlighting}
\end{Shaded}

As the last step, we only keep those variables to used in the subsequent modeling.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{var.names }\OtherTok{=} \FunctionTok{c}\NormalTok{(}\StringTok{"mass"}\NormalTok{, }\StringTok{"pedigree"}\NormalTok{, }\StringTok{"impute.log.insulin"}\NormalTok{, }\StringTok{"impute.triceps"}\NormalTok{, }\StringTok{"grp.glucose"}\NormalTok{, }\StringTok{"grp.diastolic"}\NormalTok{, }\StringTok{"grp.age"}\NormalTok{, }\StringTok{"grp.pregnant"}\NormalTok{, }\StringTok{"diabetes"}\NormalTok{) }
\NormalTok{PimaDiabetes }\OtherTok{=}\NormalTok{ PimaDiabetes[, var.names]}
\end{Highlighting}
\end{Shaded}

\hypertarget{save-analytic-dataset}{%
\subsubsection{Save Analytic Dataset}\label{save-analytic-dataset}}

The final analytic data should be saved as a permanent data for the subsequent analysis and modeling and upload the saved data set to HitHub data repository for an easy access in the future.

\begin{verbatim}
write.csv(PimaDiabetes, "C:\\Users\\75CPENG\\OneDrive - West Chester University of PA\\Desktop\\cpeng\\WCU-Teaching\\2023Summer\\STA551\\w03\\AnalyticPimaDiabetes.csv")
\end{verbatim}

The above csv file is also uploaded to the GitHub data repository at \url{https://pengdsci.github.io/STA551/w03/AnalyticPimaDiabetes.csv}.

\hypertarget{pairwise-association}{%
\subsection{Pairwise Association}\label{pairwise-association}}

Depending on the types of variables, there are three different combinations of two variables: two numeric variables, two categorical variables, one numeric variable, and one categorical variable. We will assess the association between two variables graphically based on the above three scenarios.

\hypertarget{two-numeric-variables-1}{%
\subsubsection{Two Numeric Variables}\label{two-numeric-variables-1}}

The best visual tool for assessing pairwise linear association between two numeric variables is a pair-wise scatter plot. The pair-wise scatter plot and its variants are available in several different R packages.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggpairs}\NormalTok{(PimaDiabetes,                  }\CommentTok{\# Data frame}
        \AttributeTok{columns =} \DecValTok{1}\SpecialCharTok{:}\DecValTok{4}\NormalTok{,         }\CommentTok{\# Columns}
        \FunctionTok{aes}\NormalTok{(}\AttributeTok{color =}\NormalTok{ diabetes,  }\CommentTok{\# Color by group (cat. variable)}
            \AttributeTok{alpha =} \FloatTok{0.5}\NormalTok{))      }\CommentTok{\# Transparency}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## plot: [1,1] [====>----------------------------------------------------------------------------------] 6% est: 0s
## plot: [1,2] [==========>----------------------------------------------------------------------------] 12% est: 0s
## plot: [1,3] [===============>-----------------------------------------------------------------------] 19% est: 0s
## plot: [1,4] [=====================>-----------------------------------------------------------------] 25% est: 0s
## plot: [2,1] [==========================>------------------------------------------------------------] 31% est: 0s
## plot: [2,2] [================================>------------------------------------------------------] 38% est: 0s
## plot: [2,3] [=====================================>-------------------------------------------------] 44% est: 0s
## plot: [2,4] [===========================================>-------------------------------------------] 50% est: 0s
## plot: [3,1] [================================================>--------------------------------------] 56% est: 0s
## plot: [3,2] [=====================================================>---------------------------------] 62% est: 0s
## plot: [3,3] [===========================================================>---------------------------] 69% est: 0s
## plot: [3,4] [================================================================>----------------------] 75% est: 0s
## plot: [4,1] [======================================================================>----------------] 81% est: 0s
## plot: [4,2] [===========================================================================>-----------] 88% est: 0s
## plot: [4,3] [=================================================================================>-----] 94% est: 0s
## plot: [4,4] [=======================================================================================]100% est: 0s
\end{verbatim}

\begin{center}\includegraphics{STA551EB_files/figure-latex/unnamed-chunk-68-1} \end{center}

The off-diagonal plots and numbers indicate the correlation between the pair-wise numeric variables. As expected, triceps and mass are significantly correlated. Other paired variables have weak correlations.

The main diagonal stacked density curves show the potential difference in the distribution of the underlying numeric variable in diabetes and diabetes-free groups. This means that the stacked density curves show the relation between numeric and categorical variables. These stacked density curves are not completely overlapped indicating somewhat correlation between each of these numeric variables and the binary response variable.

Because of the above interpretation between numeric variables and the binary response variable, we will not open a new subsection to illustrate the relationship between a numeric variable and a categorical variable.

\hfill\break

\hypertarget{two-categorical-variables}{%
\subsubsection{Two Categorical Variables}\label{two-categorical-variables}}

Mosaic plots are convenient to show whether two categorical variables are dependent. In EDA, we are primarily interested in whether the response (binary in this case) is independent of categorical variables. Those categorical variables that are independent of the response variable should be excluded in any of the subsequent models and algorithms.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{par}\NormalTok{(}\AttributeTok{mfrow =} \FunctionTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{))}
\FunctionTok{mosaicplot}\NormalTok{(grp.glucose }\SpecialCharTok{\textasciitilde{}}\NormalTok{ diabetes, }\AttributeTok{data=}\NormalTok{PimaDiabetes,}\AttributeTok{col=}\FunctionTok{c}\NormalTok{(}\StringTok{"Blue"}\NormalTok{,}\StringTok{"Red"}\NormalTok{), }\AttributeTok{main=}\StringTok{"glucose vs diabetes"}\NormalTok{)}
\FunctionTok{mosaicplot}\NormalTok{(grp.diastolic }\SpecialCharTok{\textasciitilde{}}\NormalTok{ diabetes, }\AttributeTok{data=}\NormalTok{PimaDiabetes,}\AttributeTok{col=}\FunctionTok{c}\NormalTok{(}\StringTok{"Blue"}\NormalTok{,}\StringTok{"Red"}\NormalTok{), }\AttributeTok{main=}\StringTok{"diastolic vs diabetes"}\NormalTok{)}
\FunctionTok{mosaicplot}\NormalTok{(grp.age }\SpecialCharTok{\textasciitilde{}}\NormalTok{ diabetes, }\AttributeTok{data=}\NormalTok{PimaDiabetes,}\AttributeTok{col=}\FunctionTok{c}\NormalTok{(}\StringTok{"Blue"}\NormalTok{,}\StringTok{"Red"}\NormalTok{), }\AttributeTok{main=}\StringTok{"age vs diabetes"}\NormalTok{)}
\FunctionTok{mosaicplot}\NormalTok{(grp.pregnant }\SpecialCharTok{\textasciitilde{}}\NormalTok{ diabetes, }\AttributeTok{data=}\NormalTok{PimaDiabetes,}\AttributeTok{col=}\FunctionTok{c}\NormalTok{(}\StringTok{"Blue"}\NormalTok{,}\StringTok{"Red"}\NormalTok{), }\AttributeTok{main=}\StringTok{"pregnant vs diabetes"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{STA551EB_files/figure-latex/unnamed-chunk-69-1} \end{center}

The top two mosaic plots demonstrate the positive association between \texttt{glucose} levels and \texttt{diastolic} readings. The bottom two mosaic plots also show that diabetes is not independent of \texttt{age} and \texttt{pregnant} times because the proportion of diabetes cases in individual categories is not identical.

\hfill\break

\hypertarget{concluding-remarks}{%
\section{Concluding Remarks}\label{concluding-remarks}}

\begin{center}\includegraphics[width=0.4\linewidth]{img03/BeginWithEndInMind} \end{center}

\hfill\break

We \textbf{begin} with records that may have missing values and \textbf{end} up with an engineered record to be fed into models and algorithms!

\begin{center}\includegraphics[width=0.95\linewidth]{img03/productionRecord} \end{center}

However, if the incoming record has a missing value in mass, the resulting engineered record cannot be fed to the candidate models and algorithms since it has a missing value (most programs will automatically delete this record), hence, there will be no predicted value for the response!

\begin{center}\includegraphics[width=0.95\linewidth]{img03/PreMeanReplaceRecord} \end{center}

Therefore, we need a replacement imputation before any model-based imputation. This is practically important since it will prevent the potential failure of the dynamic prediction system!

\hypertarget{multiple-linear-regression}{%
\chapter{Multiple Linear Regression}\label{multiple-linear-regression}}

We discussed the relationship between variables in the previous two modules. The continuous variable with a normal distribution is called the response (dependent) variable and the other variable is called the explanatory (predictor, independent, or risk) variable. If the predictor variable is a factor variable, the model is called the ANOVA model which focuses on comparing the means across all factor levels. If the predictor variable is \textbf{continuous}, the model is called simple linear regression (SLR). Note that all predictor variables are assumed to be non-random.

\hypertarget{the-practical-question}{%
\section{The Practical Question}\label{the-practical-question}}

Maximum mouth opening (MMO) is also an important diagnostic reference for dental clinicians as a preliminary evaluation. Establishing a normal range for MMO could allow dental clinicians to objectively evaluate the treatment effects and set therapeutic goals for patients performing mandibular functional exercises.

\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{img04/w04-MMO} 

}

\caption{MMO, ML, and RA}\label{fig:unnamed-chunk-74}
\end{figure}

To study the relationship between maximum mouth opening and measurements of the lower jaw (mandible). A researcher randomly selected a sample of 35 subjects and measured the dependent variable, maximum mouth opening (MMO, measured in mm), as well as predictor variables, mandibular length (ML, measured in mm), and angle of rotation of the mandible (RA, measured in degrees) of each of the 35 subjects.

\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{img04/w04-DentalDataTable} 

}

\caption{Dental Data for the multiple linear regression model (MLR)}\label{fig:unnamed-chunk-75}
\end{figure}

The question is the maximum mouth opening (MMO) is determined by \textbf{two variables simultaneously}. We want to assess how these two variables (ML and RA) impact MMO \textbf{simultaneously}.

If we pick one predictor variable at a time, ML, to build a simple linear regression model and ignore the other predictor variable (RA), you only get the marginal relationship between MMO and ML since you implicitly assume that the relationship between MMO and ML will not be impacted by RA. This implicit assumption is, in general, incorrect. We need to consider all predictor variables at the same time. This is the motivation for studying multiple linear regression (MLR).

\hypertarget{the-process-of-building-a-multiple-linear-regression-model}{%
\section{The Process of Building A Multiple Linear Regression Model}\label{the-process-of-building-a-multiple-linear-regression-model}}

The previous motivation example involves two continuous predictor variables. In real-world applications, it is common to have many predictor variables. Predictor variables are also assumed to be non-random. They could be categorical, continuous, or discrete. In a specific application, you may have a set of categorical, continuous, and discrete predictor variables in one data set.

\hypertarget{assumptions-of-mlr}{%
\subsection{Assumptions of MLR}\label{assumptions-of-mlr}}

There are several assumptions of multiple linear regression models.

\begin{itemize}
\item
  The response variable is a normal random variable and its mean is influenced by explanatory variables but not the variance.
\item
  The explanatory variables are assumed to be non-random.
\item
  The explanatory variables are assumed to be uncorrelated to each other.
\item
  The functional form of the explanatory variables in the regression model is correctly specified.
\item
  The data is a random sample taken independently from the study population with a specified distribution.
\end{itemize}

Some of these assumptions will be used directly to define model diagnostic measures. The idea is to assume all conditions are met (at least temporarily) and then fit the model to the data set.

\hypertarget{the-structure-of-mlr}{%
\subsection{The Structure of MLR}\label{the-structure-of-mlr}}

Assume that there are \(p\) predictor variables \(\{x_1, x_2, \cdots, x_p \}\), the first-order linear regression is defined in the following form

\[
y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \cdots + \beta_p x_p + \epsilon
\]

\(\beta_0\) is the intercept, \(\beta_1, \beta_2, \cdots, \beta_p\) are called slope parameters. if \(\beta_i=0\), the associated predictor variable \(x_i\) is uncorrelated with response vararible \(y\). If \(\beta_i > 0\), then \(y\) and \(x_i\) are positively correlated. In fact, \(\beta_1\) is the increment of \(y\) as \(x_i\) increases one unit and other predictors remain unchanged.

The response variable is assumed to be a normal random variable with constant variance. If the first-order linear regression function is correct, then

\[y \to N(\beta_0 + \beta_1 x_1 + \beta_2 x_2 + \cdots + \beta_p x_p, \sigma^2).\]
This also implies that \(\epsilon \to N(0,1)\). The residual of each data point can be estimated from the data with an assumed linear regression model.

\begin{figure}[!ht]

{\centering \includegraphics[width=0.8\linewidth,]{img04/w04-RegressionPlane} 

}

\caption{Illustrative regression plane: MMO vs ML and RA}\label{fig:w10-RegressionPlane}
\end{figure}

For ease of illustration, let's consider the case of the MLR with two predictor variables in the motivation example.

\[MMO = \beta_0 + \beta_1 ML + \beta_2 RA + \epsilon\]

is the first-order linear regression model. The following figure gives the graphical annotations of the fundamental concepts in linear regression models. This is a generalization of the regression line (see the analogous figure in the previous module for the simple linear regression model).

Since \(MMO\) is a normal random variable with constant variance, \(MMO \to N(\beta_0+\beta_1ML +\beta_2 RA, \sigma^2)\), or equivalently, \(\epsilon \to N(0, \sigma^2)\). The residuals are defined to be the directional vertical distances between the observed points and the regression plane.

In some practical applications, we may need \textbf{the second-order} linear regression model to reflect the actual relationship between predictor variables and the response variable. For example, \[MMO = \alpha_0 + \alpha_1 ML + \alpha_2 RA + \alpha_3 ML^2 + \alpha_4 RA^2 + \alpha_5 ML\times RA + \epsilon\] is called (the second-order) linear regression model. With the second-order terms in the regression function, we obtain the regression surface as shown in Figure.

\begin{figure}[!ht]

{\centering \includegraphics[width=9.29in,]{img04/w04-RegressionSurface} 

}

\caption{Illustrative regression surface: MMO vs ML and RA}\label{fig:w10-RegressionSurface}
\end{figure}

If the second-order linear regression is appropriate, then \(\epsilon \to N(0, \sigma^2)\) and \(E[MMO] = \alpha_0 + \alpha_1 ML + \alpha_2 RA + \alpha_3 ML^2 + \alpha_4 RA^2 + \alpha_5 ML\times RA\). The residuals of the second-order linear regression model are defined to be the directional distance between the observed points and the regression surface.

\hypertarget{more-on-model-specifications}{%
\subsection{More on Model Specifications}\label{more-on-model-specifications}}

In the above section, we introduced both first- and second-order polynomial regression models. In general, it is not common to use high-order polynomial regression models in real-world applications.

\begin{itemize}
\item
  \textbf{Interaction effect} - it is common to include interaction terms (i.e., the cross product of two or more predictor variables) in the multiple linear regression models when the effect of one variable on the response variable is dependent on the other predictor variable. In other words, the interaction terms capture the \textbf{joint effect} of predictor variables. \textbf{It is rare to have third-order or higher-order interaction terms in a regression model}.
\item
  \textbf{Dummy variables} - All categorical predictor variables are automatically converted into dummy variables (binary indicator variables). If categorical variables in the data are numerically coded, we have to turn these numerically coded variables into factor variables in the regression model.
\item
  \textbf{Discretization and Regrouping} - Discretizing numerical predictor variables and regrouping categorical or discrete predictor variables are two basic pre-process procedures that are actually very common in many practical applications.

  \begin{itemize}
  \item
    Sometimes these two procedures are required to satisfy certain model assumptions. For example, if a categorical variable has a few categories that have less than 5 observations, the resulting p-values based on certain hypothesis tests will be invalid. In this case, We have to regroup some of the categories in \textbf{meaningful ways} to resolve the \textbf{sparsity} issues in order to obtain valid results.
  \item
    In many other applications, we want the model to be easy to interpret. Discretizing numerical variables is common. For example, we can see grouped ages and salary ranges in different applications.
  \end{itemize}
\end{itemize}

\hypertarget{estimation-of-regression-coefficients}{%
\subsection{Estimation of Regression Coefficients}\label{estimation-of-regression-coefficients}}

A simple and straightforward method for estimating the coefficients of linear regression models is to minimize the sum of the squared residuals - least square estimation (LSE). To find the LSE of the regression coefficients, we need to

\begin{itemize}
\item
  choose the (first-order, second-order, or even high-order) regression function (see 3D hyper-plane or hyper-surface in the above two figures as examples).
\item
  find the distances between the observed points and the hyper-plane (or hyper-surface). These distances are the residuals of the regression - which is dependent on the regression coefficients.
\item
  calculate the sum of squared residuals. This sum of the residuals is still dependent on the regression coefficients.
\item
  find the values for the regression coefficients that minimize the sum of the squared residuals. These values are called the least square estimates (LSEs) of the corresponding regression coefficients.
\end{itemize}

R function \textbf{lm()} implements the above LSE algorithm to find the regression coefficients. We have used this function in ANOVA and simple linear regression models.

\hypertarget{model-diagnostics}{%
\subsection{Model Diagnostics}\label{model-diagnostics}}

Unlike simple linear regression models, the primary assumptions of the regression model focus on the normal distribution of the response variable and the correct regression function. For multiple linear regression models, we need to impose a couple of assumptions in addition to those in the simple linear regression models

\begin{itemize}
\tightlist
\item
  \textbf{Residual Diagnostics}
\end{itemize}

One of the fundamental assumptions of linear regression modeling is that the response variable is normally distributed with a constant variance. This implies \(\epsilon \to N(0, \sigma^2)\).

After obtaining the LSE of the regression coefficients, we can estimate the residuals and use these estimated residuals to detect the potential violations of the normality assumption of the response variable. To be more specific, we consider the first-order polynomial regression, the estimated residual of \(i\)-th observation is defined to be \(e_i = MMO - \hat{\beta}_0 + \hat{\beta}_1 ML + \hat{\beta}_2 RA\)

If there is no violation of the normality assumption, we would expect the following residual plot and Q-Q plot.

\begin{figure}[!ht]

{\centering \includegraphics[width=16.17in,]{img04/w04-GoodResidualPlots} 

}

\caption{Good residual plot and normal Q-Q plot}\label{fig:w10-GoodResidualPlot}
\end{figure}

Some of the commonly seen poor residual plots represent different violations of various assumptions. We can try to use various transformations (such Box-Cox power transformations) of the response variable to correct the issue.

\begin{figure}[!ht]

{\centering \includegraphics[width=12.79in,]{img04/w04-BadResidualPlots} 

}

\caption{Poor residual plots representing various violations of the model assumptions}\label{fig:w10-BadResidualPlots}
\end{figure}

\begin{itemize}
\tightlist
\item
  \textbf{Multicollinearity}
\end{itemize}

Some of the predictor variables are linearly correlated. The consequence of multi-collinearity causes to unstable LSE of the regression coefficients (i.e., the LSEs of the regression coefficients are sensitive to a small change in the model). It also reduces the precision of the estimate coefficients and, hence, the p-values are not reliable.

Multicollinearity affects the coefficients and p-values, but it does not influence the predictions, precision of the predictions, and the goodness-of-fit statistics. If our primary goal is to make predictions, we don't need to understand the role of each independent variable and we don't need to reduce severe multicollinearity.

If the primary goal is to perform association analysis, we need to reduce collinearity since both LSE and p-values are the keys to association analysis.

To detect multicollinearity, we can use the variance inflation factor (VIF) to inspect the multicollinearity of the individual predictor variable. There are some different methods to reduce multicollinearity. Centering predictor variables is one of them and works well sometimes. Some other advanced modeling-based methods are covered in more advanced courses.

\hypertarget{goodness-of-fit-and-variable-selection}{%
\subsection{Goodness-of-fit and Variable Selection}\label{goodness-of-fit-and-variable-selection}}

There several different goodness-of-fit measures are available for the linear regression model due to the assumption of the normality assumption of the response variable.

\begin{itemize}
\tightlist
\item
  \textbf{Coefficient of Determination}
\end{itemize}

We only introduce \textbf{the coefficient of determination \(R^2\)} which measures the percentage of variability within the -values that can be explained by the regression model. In simple linear regression models, \textbf{the coefficient of determination \(R^2\)} is simply the square of the sample Pearson correlation coefficient.

\begin{itemize}
\tightlist
\item
  \textbf{Statistical Significance and Practical Importance}
\end{itemize}

A small p-value of the significant test for a predictor variable indicates the variable is statistically significant but may not be practically important. On the other hand, some practically important predictor variables may not achieve statistical significance due to the limited sample size. In the practical applications, \textbf{we may want to include some of the practically important predictor variables in the final model regardless of their statistical significance}.

\begin{itemize}
\tightlist
\item
  \textbf{Model Selection}
\end{itemize}

One of the criteria for assessing the goodness-of-fit is the parsimony of the model. A parsimonious model is a model that accomplishes the desired level of explanation or prediction with as few predictor variables as possible. There are generally two ways of evaluating a model: Based on predictions and based on goodness of fit on the current data such as \(R^2\) and some likelihood-based measures.

R has an automatic variable selection procedure, \textbf{step()}, which uses the goodness-of-fit measure AIC (Akaike Information Criterion) which is not formally introduced in this class due to the level of mathematics needed in the definition, but we can still use it to perform the automatic variable selection. \href{http://rstudio-pubs-static.s3.amazonaws.com/2899_a9129debf6bd47d2a0501de9c0dc583d.html}{This tutorial gives detailed examples on how to use \textbf{step()} (link)}.

\hypertarget{case-study-1}{%
\section{Case Study 1}\label{case-study-1}}

We use the dental data in the motivation example for the case study.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{MMO}\OtherTok{=}\FunctionTok{c}\NormalTok{(}\FloatTok{52.34}\NormalTok{, }\FloatTok{51.90}\NormalTok{, }\FloatTok{52.80}\NormalTok{, }\FloatTok{50.29}\NormalTok{, }\FloatTok{57.79}\NormalTok{, }\FloatTok{49.41}\NormalTok{, }\FloatTok{53.28}\NormalTok{, }\FloatTok{59.71}\NormalTok{, }\FloatTok{53.32}\NormalTok{, }\FloatTok{48.53}\NormalTok{, }\FloatTok{51.59}\NormalTok{, }
      \FloatTok{58.52}\NormalTok{, }\FloatTok{62.93}\NormalTok{, }\FloatTok{57.62}\NormalTok{, }\FloatTok{65.64}\NormalTok{, }\FloatTok{52.85}\NormalTok{, }\FloatTok{64.43}\NormalTok{, }\FloatTok{57.25}\NormalTok{, }\FloatTok{50.82}\NormalTok{, }\FloatTok{40.48}\NormalTok{, }\FloatTok{59.68}\NormalTok{, }\FloatTok{54.35}\NormalTok{, }
      \FloatTok{47.00}\NormalTok{, }\FloatTok{47.23}\NormalTok{,  }\FloatTok{41.19}\NormalTok{, }\FloatTok{42.76}\NormalTok{, }\FloatTok{51.88}\NormalTok{, }\FloatTok{42.77}\NormalTok{, }\FloatTok{52.34}\NormalTok{, }\FloatTok{50.45}\NormalTok{, }\FloatTok{43.18}\NormalTok{, }\FloatTok{41.99}\NormalTok{, }\FloatTok{39.45}\NormalTok{, }
      \FloatTok{38.91}\NormalTok{, }\FloatTok{49.10}\NormalTok{)}
\DocumentationTok{\#\#}
\NormalTok{ML}\OtherTok{=}\FunctionTok{c}\NormalTok{(}\FloatTok{100.85}\NormalTok{, }\FloatTok{93.08}\NormalTok{, }\FloatTok{98.43}\NormalTok{, }\FloatTok{102.95}\NormalTok{, }\FloatTok{108.24}\NormalTok{, }\FloatTok{98.34}\NormalTok{, }\FloatTok{95.57}\NormalTok{, }\FloatTok{98.85}\NormalTok{,}\FloatTok{98.32}\NormalTok{, }\FloatTok{92.70}\NormalTok{, }\FloatTok{88.89}\NormalTok{, }
     \FloatTok{104.06}\NormalTok{,  }\FloatTok{98.18}\NormalTok{, }\FloatTok{91.01}\NormalTok{, }\FloatTok{96.98}\NormalTok{, }\FloatTok{97.85}\NormalTok{, }\FloatTok{96.89}\NormalTok{, }\FloatTok{98.35}\NormalTok{, }\FloatTok{90.65}\NormalTok{, }\FloatTok{92.99}\NormalTok{, }\FloatTok{108.97}\NormalTok{, }\FloatTok{91.85}\NormalTok{, }
     \FloatTok{104.30}\NormalTok{, }\FloatTok{93.16}\NormalTok{, }\FloatTok{94.18}\NormalTok{, }\FloatTok{89.56}\NormalTok{, }\FloatTok{105.85}\NormalTok{, }\FloatTok{89.29}\NormalTok{, }\FloatTok{92.58}\NormalTok{, }\FloatTok{98.64}\NormalTok{, }\FloatTok{83.70}\NormalTok{, }\FloatTok{88.46}\NormalTok{, }\FloatTok{94.93}\NormalTok{, }
     \FloatTok{96.81}\NormalTok{, }\FloatTok{93.13}\NormalTok{)}
\DocumentationTok{\#\#}
\NormalTok{RA }\OtherTok{=} \FunctionTok{c}\NormalTok{(}\FloatTok{32.08}\NormalTok{, }\FloatTok{39.21}\NormalTok{, }\FloatTok{33.74}\NormalTok{, }\FloatTok{34.19}\NormalTok{, }\FloatTok{35.13}\NormalTok{, }\FloatTok{30.92}\NormalTok{, }\FloatTok{37.71}\NormalTok{, }\FloatTok{44.71}\NormalTok{, }\FloatTok{33.17}\NormalTok{, }\FloatTok{31.74}\NormalTok{, }\FloatTok{37.07}\NormalTok{, }
       \FloatTok{38.71}\NormalTok{, }\FloatTok{43.89}\NormalTok{, }\FloatTok{41.06}\NormalTok{, }\FloatTok{41.92}\NormalTok{, }\FloatTok{35.25}\NormalTok{, }\FloatTok{45.11}\NormalTok{, }\FloatTok{39.44}\NormalTok{, }\FloatTok{38.33}\NormalTok{, }\FloatTok{25.93}\NormalTok{, }\FloatTok{36.78}\NormalTok{, }\FloatTok{42.02}\NormalTok{, }
       \FloatTok{27.20}\NormalTok{, }\FloatTok{31.37}\NormalTok{, }\FloatTok{27.87}\NormalTok{, }\FloatTok{28.69}\NormalTok{, }\FloatTok{31.04}\NormalTok{, }\FloatTok{32.78}\NormalTok{, }\FloatTok{37.82}\NormalTok{, }\FloatTok{33.36}\NormalTok{, }\FloatTok{31.93}\NormalTok{, }\FloatTok{28.32}\NormalTok{, }\FloatTok{24.82}\NormalTok{, }
       \FloatTok{23.88}\NormalTok{, }\FloatTok{36.17}\NormalTok{)}

\NormalTok{DentalData }\OtherTok{=} \FunctionTok{as.data.frame}\NormalTok{(}\FunctionTok{cbind}\NormalTok{(}\AttributeTok{MMO =}\NormalTok{ MMO, }\AttributeTok{ML =}\NormalTok{ ML, }\AttributeTok{RA =}\NormalTok{ RA))}
\end{Highlighting}
\end{Shaded}

\begin{itemize}
\tightlist
\item
  \textbf{Pair-wise Scatter Plot}
\end{itemize}

This pairwise scatter plot tells whether there are significant correlations between \textbf{numerical predictor variables}.

\begin{figure}[!ht]

{\centering \includegraphics{STA551EB_files/figure-latex/unnamed-chunk-77-1} 

}

\caption{Pair-wise scatter plot}\label{fig:unnamed-chunk-77}
\end{figure}

We can see the following patterns from the above pair-wise scatter plot.

(1). Both ML and RA are linearly correlated with the response variable MMO. This is what we expected.

(2). ML and RA are not linearly correlated. This indicates that there is no collinearity issue.

(3). We also don't see any special patterns such as outliers and extremely skewed distribution. There is no need to perform discretization and regrouping procedures on the predictor variables.

(4). In this data set, there is no categorical variables or categorical variable with a numerical coding system in this data set. There is no need to create dummy variables.

\begin{itemize}
\tightlist
\item
  \textbf{Initial model}
\end{itemize}

The following initial model includes all predictor variables. The residual plots indicate that

(1). One of the observations seems to be an outlier (observation 15);

(2). There is a minor violation of the assumption of constant variance.

(3). There is also a minor violation of the assumption of normality of the distribution of the residuals.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ini.model }\OtherTok{=} \FunctionTok{lm}\NormalTok{(MMO }\SpecialCharTok{\textasciitilde{}}\NormalTok{ ML }\SpecialCharTok{+}\NormalTok{ RA, }\AttributeTok{data =}\NormalTok{ DentalData)   }\CommentTok{\# fit a linear model with interaction effect}
\FunctionTok{par}\NormalTok{(}\AttributeTok{mfrow=}\FunctionTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{), }\AttributeTok{mar=}\FunctionTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{,}\DecValTok{3}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{))}
\FunctionTok{plot}\NormalTok{(ini.model)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics{STA551EB_files/figure-latex/unnamed-chunk-78-1} 

}

\caption{Residual plots of the the linear regression model.}\label{fig:unnamed-chunk-78}
\end{figure}

Next, we will carry the Box-Cox transformation to identify a potential power transformation of the response variable MMO.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(MASS)}
\FunctionTok{boxcox}\NormalTok{(MMO }\SpecialCharTok{\textasciitilde{}}\NormalTok{ ML }\SpecialCharTok{+}\NormalTok{ RA, }
       \AttributeTok{data =}\NormalTok{ DentalData, }
       \AttributeTok{lambda =} \FunctionTok{seq}\NormalTok{(}\SpecialCharTok{{-}}\DecValTok{1}\NormalTok{, }\FloatTok{1.5}\NormalTok{, }\AttributeTok{length =} \DecValTok{10}\NormalTok{), }
       \AttributeTok{xlab=}\FunctionTok{expression}\NormalTok{(}\FunctionTok{paste}\NormalTok{(lambda)))}
\FunctionTok{title}\NormalTok{(}\AttributeTok{main =} \StringTok{"Box{-}Cox Transformation: 95\% CI of lambda"}\NormalTok{,}
      \AttributeTok{col.main =} \StringTok{"navy"}\NormalTok{, }\AttributeTok{cex.main =} \FloatTok{0.9}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics{STA551EB_files/figure-latex/unnamed-chunk-79-1} 

}

\caption{Box-cox transformation for power transformation.}\label{fig:unnamed-chunk-79}
\end{figure}

Since both 0 and 1 are in the \(95\%\) confidence interval of \(\lambda\), technically speaking, there is no need to perform the power transformation. By the optimal \(\lambda\) is closer to 0, we try to perform the log transformation (corresponding to \(\lambda =0\)) to see whether there will some improvement of the initial model

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{transform.model }\OtherTok{=} \FunctionTok{lm}\NormalTok{(}\FunctionTok{log}\NormalTok{(MMO) }\SpecialCharTok{\textasciitilde{}}\NormalTok{ ML }\SpecialCharTok{*}\NormalTok{ RA, }\AttributeTok{data  =}\NormalTok{ DentalData)}
\FunctionTok{par}\NormalTok{(}\AttributeTok{mfrow=}\FunctionTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{), }\AttributeTok{mar =} \FunctionTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{))}
\FunctionTok{plot}\NormalTok{(transform.model)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics{STA551EB_files/figure-latex/unnamed-chunk-80-1} 

}

\caption{Residual plots for the regression with transformed response variable.}\label{fig:unnamed-chunk-80}
\end{figure}

The above residual plots indicate an improvement in model fit. We will use the transformed response to build the final model.

\begin{itemize}
\tightlist
\item
  \textbf{Final Model}
\end{itemize}

The model based on the log-transformed response is summarized in the following.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{kable}\NormalTok{(}\FunctionTok{summary}\NormalTok{(transform.model)}\SpecialCharTok{$}\NormalTok{coef, }\AttributeTok{caption =} \StringTok{"Summarized statistics of the regression }
\StringTok{      coefficients of the model with a log{-}transformed response"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{table}

\caption{\label{tab:unnamed-chunk-81}Summarized statistics of the regression 
      coefficients of the model with a log-transformed response}
\centering
\begin{tabular}[t]{l|r|r|r|r}
\hline
  & Estimate & Std. Error & t value & Pr(>|t|)\\
\hline
(Intercept) & 1.9957108 & 1.0023242 & 1.9910831 & 0.0553479\\
\hline
ML & 0.0124400 & 0.0104503 & 1.1903999 & 0.2429256\\
\hline
RA & 0.0296960 & 0.0293716 & 1.0110477 & 0.3198204\\
\hline
ML:RA & -0.0000884 & 0.0003059 & -0.2890324 & 0.7744807\\
\hline
\end{tabular}
\end{table}

we can see that the interaction effect is insignificant in the model. We drop the highest term in the regression model either manually or automatically. In the next code chunk, we use the automatic variable selection method to find the final model.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{transform.model }\OtherTok{=} \FunctionTok{lm}\NormalTok{(}\FunctionTok{log}\NormalTok{(MMO)}\SpecialCharTok{\textasciitilde{}}\NormalTok{ML}\SpecialCharTok{*}\NormalTok{RA, }\AttributeTok{data =}\NormalTok{ DentalData)}
\DocumentationTok{\#\# I will use the automatic variable selection function to search the final model}
\NormalTok{final.model }\OtherTok{=}  \FunctionTok{step}\NormalTok{(transform.model, }\AttributeTok{direction =} \StringTok{"backward"}\NormalTok{, }\AttributeTok{trace =} \DecValTok{0}\NormalTok{)}
\FunctionTok{kable}\NormalTok{(}\FunctionTok{summary}\NormalTok{(final.model)}\SpecialCharTok{$}\NormalTok{coef, }\AttributeTok{caption =} \StringTok{"Summary statistics of the regression }
\StringTok{      coefficients of the final model"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{table}

\caption{\label{tab:unnamed-chunk-82}Summary statistics of the regression 
      coefficients of the final model}
\centering
\begin{tabular}[t]{l|r|r|r|r}
\hline
  & Estimate & Std. Error & t value & Pr(>|t|)\\
\hline
(Intercept) & 2.2833535 & 0.1176375 & 19.410076 & 0\\
\hline
ML & 0.0094391 & 0.0011705 & 8.064482 & 0\\
\hline
RA & 0.0212140 & 0.0012017 & 17.653748 & 0\\
\hline
\end{tabular}
\end{table}

Now we have three candidate models to select from. We extract the coefficient of determination (\(R^2\)) of each of the three candidate models.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{r.ini.model }\OtherTok{=} \FunctionTok{summary}\NormalTok{(ini.model)}\SpecialCharTok{$}\NormalTok{r.squared}
\NormalTok{r.transfd.model }\OtherTok{=} \FunctionTok{summary}\NormalTok{(transform.model)}\SpecialCharTok{$}\NormalTok{r.squared}
\NormalTok{r.final.model }\OtherTok{=} \FunctionTok{summary}\NormalTok{(final.model)}\SpecialCharTok{$}\NormalTok{r.squared}
\DocumentationTok{\#\#}
\NormalTok{Rsquare }\OtherTok{=} \FunctionTok{cbind}\NormalTok{(}\AttributeTok{ini.model =}\NormalTok{ r.ini.model, }\AttributeTok{transfd.model =}\NormalTok{ r.transfd.model, }
                \AttributeTok{final.model =}\NormalTok{ r.final.model)}
\FunctionTok{kable}\NormalTok{(Rsquare, }\AttributeTok{caption=}\StringTok{"Coefficients of correlation of the three candidate models"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{table}

\caption{\label{tab:unnamed-chunk-83}Coefficients of correlation of the three candidate models}
\centering
\begin{tabular}[t]{r|r|r}
\hline
ini.model & transfd.model & final.model\\
\hline
0.9204481 & 0.9257218 & 0.9255216\\
\hline
\end{tabular}
\end{table}

The second and the third model have almost the same \(R^2\), \(92.56\%\) and \(92.57\%\). Both models are based on the log-transformed MMO. The interpretations of these two models are not straightforward. The initial model has a slightly lower \(92.0\%\). Since the initial model has a simple structure and is easy to interpret, we choose the initial model as the final model to report. The summarized statistic is given in the following table.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{summary.ini.model }\OtherTok{=} \FunctionTok{summary}\NormalTok{(ini.model)}\SpecialCharTok{$}\NormalTok{coef}
\FunctionTok{kable}\NormalTok{(summary.ini.model, }\AttributeTok{caption =} \StringTok{"Summary of the final working model"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{table}

\caption{\label{tab:unnamed-chunk-84}Summary of the final working model}
\centering
\begin{tabular}[t]{l|r|r|r|r}
\hline
  & Estimate & Std. Error & t value & Pr(>|t|)\\
\hline
(Intercept) & -31.4247984 & 6.1474668 & -5.111829 & 1.44e-05\\
\hline
ML & 0.4731743 & 0.0611653 & 7.735992 & 0.00e+00\\
\hline
RA & 1.0711725 & 0.0627967 & 17.057792 & 0.00e+00\\
\hline
\end{tabular}
\end{table}

In summary, both ML and RA are statistically significant (p-value \(\approx 0\)) and both are positively correlated to MMO. Further, for a given angle of rotation of the mandible (RA), when mandibular length (ML) increases by 1mm, the maximum mouth opening (MMO) increases by 0.473 mm. However, for holding ML, a 1-degree increase in RA will result in a 1.071 mm increase in MMO.

\hypertarget{case-study-2}{%
\section{Case Study 2}\label{case-study-2}}

We discussed the ANOVA model in module 8. In fact, the ANOVA model is a special linear regression model. The location is a factor variable. We now build a linear regression using mussel shell length as the response and the location as the predictor variable in the following (code is copied from module 8).

Since predictor variable location is a categorical factor variable, R function \textbf{lm()} will automatically define four dummy variables for each category except for the baseline category is, by default, the smallest character values (alphabetical order). In our example, the value \textbf{Magadan} is the smallest. Other categories will be compared with the baseline category through the corresponding dummy variable.

To be more specific, the four dummy variables associated with the four categories will be defined by

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  locationNewport = 1 if the location is Newport, 0 otherwise;
\item
  locationPetersburg = 1 if the location is Petersburg, 0 otherwise;
\item
  locationTillamook = 1 if the location is Tillamook, 0 otherwise;
\item
  locationTvarminne = 1 if the location is Tvarminne, 0 otherwise.
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x1 }\OtherTok{=} \FunctionTok{c}\NormalTok{(}\FloatTok{0.0571}\NormalTok{,}\FloatTok{0.0813}\NormalTok{, }\FloatTok{0.0831}\NormalTok{, }\FloatTok{0.0976}\NormalTok{, }\FloatTok{0.0817}\NormalTok{, }\FloatTok{0.0859}\NormalTok{, }\FloatTok{0.0735}\NormalTok{, }\FloatTok{0.0659}\NormalTok{, }\FloatTok{0.0923}\NormalTok{, }\FloatTok{0.0836}\NormalTok{) }
\NormalTok{x2 }\OtherTok{=} \FunctionTok{c}\NormalTok{(}\FloatTok{0.0873}\NormalTok{,}\FloatTok{0.0662}\NormalTok{, }\FloatTok{0.0672}\NormalTok{, }\FloatTok{0.0819}\NormalTok{, }\FloatTok{0.0749}\NormalTok{, }\FloatTok{0.0649}\NormalTok{, }\FloatTok{0.0835}\NormalTok{, }\FloatTok{0.0725}\NormalTok{)}
\NormalTok{x3 }\OtherTok{=} \FunctionTok{c}\NormalTok{(}\FloatTok{0.0974}\NormalTok{,}\FloatTok{0.1352}\NormalTok{, }\FloatTok{0.0817}\NormalTok{, }\FloatTok{0.1016}\NormalTok{, }\FloatTok{0.0968}\NormalTok{, }\FloatTok{0.1064}\NormalTok{, }\FloatTok{0.1050}\NormalTok{)}
\NormalTok{x4 }\OtherTok{=} \FunctionTok{c}\NormalTok{(}\FloatTok{0.1033}\NormalTok{,}\FloatTok{0.0915}\NormalTok{, }\FloatTok{0.0781}\NormalTok{, }\FloatTok{0.0685}\NormalTok{, }\FloatTok{0.0677}\NormalTok{, }\FloatTok{0.0697}\NormalTok{, }\FloatTok{0.0764}\NormalTok{, }\FloatTok{0.0689}\NormalTok{)}
\NormalTok{x5 }\OtherTok{=} \FunctionTok{c}\NormalTok{(}\FloatTok{0.0703}\NormalTok{,}\FloatTok{0.1026}\NormalTok{, }\FloatTok{0.0956}\NormalTok{, }\FloatTok{0.0973}\NormalTok{, }\FloatTok{0.1039}\NormalTok{, }\FloatTok{0.1045}\NormalTok{)}
\NormalTok{len  }\OtherTok{=} \FunctionTok{c}\NormalTok{(x1, x2, x3, x4, x5)      }\CommentTok{\# pool all sub{-}samples of lengths}
\NormalTok{location }\OtherTok{=} \FunctionTok{c}\NormalTok{(}\FunctionTok{rep}\NormalTok{(}\StringTok{"Tillamook"}\NormalTok{, }\FunctionTok{length}\NormalTok{(x1)), }
             \FunctionTok{rep}\NormalTok{(}\StringTok{"Newport"}\NormalTok{, }\FunctionTok{length}\NormalTok{(x2)),}
             \FunctionTok{rep}\NormalTok{(}\StringTok{"Petersburg"}\NormalTok{, }\FunctionTok{length}\NormalTok{(x3)),}
             \FunctionTok{rep}\NormalTok{(}\StringTok{"Magadan"}\NormalTok{, }\FunctionTok{length}\NormalTok{(x4)),}
             \FunctionTok{rep}\NormalTok{(}\StringTok{"Tvarminne"}\NormalTok{, }\FunctionTok{length}\NormalTok{(x5)))     }\CommentTok{\# location vector matches the lengths}
\NormalTok{data.matrix }\OtherTok{=} \FunctionTok{cbind}\NormalTok{(}\AttributeTok{len =}\NormalTok{  len, }\AttributeTok{location =}\NormalTok{ location)   }\CommentTok{\# data a data table}
\NormalTok{musseldata }\OtherTok{=} \FunctionTok{as.data.frame}\NormalTok{(data.matrix)        }\CommentTok{\# data frame}
\DocumentationTok{\#\# End of data set creation}
\DocumentationTok{\#\#}
\DocumentationTok{\#\# Starting building the ANOVA model}
\NormalTok{anova.model}\FloatTok{.01} \OtherTok{=} \FunctionTok{lm}\NormalTok{(len }\SpecialCharTok{\textasciitilde{}}\NormalTok{ location, }\AttributeTok{data =}\NormalTok{ musseldata)  }\CommentTok{\# define a model for generating the ANOVA}
\DocumentationTok{\#\#}
\FunctionTok{par}\NormalTok{(}\AttributeTok{mfrow=}\FunctionTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{), }\AttributeTok{mar =} \FunctionTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{))}
\FunctionTok{plot}\NormalTok{(anova.model}\FloatTok{.01}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics{STA551EB_files/figure-latex/unnamed-chunk-85-1} 

}

\caption{Residual plots of the anova model.}\label{fig:unnamed-chunk-85}
\end{figure}

The above residual plots indicate no serious violation of the model assumption. The model that generates the above residual plot will be used as the final working model. The inference of the regression coefficients is summarized in the following table.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sum.stats }\OtherTok{=} \FunctionTok{summary}\NormalTok{(anova.model}\FloatTok{.01}\NormalTok{)}\SpecialCharTok{$}\NormalTok{coef}
\FunctionTok{kable}\NormalTok{(sum.stats, }\AttributeTok{caption =} \StringTok{"Summary of the ANOVA model"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{table}

\caption{\label{tab:unnamed-chunk-86}Summary of the ANOVA model}
\centering
\begin{tabular}[t]{l|r|r|r|r}
\hline
  & Estimate & Std. Error & t value & Pr(>|t|)\\
\hline
(Intercept) & 0.0780125 & 0.0044536 & 17.5168782 & 0.0000000\\
\hline
locationNewport & -0.0032125 & 0.0062983 & -0.5100593 & 0.6133053\\
\hline
locationPetersburg & 0.0254304 & 0.0065193 & 3.9007522 & 0.0004300\\
\hline
locationTillamook & 0.0021875 & 0.0059751 & 0.3661039 & 0.7165558\\
\hline
locationTvarminne & 0.0176875 & 0.0068029 & 2.5999834 & 0.0136962\\
\hline
\end{tabular}
\end{table}

From the above summary tale, we can see that P-values associated with location dummy variables locationNewport, locationTillamook are bigger than 0.05 meaning the means associated with \textbf{Newport}, \textbf{Tillamook}, and the baseline \textbf{Magadan} (not appearing in the summary table). The p-values associated with \textbf{Petersburg} and \textbf{Tvarminn} are less the 0.05 which implies that the mean length of these two locations is significantly different from that of the baseline location \textbf{Magadan}. Further, the coefficient associated with dummy variable \textbf{locationPetersburg} indicates that the mean length of the mussel shell in \textbf{Petersburg} is 0.0543 units longer than that in the baseline location \textbf{Magadan}. We can also interpret the coefficients associated with \textbf{locationTvarminne}.

\hfill\break

\hypertarget{logistic-regression-models}{%
\chapter{Logistic Regression Models}\label{logistic-regression-models}}

Linear regression models are used to assess the association between the continuous response variable and other predictor variables. If the response variable is a binary categorical variable, the linear regression model is not appropriate. We need a new model, the logistic regression model, to assess the association between the binary response variable and other predictor variables.

This module focuses on the regression model with a binary response.

\hypertarget{motivational-example-and-practical-question}{%
\section{Motivational Example and Practical Question}\label{motivational-example-and-practical-question}}

\textbf{Example }: Longevity in male fruit flies is positively associated with adult size. But reproduction has a high physiological cost that could impact longevity.

\begin{center}\includegraphics[width=0.8\linewidth]{img04/w04-FruitFliesImage} \end{center}

The original study looks at the association between longevity and adult size in male fruit flies kept under one of two conditions. One group is kept with sexually active females over the male's life span. The other group is cared for in the same way but kept with females that are not sexually active.

\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{img04/w04-FruitFlies} 

}

\caption{Fruit Flies Data Table}\label{fig:unnamed-chunk-89}
\end{figure}

The above table gives the longevity in days for the male fruit flies given an opportunity to reproduce (IndReprod = 0) and for those deprived of the opportunity (IndReprod = 1).

The data was collected from a case-control study design. The original study association analysis used the multiple linear regression model in which Longevity was a response variable and Thorax and IndReprod were used as predictor variables. In this example, we build a logistic regression to assess the association between longevity and reduction. Due to the case-control study design, the resulting logistic regression cannot be used as a predictive model.

\begin{figure}

{\centering \includegraphics{STA551EB_files/figure-latex/unnamed-chunk-91-1} 

}

\caption{The scatter plots of a binary response v.s. a continuous predictor variable}\label{fig:unnamed-chunk-91}
\end{figure}

Since the response variable is binary (i.e., it can only take on two distinct values, 0 and 1 in this example), the linear regression line is a bad choice since (1) the response variable is not continuous, (2) the fitted regression line can take on any values between negative infinity and positive infinity. The response variable takes on only either 0 or 1 (see the dark red straight line).

A meaningful approach to assess the association between the binary response variable and the predictor variable by looking at how the predictor variables impact the probability of observing the \textbf{bigger} value (i.e., the one that has a higher alphabetical order) of the response variable. The above \textbf{S} curve describes the relationship between the values of the predictor variable(s) and the probability of observing the \textbf{bigger} value of the response variable.

\hypertarget{logistic-regression-models-and-applications}{%
\section{Logistic Regression Models and Applications}\label{logistic-regression-models-and-applications}}

Logistic regression is a member of the generalized linear regression (GLM) family which includes the linear regression models. The model-building process is the same as that in linear regression modeling.

\hypertarget{the-structure-of-the-logistic-regression-model}{%
\subsection{The Structure of the Logistic Regression Model}\label{the-structure-of-the-logistic-regression-model}}

The actual probability function of observing the \textbf{bigger} value of the response variable for giving the predictor variable is given by

\[
P(Y=1) = \frac{\exp(\beta_0 + \beta_1 Longevity)}{1 + \exp(\beta_0 + \beta_1 Longevity)}
\]

If \(\beta_1\) (also called slope parameter) is equal to zero, the longevity and the reproduction status are NOT associated. Otherwise, there is an association between the response and the predictor variables. The sign of the slope parameter reflects the positive or negative association.

\hypertarget{assumptions-and-diagnostics}{%
\subsection{Assumptions and Diagnostics}\label{assumptions-and-diagnostics}}

There are assumptions for the binary logistic regression model.

\begin{itemize}
\item
  The response variable must be binary (taking on only two distinct values).
\item
  Predictor variables are assumed to be uncorrelated.
\item
  The functional form of the predictor variables is correctly specified.
\end{itemize}

The model diagnostics for logistic regression are much more complicated than the linear regression models. We will not discuss this topic in this course.

\hypertarget{coefficient-estimation-and-interpretation}{%
\subsection{Coefficient Estimation and Interpretation}\label{coefficient-estimation-and-interpretation}}

The estimation of the logistic regression coefficients is not as intuitive as we saw in the linear regression model (regression lines and regression plane or surface). An advanced mathematical method needs to be used to estimate the regression coefficient. The R function \textbf{glm()} can be used to find the estimate of regression coefficients.

The interpretation of the coefficients of the logistic regression model is also not straightforward. In the motivational example, the value of \(\beta_1\) is the change of log odds of observing reproduction status to be 1. As usual, we will not make an inference from the intercept. In case-control data, the intercept is inestimable.

The output of \textbf{glm()} contains information similar to what has been seen in the output of \textbf{lm()} in the linear regression model.

\hypertarget{use-of-glm-and-annotations}{%
\subsection{\texorpdfstring{Use of \textbf{glm()} and Annotations}{Use of glm() and Annotations}}\label{use-of-glm-and-annotations}}

We use the motivational example to illustrate the setup of \textbf{glm()} and the interpretation of the output.

\begin{Shaded}
\begin{Highlighting}[]
\DocumentationTok{\#\# Fit the logistic regression}
\NormalTok{mymodel }\OtherTok{=}\FunctionTok{glm}\NormalTok{(IndReprod }\SpecialCharTok{\textasciitilde{}}\NormalTok{ Longevity,   }\CommentTok{\# model formula, the response variable is on the left side.}
          \AttributeTok{family=}\NormalTok{binomial,            }\CommentTok{\# this must be specified since the response is binary!}
          \AttributeTok{data=}\NormalTok{fruitflies)            }\CommentTok{\# data frame {-} the variables in the model MUST identical}
                                      \CommentTok{\# to the ones in the data frame!!!}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{img04/w04-glm-output} 

}

\caption{The complete output of glm()}\label{fig:unnamed-chunk-93}
\end{figure}

The output has four major pieces of information: model formula, five-number-summary of the deviance residuals, significant test results of the predictor variables, and goodness-of-fit measures. In this course, we only focus on the significance tests.

\hypertarget{applications-of-logistic-regression-models}{%
\subsection{Applications of Logistic Regression Models}\label{applications-of-logistic-regression-models}}

Like other regression models, logistic regression models have two basic applications: association analysis and prediction analysis.

The association analysis focuses on the interpretation of the regression coefficients that have information about whether predictor variables are associated with the response variable through the probability of the \textbf{bigger} value of the response variable.

Since the logistic regression builds the relationship between the probability of observing the \textbf{bigger} value of the response and the predictor variable, predicting the \textbf{value of the response variable} requires a cut-off probability in order to assign a value of 1 or 0 to the response variable. The prediction process of a logistic regression model is depicted in the following figure.

\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{img04/w04-LogisticPrediction} 

}

\caption{Prediction process of the logistic regression models}\label{fig:unnamed-chunk-94}
\end{figure}

\hypertarget{case-studies}{%
\section{Case Studies}\label{case-studies}}

We present two examples in this section.

\hypertarget{the-simple-logistic-regression-model}{%
\subsection{The simple logistic regression model}\label{the-simple-logistic-regression-model}}

Suzuki et al.~(2006) measured sand grain size on 28 beaches in Japan and observed the presence or absence of the burrowing wolf spider Lycosa ishikariana on each beach. Sand grain size is a measurement variable, and spider presence or absence is a nominal variable. Spider presence or absence is the dependent variable; if there is a relationship between the two variables, it would be sand grain size affecting spiders, not the presence of spiders affecting the sand.

\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{img04/w04-SpiderDataTable} 

}

\caption{Spider Data Table}\label{fig:unnamed-chunk-95}
\end{figure}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{grainsize}\OtherTok{=}\FunctionTok{c}\NormalTok{(}\FloatTok{0.245}\NormalTok{, }\FloatTok{0.247}\NormalTok{, }\FloatTok{0.285}\NormalTok{, }\FloatTok{0.299}\NormalTok{, }\FloatTok{0.327}\NormalTok{, }\FloatTok{0.347}\NormalTok{, }\FloatTok{0.356}\NormalTok{, }\FloatTok{0.360}\NormalTok{, }\FloatTok{0.363}\NormalTok{, }\FloatTok{0.364}\NormalTok{, }
            \FloatTok{0.398}\NormalTok{, }\FloatTok{0.400}\NormalTok{, }\FloatTok{0.409}\NormalTok{, }\FloatTok{0.421}\NormalTok{, }\FloatTok{0.432}\NormalTok{, }\FloatTok{0.473}\NormalTok{, }\FloatTok{0.509}\NormalTok{, }\FloatTok{0.529}\NormalTok{, }\FloatTok{0.561}\NormalTok{, }\FloatTok{0.569}\NormalTok{, }
            \FloatTok{0.594}\NormalTok{, }\FloatTok{0.638}\NormalTok{, }\FloatTok{0.656}\NormalTok{, }\FloatTok{0.816}\NormalTok{, }\FloatTok{0.853}\NormalTok{, }\FloatTok{0.938}\NormalTok{, }\FloatTok{1.036}\NormalTok{, }\FloatTok{1.045}\NormalTok{)}
\NormalTok{status}\OtherTok{=}\FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{, }
         \DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{)}
\NormalTok{spider }\OtherTok{=} \FunctionTok{as.data.frame}\NormalTok{(}\FunctionTok{cbind}\NormalTok{(}\AttributeTok{grainsize =}\NormalTok{ grainsize, }\AttributeTok{status =}\NormalTok{ status))}
\end{Highlighting}
\end{Shaded}

\textbf{Fitting a Simple Logistic Regression Model}

Since there is only one predictor variable in this study, simply choose the simple linear regression model for this data set.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spider.model }\OtherTok{=} \FunctionTok{glm}\NormalTok{(status }\SpecialCharTok{\textasciitilde{}}\NormalTok{ grainsize, }
                   \AttributeTok{family =}\NormalTok{ binomial,}
                   \AttributeTok{data =}\NormalTok{ spider)}
\NormalTok{significant.tests }\OtherTok{=} \FunctionTok{summary}\NormalTok{(spider.model)}\SpecialCharTok{$}\NormalTok{coef}
\FunctionTok{kable}\NormalTok{(significant.tests, }\AttributeTok{caption =} \StringTok{"Summary of the significant tests of }
\StringTok{      the logistic regression model"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{table}

\caption{\label{tab:unnamed-chunk-97}Summary of the significant tests of 
      the logistic regression model}
\centering
\begin{tabular}[t]{l|r|r|r|r}
\hline
  & Estimate & Std. Error & z value & Pr(>|z|)\\
\hline
(Intercept) & -1.647625 & 1.354191 & -1.216686 & 0.2237237\\
\hline
grainsize & 5.121553 & 3.006174 & 1.703678 & 0.0884413\\
\hline
\end{tabular}
\end{table}

\textbf{Association Analysis}

The above significant tests indicate that the grain size does not achieve significance (p-value = 0.08844) at level 0.05. Note that the p-value is calculated based on the sample, it is also a random variable. Moreover, the sample size in this study is relatively small. We will claim the association between the two variables. As the grain size increases by one unit, the log odds of observing the wolf spider burrowing increase by 5.121553. In other words, the grain size and the presence of spiders are positively associated.

\textbf{Prediction Analysis}

As an example, we choose two new grain sizes 0.33 and 0.57, and want to predict the presence of the wide spiders on the beaches with the given grain sizes. We used the R function \textbf{predict()} in linear regression, we use the same function to make predictions in the logistic regression model.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spider.model }\OtherTok{=} \FunctionTok{glm}\NormalTok{(status }\SpecialCharTok{\textasciitilde{}}\NormalTok{ grainsize, }
                   \AttributeTok{family =}\NormalTok{ binomial,}
                   \AttributeTok{data =}\NormalTok{ spider)}
\DocumentationTok{\#\# }
\NormalTok{mynewdata }\OtherTok{=} \FunctionTok{data.frame}\NormalTok{(}\AttributeTok{grainsize=}\FunctionTok{c}\NormalTok{(}\FloatTok{0.275}\NormalTok{, }\FloatTok{0.57}\NormalTok{))}
\NormalTok{pred.prob }\OtherTok{=} \FunctionTok{predict}\NormalTok{(spider.model, }\AttributeTok{newdata =}\NormalTok{ mynewdata, }
        \AttributeTok{type =} \StringTok{"response"}\NormalTok{)}
\DocumentationTok{\#\# threshold probability}
\NormalTok{cut.off.prob }\OtherTok{=} \FloatTok{0.5}
\NormalTok{pred.response }\OtherTok{=} \FunctionTok{ifelse}\NormalTok{(pred.prob }\SpecialCharTok{\textgreater{}}\NormalTok{ cut.off.prob, }\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{)  }\CommentTok{\# This predicts the response}
\NormalTok{pred.table }\OtherTok{=} \FunctionTok{cbind}\NormalTok{(}\AttributeTok{new.grain.size =} \FunctionTok{c}\NormalTok{(}\FloatTok{0.275}\NormalTok{, }\FloatTok{0.57}\NormalTok{), }\AttributeTok{pred.response =}\NormalTok{ pred.response)}
\FunctionTok{kable}\NormalTok{(pred.table, }\AttributeTok{caption =} \StringTok{"Predicted Value of response variable }
\StringTok{      with the given cut{-}off probability"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{table}

\caption{\label{tab:unnamed-chunk-98}Predicted Value of response variable 
      with the given cut-off probability}
\centering
\begin{tabular}[t]{r|r}
\hline
new.grain.size & pred.response\\
\hline
0.275 & 0\\
\hline
0.570 & 1\\
\hline
\end{tabular}
\end{table}

\hypertarget{multiple-logistic-regression-model}{%
\subsection{Multiple Logistic Regression Model}\label{multiple-logistic-regression-model}}

In this case study, we used a published study on bird introduction in New Zealand. The objective is to predict the success of avian introduction to New Zealand. Detailed information about the study can be found in the following article. \url{https://pengdsci.github.io/STA551/w04/Correlates-of-introduction-success-in-exotic.pdf}. The data is included in the article. A text format data file was created and can be downloaded or read directly from the following URL: \href{https://pengdsci.github.io/STA551/w04/img/birds-data.txt}{https://pengdsci.github.io/STA551/w04//img/birds-data.txt}.

\begin{center}\includegraphics[width=0.8\linewidth]{img04/w04-BirdImage} \end{center}

The response variable: Status - status of success
Predictor variables:

\begin{itemize}
\tightlist
\item
  length - female body length (mm)
\item
  mass = female body mass (g)
\item
  range = geographic range (\% area of Australia)
\item
  migr = migration score: 1= sedentary, 2 = sedentary and migratory, 3 = migratory
\item
  insect = the number of months in a year with insects in the diet
\item
  diet = diet score: 1 = herbivorous; 2 = omnivorous, 3 = carnivorous.
\item
  clutch = clutch size
\item
  broods = number of broods per season
\item
  wood = use as woodland scored as frequent(1) or infrequent(2)
\item
  upland = use of the upland as frequent(1) or infrequent(2)
\item
  water = use of water scored as frequent(1) or infrequent(2)
\item
  release = minimum number of release events
\item
  indiv = minimum of the number of individuals introduced.
\end{itemize}

We next read the data from the given URL directly to R. Since there are some records with missing values. We drop those records with at least one missing value.

There are several categorical variables coded in numerical form. Among them, \textbf{migr} and \textbf{diet} have three categories and the rest of the categorical variables have two categories. In practice, a \textbf{categorical variable with more than two categories must be specified as factor variables} so R can define dummy variables to capture the difference across the difference.

We conducted an exploratory analysis on \textbf{nigr} and \textbf{diet} and found a flat discrepancy across the effect. We simply treat them as discrete numerical variables using the numerical coding as the values of the variables.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{NZbirds}\OtherTok{=}\FunctionTok{read.table}\NormalTok{(}\StringTok{"https://pengdsci.github.io/STA551/w04/img/birds{-}data.txt"}\NormalTok{, }\AttributeTok{header=}\ConstantTok{TRUE}\NormalTok{)}
\NormalTok{birds }\OtherTok{=} \FunctionTok{na.omit}\NormalTok{(NZbirds)}
\end{Highlighting}
\end{Shaded}

\begin{itemize}
\tightlist
\item
  \textbf{Build an Initial Model}
\end{itemize}

We first build a logistic regression model that contains all predictor variables in the data set. This model is usually called the full model. Note that the response variable is the success status (1 = success, 0 = failure). Species is a kind of ID, it should not be included in the model.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{initial.model }\OtherTok{=} \FunctionTok{glm}\NormalTok{(status }\SpecialCharTok{\textasciitilde{}}\NormalTok{ length }\SpecialCharTok{+}\NormalTok{ mass }\SpecialCharTok{+}\NormalTok{ range }\SpecialCharTok{+}\NormalTok{ migr }\SpecialCharTok{+}\NormalTok{ insect }\SpecialCharTok{+}\NormalTok{ diet }\SpecialCharTok{+}\NormalTok{ clutch }\SpecialCharTok{+}\NormalTok{ broods }\SpecialCharTok{+} 
\NormalTok{                      wood }\SpecialCharTok{+}\NormalTok{ upland }\SpecialCharTok{+}\NormalTok{ water }\SpecialCharTok{+}\NormalTok{ release }\SpecialCharTok{+}\NormalTok{ indiv, }\AttributeTok{family =}\NormalTok{ binomial, }\AttributeTok{data =}\NormalTok{ birds)}
\NormalTok{coefficient.table }\OtherTok{=} \FunctionTok{summary}\NormalTok{(initial.model)}\SpecialCharTok{$}\NormalTok{coef}
\FunctionTok{kable}\NormalTok{(coefficient.table, }\AttributeTok{caption =} \StringTok{"Significance tests of logistic regression model"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{table}

\caption{\label{tab:unnamed-chunk-101}Significance tests of logistic regression model}
\centering
\begin{tabular}[t]{l|r|r|r|r}
\hline
  & Estimate & Std. Error & z value & Pr(>|z|)\\
\hline
(Intercept) & -6.3380099 & 5.7167615 & -1.1086714 & 0.2675720\\
\hline
length & -0.0028150 & 0.0053169 & -0.5294328 & 0.5965052\\
\hline
mass & 0.0026679 & 0.0016740 & 1.5937709 & 0.1109874\\
\hline
range & -0.1316071 & 0.3502344 & -0.3757688 & 0.7070888\\
\hline
migr & -2.0435447 & 1.1158239 & -1.8314222 & 0.0670376\\
\hline
insect & 0.1479915 & 0.2123645 & 0.6968752 & 0.4858809\\
\hline
diet & 2.0285053 & 1.8832013 & 1.0771580 & 0.2814097\\
\hline
clutch & 0.0793804 & 0.2683046 & 0.2958594 & 0.7673375\\
\hline
broods & 0.0217705 & 0.9283268 & 0.0234513 & 0.9812903\\
\hline
wood & 2.4902098 & 1.6416010 & 1.5169397 & 0.1292819\\
\hline
upland & -4.7134743 & 2.8648273 & -1.6452909 & 0.0999098\\
\hline
water & 0.2349442 & 2.6701934 & 0.0879877 & 0.9298864\\
\hline
release & -0.0129156 & 0.1932112 & -0.0668472 & 0.9467033\\
\hline
indiv & 0.0159265 & 0.0083240 & 1.9133152 & 0.0557077\\
\hline
\end{tabular}
\end{table}

The p-values in the above significance test table are all bigger than 0.5. We next search for the best model by dropping some of the insignificant predictor variables. Since there are so many different ways to drop variables, next we use an automatic variable procedure to search the final model.

\begin{itemize}
\tightlist
\item
  \textbf{Automatic Variable Selection}
\end{itemize}

R has an automatic variable selection function \textbf{step()} for searching the final model. We will start from the initial model and drop insignificant variables using AIC as an inclusion/exclusion criterion.

In practice, sometimes, there may be some practically important predictor variables. Practitioners want to include these practically important variables in the model regardless of their statistical significance. Therefore we can fit the smallest model that includes only those practically important variables. The final model should be \textbf{between} the smallest model, which we will call a \textbf{reduced model}, and the initial model, which we will call a \textbf{full model}. For illustration, we assume \textbf{insect} and \textbf{range} are practically important, we want to include these two variables in the final model regardless of their statistical significance.

In summary, we define two models: the full model and the reduced model. The final best model will be the model between the full and reduced models. The summary table of significant tests is given below.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{full.model }\OtherTok{=}\NormalTok{ initial.model  }\CommentTok{\# the *biggest model* that includes all predictor variables}
\NormalTok{reduced.model }\OtherTok{=} \FunctionTok{glm}\NormalTok{(status }\SpecialCharTok{\textasciitilde{}}\NormalTok{ range }\SpecialCharTok{+}\NormalTok{ insect , }\AttributeTok{family =}\NormalTok{ binomial, }\AttributeTok{data =}\NormalTok{ birds)}
\NormalTok{final.model }\OtherTok{=}  \FunctionTok{step}\NormalTok{(full.model, }
                    \AttributeTok{scope=}\FunctionTok{list}\NormalTok{(}\AttributeTok{lower=}\FunctionTok{formula}\NormalTok{(reduced.model),}\AttributeTok{upper=}\FunctionTok{formula}\NormalTok{(full.model)),}
                    \AttributeTok{data =}\NormalTok{ birds, }
                    \AttributeTok{direction =} \StringTok{"backward"}\NormalTok{,}
                    \AttributeTok{trace =} \DecValTok{0}\NormalTok{)   }\CommentTok{\# trace = 0: suppress the detailed selection process}
\NormalTok{final.model.coef }\OtherTok{=} \FunctionTok{summary}\NormalTok{(final.model)}\SpecialCharTok{$}\NormalTok{coef}
\FunctionTok{kable}\NormalTok{(final.model.coef , }\AttributeTok{caption =} \StringTok{"Summary table of significant tests"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{table}

\caption{\label{tab:unnamed-chunk-102}Summary table of significant tests}
\centering
\begin{tabular}[t]{l|r|r|r|r}
\hline
  & Estimate & Std. Error & z value & Pr(>|z|)\\
\hline
(Intercept) & -3.4376339 & 2.1379440 & -1.6079158 & 0.1078536\\
\hline
mass & 0.0019393 & 0.0007326 & 2.6470676 & 0.0081193\\
\hline
range & 0.0000141 & 0.3098265 & 0.0000456 & 0.9999636\\
\hline
migr & -2.0239952 & 0.9603017 & -2.1076659 & 0.0350599\\
\hline
insect & 0.2704685 & 0.1425911 & 1.8968119 & 0.0578528\\
\hline
wood & 1.9489529 & 1.3174130 & 1.4793789 & 0.1390391\\
\hline
upland & -4.7306337 & 2.0981447 & -2.2546746 & 0.0241538\\
\hline
indiv & 0.0138120 & 0.0040579 & 3.4037151 & 0.0006648\\
\hline
\end{tabular}
\end{table}

\begin{itemize}
\tightlist
\item
  \textbf{Interpretation - Association Analysis}
\end{itemize}

The summary table contains the two practically important variables \textbf{range} and \textbf{insect}. \textbf{range} does not achieve statistical significance (p-value \(\approx\) 1) and \textbf{insect} is slightly higher than the significance level of 0.005. Both variables are seemingly positively associated with the response variable.

The following interpretation of the individual predictor variable assumes other life-history variables and the introduction effort variable.

\textbf{migr} and \textbf{upland} are negatively associated with the response variable. The odds of success in introducing migratory birds are lower than the sedentary birds. Similarly, birds using upland infrequently have lower odds to be successfully introduced than those using upland frequently.

\textbf{insect} is significant (p-value =0.058). The \textbf{odds of success} increase as the number of months of having insects in the diet increases.

\textbf{mass} and \textbf{indiv} are positively associated with the response variable. The odds of success increase and the body mass increases Similarly, the odds of success increase as the number of minimum birds of the species increases.

\textbf{wood} does not achieve statistical significance but seemed to be positively associated with the response variable.

\begin{itemize}
\tightlist
\item
  \textbf{Predictive Analysis}
\end{itemize}

As an illustration, we use the final model to predict the status of successful introduction based on the new values of the predictor variables associated with two species. See the numerical feature given in the code chunk.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{mynewdata }\OtherTok{=} \FunctionTok{data.frame}\NormalTok{(}\AttributeTok{mass=}\FunctionTok{c}\NormalTok{(}\DecValTok{560}\NormalTok{, }\DecValTok{921}\NormalTok{),}
                       \AttributeTok{range =} \FunctionTok{c}\NormalTok{(}\FloatTok{0.75}\NormalTok{, }\FloatTok{1.2}\NormalTok{),}
                       \AttributeTok{migr =} \FunctionTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{,}\DecValTok{1}\NormalTok{),}
                       \AttributeTok{insect =} \FunctionTok{c}\NormalTok{(}\DecValTok{6}\NormalTok{, }\DecValTok{12}\NormalTok{),}
                       \AttributeTok{wood =} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{),}
                       \AttributeTok{upland =} \FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{1}\NormalTok{),}
                       \AttributeTok{indiv =} \FunctionTok{c}\NormalTok{(}\DecValTok{123}\NormalTok{, }\DecValTok{571}\NormalTok{))}
\NormalTok{pred.success.prob }\OtherTok{=} \FunctionTok{predict}\NormalTok{(final.model, }\AttributeTok{newdata =}\NormalTok{ mynewdata, }\AttributeTok{type=}\StringTok{"response"}\NormalTok{)}
\DocumentationTok{\#\#}
\DocumentationTok{\#\# threshold probability}
\NormalTok{cut.off.prob }\OtherTok{=} \FloatTok{0.5}
\NormalTok{pred.response }\OtherTok{=} \FunctionTok{ifelse}\NormalTok{(pred.success.prob }\SpecialCharTok{\textgreater{}}\NormalTok{ cut.off.prob, }\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{)  }\CommentTok{\# This predicts the response}
\DocumentationTok{\#\# Add the new predicted response to Mynewdata}
\NormalTok{mynewdata}\SpecialCharTok{$}\NormalTok{Pred.Response }\OtherTok{=}\NormalTok{ pred.response}
\DocumentationTok{\#\#}
\FunctionTok{kable}\NormalTok{(mynewdata, }\AttributeTok{caption =} \StringTok{"Predicted Value of response variable }
\StringTok{      with the given cut{-}off probability"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{table}

\caption{\label{tab:unnamed-chunk-103}Predicted Value of response variable 
      with the given cut-off probability}
\centering
\begin{tabular}[t]{r|r|r|r|r|r|r|r}
\hline
mass & range & migr & insect & wood & upland & indiv & Pred.Response\\
\hline
560 & 0.75 & 2 & 6 & 1 & 0 & 123 & 0\\
\hline
921 & 1.20 & 1 & 12 & 1 & 1 & 571 & 1\\
\hline
\end{tabular}
\end{table}

The predicted status of the successful introduction of the two species is attached to the two new records.

\hfill\break

\hypertarget{basics-of-bootstrap-method}{%
\chapter{Basics of Bootstrap Method}\label{basics-of-bootstrap-method}}

The bootstrap method is a data-based simulation method for statistical inference. The method assumes that

\begin{itemize}
\item
  The sample is a random sample representing the population;
\item
  The sample size is large enough such that the empirical distribution can be close to the true distribution.
\end{itemize}

\hypertarget{basic-idea-of-bootstrap-method.}{%
\section{Basic Idea of Bootstrap Method.}\label{basic-idea-of-bootstrap-method.}}

The objective is to estimate a population parameter such as mean, variance, correlation coefficient, regression coefficients, etc. from a random sample without assuming any probability distribution of the underlying distribution of the population.

For convenience, we assume that the population of interest has a cumulative distribution function \(F(x: \theta)\), where \(\theta\) is a vector of the population. For example, You can think about the following distributions

\begin{itemize}
\item
  \textbf{Normal distribution}: \(N(\mu, \sigma^2)\), the distribution function is given by

  \[f(x:\theta) = \frac{1}{\sqrt{2\pi}\sigma}\exp\left(-\frac{(x-\mu)^2}{2\sigma^2}\right)\]
  where \(\theta = (\mu, \sigma)\). Since the normal distribution is so fundamental in statistics, we use the special notation for the cumulative distribution \(\phi_{\mu, \sigma^2}(x)\) or simply \(\phi(x)\). The corresponding probability function
\item
  \textbf{Binomial distribution}: \(Binom(n, p)\), the probability distribution is given by
\end{itemize}

\[ P(x) = \frac{n!}{x!(n-x)!}p^x(1-p)^{n-x}, x = 0, 1, 2, \cdots, n-1, n.\]
where \(\theta = p\). \emph{Caution}: \(n\) is NOT a parameter!

We have already learned how to make inferences about population means and variances under various assumptions in elementary statistics. In this note, we introduce a \textbf{new approach} to making inferences only based on a given random sample taken from the underlying population.

As an example, we focus on the population mean. For other parameters, we can follow the same idea to make bootstrap inferences.

\hypertarget{random-sample-from-population}{%
\subsection{Random Sample from Population}\label{random-sample-from-population}}

We have introduced various study designs and sampling plans to obtain random samples from a given population with the distribution function \(F(x:\theta)\). Let \(\mu\) be the population mean.

\begin{itemize}
\tightlist
\item
  \textbf{Random Sample}. Let
\end{itemize}

\[\{x_1, x_2, \cdots, x_n\} \to F(x:\theta)\]
be a random sample from population \(F(x:\theta)\).

\begin{itemize}
\tightlist
\item
  \textbf{Sample Mean}. The point estimate is given by
\end{itemize}

\[\hat{\mu} = \frac{\sum_{i=1}^n x_i}{n}\]

\begin{itemize}
\item
  \textbf{Sampling Distribution of \(\hat{\mu}\)}. In order to construct the confidence interval of \(\mu\) or make hypothesis testing about \(\mu\), we need to know the sampling distribution of \(\hat{\mu}\). From elementary statistics, we have the following results.

  \begin{itemize}
  \item
    \(\hat{\mu}\) is normally distributed if (1). \(n\) is large; or (2). the population is normal and population variance is known.
  \item
    the standardized \(\hat{\mu}\) follows a t-distribution if the population is normal and population variance is unknown.
  \item
    \(\hat{\mu}\) is \textbf{unknown} of the population is not normal and the sample size is not large enough.
  \end{itemize}
\item
  In the last case of the previous bullet point, we don't have the theory to derive the sampling distribution based on a \textbf{single} sample. However, if the sampling is not too expensive and time-consuming, we take following the sample study design and sampling plan to repeatedly take a large number, 1000, samples of the same size from the population. We calculate the mean of each of the 1000 samples and obtain 1000 sample means \(\{\hat{\mu}_1, \hat{\mu}_2, \cdots, \hat{\mu}_{1000}\}\). Then the empirical distribution of \(\hat{\mu}\).
\end{itemize}

The following figure depicts how to approximate the sampling distribution of the point estimator of the population parameter.

\begin{figure}

{\centering \includegraphics[width=0.6\linewidth]{img05/w05-ApproxSamplingDist} 

}

\caption{Figure 1. Steps for estimating the sampling distribution of a point estimator of the population parameter}\label{fig:unnamed-chunk-105}
\end{figure}

\textbf{Example 1:} {[}\textbf{Simulated data}{]} Assume that the particular numeric characteristics of the WCU student population are the heights of all students.

\begin{itemize}
\item
  We don't know the distribution of the heights.
\item
  We also don't know \emph{whether a specific sample size is large enough to use the central limit theorem}. This means we don't know whether it is appropriate to use the central limit theorem to characterize the sampling distribution of the mean height.
\end{itemize}

Due to the above constraints, we cannot find the sampling distribution of the sample mean using only the knowledge of elementary statistics. However, if sampling is not expensive, we take repeated samples with the same sample size. The resulting sample means can be used to approximate the sampling distribution of the sample mean.

Next, we use R and the simulated data set \url{https://pengdsci.github.io/STA551/w05/w05-wcuheights.txt} to implement the above idea. I will use simple code with comments to explain the task of each line of code so you can easily understand the coding logic.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# read the delimited data from URL}
\NormalTok{wcu.height }\OtherTok{=} \FunctionTok{read.table}\NormalTok{(}\StringTok{"https://pengdsci.github.io/STA551/w05/w05{-}wcuheights.txt"}\NormalTok{, }\AttributeTok{header =} \ConstantTok{TRUE}\NormalTok{)}
\NormalTok{sample.mean.vec }\OtherTok{=} \ConstantTok{NULL}      \CommentTok{\# define an empty vector to hold sample means of repeated samples.}
\ControlFlowTok{for}\NormalTok{(i }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\DecValTok{1000}\NormalTok{)\{           }\CommentTok{\# starting for{-}loop to take repeated random samples with n = 81}
\NormalTok{  ith.sample }\OtherTok{=} \FunctionTok{sample}\NormalTok{( wcu.height}\SpecialCharTok{$}\NormalTok{Height,       }\CommentTok{\# population of all WCU students heights}
                       \DecValTok{81}\NormalTok{,                      }\CommentTok{\# sample size = 81 values in the sample}
                       \AttributeTok{replace =} \ConstantTok{FALSE}          \CommentTok{\# sample without replacement}
\NormalTok{                 )                              }\CommentTok{\# this is the i{-}th random sample}
\NormalTok{   sample.mean.vec[i] }\OtherTok{=} \FunctionTok{mean}\NormalTok{(ith.sample)        }\CommentTok{\# calculate the mean of i{-}th sample and save it in}
                                                \CommentTok{\# the empty vector: sample.mean.vec }
\NormalTok{  \}}
\end{Highlighting}
\end{Shaded}

Next, we make a histogram of the sample means saved in \textbf{sample.mean.vec}.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{hist}\NormalTok{(sample.mean.vec,                                               }\CommentTok{\# data used for histogram}
     \AttributeTok{breaks =} \DecValTok{14}\NormalTok{,                                                   }\CommentTok{\# specify the number of vertical bars}
     \AttributeTok{xlab =} \StringTok{"sample means of repeated samples"}\NormalTok{,                     }\CommentTok{\# change the label of x{-}axis}
     \AttributeTok{main=}\StringTok{"Approximated Sampling Distribution of the Sample Mean"}\NormalTok{)   }\CommentTok{\# add a title to the histogram}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics{STA551EB_files/figure-latex/unnamed-chunk-107-1} 

}

\caption{Figure 2. Approximated sampling distribution of sample mean used the repeated samples.}\label{fig:unnamed-chunk-107}
\end{figure}

\hypertarget{bootstrap-sampling-and-bootstrap-sampling-distribution}{%
\subsection{Bootstrap Sampling and Bootstrap Sampling Distribution}\label{bootstrap-sampling-and-bootstrap-sampling-distribution}}

Recall the situation in \textbf{Example 1} in which we were not able to use the normality assumption of the population and the central limit theorem (CLT) but were allowed to take repeated samples from the population. In practice, taking samples from the population can be very expensive. Is there any way to estimate the sampling distribution of the sample mean? The answer is YES under the assumption the sample yields a valid estimation of the original population distribution.

\begin{itemize}
\item
  \textbf{Bootstrap Sampling} With the assumption that the sample yields a good approximation of the population distribution, we can take bootstrap samples from the \textbf{actual} sample. Let
  \[\{x_1, x_2, \cdots, x_n\} \to F(x:\theta)\] be the actual random sample taken from the population. A \textbf{bootstrap sample} is obtained by taking a sample \textbf{with replacement} from the original data set (not the population!) with the same size as the original sample. Because \textbf{with replacement} was used, some values in the bootstrap sample appear once, some twice, and so on, and some do not appear at all.
\item
  \textbf{Notation of Bootstrap Sample}. We use \(\{x_1^{(i*)}, x_2^{(i*)}, \cdots, x_n^{(i*)}\}\) to denote the \(i^{th}\) bootstrap sample. Then the corresponding mean is called bootstrap sample mean and denoted by \(\hat{\mu}_i^*\), for \(i = 1, 2, ..., n\).
\item
  \textbf{Bootstrap sampling distribution} of the sample mean can be estimated by taking a large number, say B, of bootstrap samples. The resulting B bootstrap sample means are used to estimate the sampling distribution. Note that, in practice, B is bigger than 1000.
\end{itemize}

The above Bootstrap sampling process is illustrated in the following figure.

\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{img05/w05-BootSamplingDist} 

}

\caption{Steps for the Bootstrap sampling distribution of a point estimator of the population parameter}\label{fig:unnamed-chunk-108}
\end{figure}

\begin{itemize}
\tightlist
\item
  \textbf{Example 2:} {[}\textbf{continue to use WCU Heights}{]}. We use the Bootstrap method to estimate the sampling distribution of the sample mean.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\DocumentationTok{\#\#\# Read the delimited data from the URL}
\NormalTok{wcu.height }\OtherTok{=} \FunctionTok{read.table}\NormalTok{(}\StringTok{"https://pengdsci.github.io/STA551/w05/w05{-}wcuheights.txt"}\NormalTok{, }\AttributeTok{header =} \ConstantTok{TRUE}\NormalTok{)}
\CommentTok{\# taking the original random sample from the population}
\NormalTok{original.sample }\OtherTok{=} \FunctionTok{sample}\NormalTok{( wcu.height}\SpecialCharTok{$}\NormalTok{Height,    }\CommentTok{\# population of all WCU students heights}
                       \DecValTok{81}\NormalTok{,                      }\CommentTok{\# sample size = 81 values in the sample}
                       \AttributeTok{replace =} \ConstantTok{FALSE}          \CommentTok{\# sample without replacement}
\NormalTok{                 )                              }
\DocumentationTok{\#\#\# Bootstrap sampling begins }
\NormalTok{bt.sample.mean.vec }\OtherTok{=} \ConstantTok{NULL}      \CommentTok{\# define an empty vector to hold sample means of repeated samples.}
\ControlFlowTok{for}\NormalTok{(i }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\DecValTok{1000}\NormalTok{)\{              }\CommentTok{\# starting for{-}loop to take bootstrap samples with n = 81}
\NormalTok{  ith.bt.sample }\OtherTok{=} \FunctionTok{sample}\NormalTok{( original.sample,    }\CommentTok{\# Original sample with 81 WCU students\textquotesingle{} heights}
                       \DecValTok{81}\NormalTok{,                    }\CommentTok{\# sample size = 81 MUST be equal to the sample size!!}
                       \AttributeTok{replace =} \ConstantTok{TRUE}         \CommentTok{\# MUST use WITH REPLACEMENT!!}
\NormalTok{                 )                            }\CommentTok{\# This is the i{-}th Bootstrap sample}
\NormalTok{  bt.sample.mean.vec[i] }\OtherTok{=} \FunctionTok{mean}\NormalTok{(ith.bt.sample) }\CommentTok{\# calculate the mean of i{-}th bootstrap sample and }
                                              \CommentTok{\# save it in the empty vector: sample.bt.mean.vec }
\NormalTok{  \}}
\end{Highlighting}
\end{Shaded}

The following histogram shows the bootstrap sampling distribution of the sample with size \(n=81\).

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{hist}\NormalTok{(bt.sample.mean.vec,                                         }\CommentTok{\# data used for histogram}
     \AttributeTok{breaks =} \DecValTok{14}\NormalTok{,                                                }\CommentTok{\# specify the number of vertical bars}
     \AttributeTok{xlab =} \StringTok{"Bootstrap sample means"}\NormalTok{,                            }\CommentTok{\# change the label of x{-}axis}
     \AttributeTok{main=}\StringTok{"Bootstrap Sampling Distribution of the Sample Mean"}\NormalTok{)   }\CommentTok{\# add a title to the histogram}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics{STA551EB_files/figure-latex/unnamed-chunk-110-1} 

}

\caption{Figure 4. Bootstrap sampling distribution of sample means}\label{fig:unnamed-chunk-110}
\end{figure}

\hypertarget{relationship-between-two-estimated-sampling-distributions}{%
\subsection{Relationship between Two Estimated Sampling Distributions}\label{relationship-between-two-estimated-sampling-distributions}}

We can see that the two sampling distributions are slightly different. If we are allowed to take repeated samples from the population, we should always use the repeated sample approach since it yields a better estimate of the true sampling distribution.

The bootstrap estimate of the sampling distribution is used when no theoretical confidence intervals are unavailable and the repeated sample is not possible. This does not mean that the bootstrap methods do not have limitations. In fact, the implicit assumption of the bootstrap method is that \textbf{the original sample has enough information to estimate the true population distribution}.

\hypertarget{bootstrap-confidence-intervals}{%
\section{Bootstrap Confidence Intervals}\label{bootstrap-confidence-intervals}}

First of all, all bootstrap confidence intervals are constructed based on the bootstrap sampling distribution of the underlying point estimator of the parameter of interest.

There are at least five different bootstrap confidence intervals. You can find these definitions from Chapter 4 of Roff's eBook \url{https://ebookcentral.proquest.com/lib/wcupa/reader.action?docID=261114\&ppg=7} (need WCU login credential to access the book). We only focus on the percentile method in which we simply define the confidence limit(s) by using the corresponding percentile(s) of the bootstrap estimates of the parameter of interest. R has a built-in function, \textbf{quantile()}, to find percentiles.

\begin{itemize}
\tightlist
\item
  \textbf{Example 3}: We construct a 95\% two-sided bootstrap percentile confidence interval of the mean height of WCU students. This is equivalent to finding the 2.5\% and 97.5\% percentiles. We use the following one-line code.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{CI }\OtherTok{=} \FunctionTok{quantile}\NormalTok{(bt.sample.mean.vec, }\FunctionTok{c}\NormalTok{(}\FloatTok{0.025}\NormalTok{, }\FloatTok{0.975}\NormalTok{))}
\NormalTok{CI}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##     2.5%    97.5% 
## 68.55556 70.45710
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#kable(CI, caption = "95\% bootstrap percentile confidence interval of the mean height")}
\end{Highlighting}
\end{Shaded}

Various bootstrap methods were implemented in the R library \textbf{\{boot\}}. UCLA Statistical Consulting \url{https://stats.idre.ucla.edu/r/faq/how-can-i-generate-bootstrap-statistics-in-r/} has a nice tutorial on bootstrap confidence intervals. You can use the built-in function \textbf{boot.ci()} to find all 5 bootstrap confidence intervals after you create the boot object. I will leave it to you if you want to explore more about the library.

\hypertarget{bootstrap-confidence-interval-of-correlation-coefficient}{%
\section{Bootstrap Confidence Interval of Correlation Coefficient}\label{bootstrap-confidence-interval-of-correlation-coefficient}}

As a case study, we will illustrate one bootstrap method to sample a random sample with multiple variables and use the bootstrap samples to calculate the corresponding bootstrap correlation coefficient. The bootstrap percentile confidence interval of the correlation coefficient.

\hypertarget{bootstrapping-data-set}{%
\subsection{Bootstrapping Data Set}\label{bootstrapping-data-set}}

There are different ways to take bootstrap samples. \textbf{The key point is that we cannot sample individual variables in the data frame separately to avoid mismatching!} The method we introduce here is also called bootstrap sampling cases. Here are the basic steps:

\begin{itemize}
\item
  We define the vector of row \(ID\). That is, \(ID = \{1, 2, 3, ..., n\}\).
\item
  Take a bootstrap sample from \(ID\) (i.e., sampling with replacement) with same size = n, denoted by \(ID^*\). As commented earlier, there will be replicates in \(ID^*\) and some values in \(ID\) are not in \(ID^*\).
\item
  Use \(ID^*\) to select the corresponding rows to form a bootstrap sample and then perform bootstrap analysis.
\end{itemize}

Here is an example of taking the bootstrap sample from the original sample with multiple variables. The data set we use here is well-known and is available at \url{https://pengdsci.github.io/STA551/w05/w05-iris.txt}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# read data into R from URL: }
\NormalTok{iris }\OtherTok{=} \FunctionTok{read.table}\NormalTok{(}\StringTok{"https://pengdsci.github.io/STA551/w05/w05{-}iris.txt"}\NormalTok{, }\AttributeTok{header =} \ConstantTok{TRUE}\NormalTok{)}
\NormalTok{n }\OtherTok{=} \FunctionTok{dim}\NormalTok{(iris)[}\DecValTok{1}\NormalTok{]        }\CommentTok{\# returns the dimension of the data frame, 1st component is the number of rows            }
\NormalTok{bt.ID }\OtherTok{=} \FunctionTok{sample}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\NormalTok{n, }\AttributeTok{replace =} \ConstantTok{TRUE}\NormalTok{)   }\CommentTok{\# bootstrap IDs, MUST use replacement method!}
\FunctionTok{sort}\NormalTok{(bt.ID)                 }\CommentTok{\# check the content of bt.ID. I sort the bt.ID to see the replicate easily}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##   [1]   1   2   2   2   5   6   6   7  10  11  13  13  15  16  19  19  20  21  22  22  23  24  24  24  24  27  29  29
##  [29]  31  31  34  36  37  38  38  39  40  40  41  43  44  44  45  45  46  47  47  47  49  50  50  51  52  53  54  56
##  [57]  57  59  59  60  61  62  64  65  65  67  67  68  71  71  72  75  77  79  80  81  82  83  84  84  86  86  86  88
##  [85]  89  90  91  91  92  92  92  92  93  95  96  96  96  97  97  98  98 100 101 101 102 105 107 109 110 110 112 113
## [113] 113 118 118 118 119 120 121 121 124 125 126 128 129 129 130 130 135 135 135 136 136 136 137 138 138 140 141 141
## [141] 141 141 142 143 144 145 145 146 148 149
\end{verbatim}

Next, we use the above bt.ID to take the bootstrap sample from the original data set \textbf{iris}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{bt.iris }\OtherTok{=}\NormalTok{ iris[bt.ID,]   }\CommentTok{\# taking bootstrap cases (or rows, records) using the bt.ID}
\NormalTok{bt.iris                  }\CommentTok{\# display the bootstrap sample}
\end{Highlighting}
\end{Shaded}

\hypertarget{confidence-interval-of-coefficient-correlation}{%
\subsection{Confidence Interval of Coefficient Correlation}\label{confidence-interval-of-coefficient-correlation}}

In this section, we construct a 95\% bootstrap percentile confidence interval for the coefficient correlation between the SepalLength and SepalWidth given in \textbf{iris}. Note that R built-in function \textbf{cor(x,y)} can be used to calculate the bootstrap correlation coefficient directly. The R code for constructing the bootstrap confidence interval for the coefficient correlation is given below.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{iris }\OtherTok{=} \FunctionTok{read.table}\NormalTok{(}\StringTok{"https://pengdsci.github.io/STA551/w05/w05{-}iris.txt"}\NormalTok{, }\AttributeTok{header =} \ConstantTok{TRUE}\NormalTok{)}
\NormalTok{n }\OtherTok{=} \FunctionTok{dim}\NormalTok{(iris)[}\DecValTok{1}\NormalTok{]        }\CommentTok{\# returns the dimension of the data frame, 1st component is the number of rows            }
\DocumentationTok{\#\#}
\NormalTok{bt.cor.vec }\OtherTok{=} \ConstantTok{NULL}    \CommentTok{\# empty vector bootstrap correlation coefficients}
\ControlFlowTok{for}\NormalTok{ (i }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\DecValTok{5000}\NormalTok{)\{   }\CommentTok{\# this time I take 5000 bootstrap samples for this example.}
\NormalTok{  bt.ID.i }\OtherTok{=} \FunctionTok{sample}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\NormalTok{n, }\AttributeTok{replace =} \ConstantTok{TRUE}\NormalTok{)  }\CommentTok{\# bootstrap IDs, MUST use replacement method!}
\NormalTok{  bt.iris.i }\OtherTok{=}\NormalTok{ iris[bt.ID.i, ]            }\CommentTok{\# i{-}th bootstrap ID}
\NormalTok{  bt.cor.vec[i] }\OtherTok{=} \FunctionTok{cor}\NormalTok{(bt.iris.i}\SpecialCharTok{$}\NormalTok{SepalLength, bt.iris.i}\SpecialCharTok{$}\NormalTok{SepalWidth)   }\CommentTok{\# i{-}th bootstrap correlation coefficient}
\NormalTok{\}}
\NormalTok{bt.CI }\OtherTok{=} \FunctionTok{quantile}\NormalTok{(bt.cor.vec, }\FunctionTok{c}\NormalTok{(}\FloatTok{0.025}\NormalTok{, }\FloatTok{0.975}\NormalTok{) )}
\NormalTok{bt.CI}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##       2.5%      97.5% 
## -0.2558453  0.0376732
\end{verbatim}

\textbf{Interpretation:} We are 95\% confident that there is no statistically significant correlation between sepal length and sepal width based on the given sample. This may be because the data set contains three different types of iris.

Next, we make two plots to visualize the relationship between the two variables.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{par}\NormalTok{(}\AttributeTok{mfrow=}\FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{))   }\CommentTok{\# Layout a plot sheet: 1 row and 2 columns}
\DocumentationTok{\#\# histogram}
\FunctionTok{hist}\NormalTok{(bt.cor.vec, }\AttributeTok{breaks =} \DecValTok{14}\NormalTok{, }
       \AttributeTok{main=}\StringTok{"Bootstrap Sampling }\SpecialCharTok{\textbackslash{}n}\StringTok{  Distribution of Correlation"}\NormalTok{,}
      \AttributeTok{xlab =} \StringTok{"Bootstrap Correlation Coefficient"}\NormalTok{)}
\DocumentationTok{\#\# scatter plot}
\FunctionTok{plot}\NormalTok{(iris}\SpecialCharTok{$}\NormalTok{SepalLength, iris}\SpecialCharTok{$}\NormalTok{SepalWidth,}
     \AttributeTok{main =} \StringTok{"Sepal Length vs Width"}\NormalTok{,}
     \AttributeTok{xlab =} \StringTok{"Sepal Length"}\NormalTok{,}
     \AttributeTok{ylab =} \StringTok{"Sepal Width"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics{STA551EB_files/figure-latex/unnamed-chunk-115-1} 

}

\caption{Left panel: histogram of the bootstrap coefficient of correlation. Right panel: the scatter plot of the sepal length and width.}\label{fig:unnamed-chunk-115}
\end{figure}

\hfill\break

\hypertarget{method-of-cross-validation}{%
\chapter{Method of Cross-Validation}\label{method-of-cross-validation}}

In classical statistical modeling, we select candidate models based on exploratory data analysis and visual analysis. The candidate models are then fit to the analytic data set. Since all models have some sort of assumptions (think about linear regression and logistic and Poisson regression models), and then carry out model diagnostic analyses to identify potential violations of the model assumption. Of cause, we assume that the data represent the population. This modeling process is depicted in the following diagram.

\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{img06/w06-StatModelProcess} 

}

\caption{Statistical modeling process.}\label{fig:unnamed-chunk-117}
\end{figure}

\hypertarget{regression-models}{%
\section{Regression Models}\label{regression-models}}

Recall the process of the following two representative statistical models

\hypertarget{linear-regression-model}{%
\subsection{Linear Regression Model}\label{linear-regression-model}}

First, we look at the process of how to build a linear regression model for a data set.

\begin{itemize}
\item
  \textbf{Explicit and Implicit Assumptions}. The following are some of the assumptions about a normal-based linear regression model:

  \begin{itemize}
  \tightlist
  \item
    Predictor (feature) variables are (linearly or curve-linearly) correlated to the response variable (also called label in machine learning terms);
  \item
    Predictor variables themselves are NOT highly linearly correlated (i.e., no serious collinearity);
  \item
    The response variable is normally distributed;
  \item
    The variance of the response variable is constant;
  \item
    There are no outlier (influential) observations;
  \item
    information on all relevant predictor variables is available in the data (i.e., no important predictor is missing);
  \end{itemize}
\item
  \textbf{Model Fitting (Parameter Estimating)}. The least-square estimator (LSE), which is equivalent to the maximum likelihood estimator (MLE) when the response variable is assumed to be a normal distribution, can be used to estimate the regression coefficients.
\item
  \textbf{Model Selection and Diagnostics}. Since several implicit and explicit assumptions have been assumed underlying the linear regression, different sets of diagnostic measures were developed to detect different potential violations of the model.

  \begin{itemize}
  \tightlist
  \item
    \emph{global goodness-of-fit}: R2, AIC and SBC (requires normality assumption of the response variable), MSE, etc.
  \item
    \emph{local goodness-of-fit/model selection}: R, F-test (need normality and equal variance of the response variable), t-test, likelihood ratio test, Mallow's Cp, etc.
  \item
    \emph{normality}: QQ-plot and probability plot for a visual check, goodness-of-fit tests such as Anderson-Darling, Cramer-von Miss, Kolmogorov-Smirnov, Shapiro-Wilks, and Pearson's chi-square tests, etc.
  \item
    \emph{detecting outliers/influential observations}: leverage point-hat matrix, DFITT - defined based on leave-one-out resampling method, cook's distance, (scaled) residual plots, etc.
  \item
    \emph{verifying constant variance}: F-test (requires normality), Brown-Forsythe test (nonparametric), Breusch-Pagan Test (also nonparametric), Bartlett's Test (requires normality), etc.
  \item
    \emph{detecting collinearity}: Variance inflation factor (VIF) for data-based and structural collinearity.
  \item
    \emph{detecting mission of determinant variable}:
  \end{itemize}
\end{itemize}

\hypertarget{logistic-regression-model}{%
\subsection{Logistic Regression Model}\label{logistic-regression-model}}

For the binary logistic model, we also follow the same steps to identify the final model. For example, the well-known binary logistic regression modeling follows similar steps:

\begin{itemize}
\item
  \textbf{Assumptions}: Unlike the linear regression model which has a strong assumption of normality, the logistic regression model assumes the following

  \begin{itemize}
  \tightlist
  \item
    \emph{binomial distribution}: The dependent variable is binary.
  \item
    \emph{independence}: The logistic regression requires the observations to be independent of each other.
  \item
    \emph{collinearity}: The logistic regression requires there to be little or no multicollinearity among the independent variables.
  \item
    \emph{linearity}: The logistic regression assumes linearity of independent variables and the log odds of the event of interest.
  \item
    \emph{large sample size}: The logistic regression typically requires a large sample size. A general guideline is that you need a minimum of 10 observations with the least frequent outcome for each independent variable in your model.
  \item
    \emph{mis-specification}: No important variables are omitted. No extraneous variables are included.
  \item
    \emph{measurement error}: The independent variables are measured without error.
  \end{itemize}
\item
  \textbf{Model Fitting}: The coefficients of the logistic regression model are estimated using a maximum likelihood estimator. Note LSE cannot be estimated for the logistic regression model.
\item
  \textbf{Model Selection and Diagnostics}: Unlike the normal linear regression model, there are a few diagnostic methods one can use in logistic regression models.

  \begin{itemize}
  \tightlist
  \item
    \emph{misspecification}: link test (large sample test);
  \item
    \emph{goodness-of-fit}: log-likelihood chi-square and pseudo-R-square; Hosmer-Lemeshow's lack of fit test AIC and SBC.
  \item
    \emph{multi-collinearity}: VIF
  \item
    \emph{influential points}: Cook's distance, DBETA, deviance residuals
  \end{itemize}
\end{itemize}

\hypertarget{inference-about-association-and-prediction}{%
\subsection{Inference about Association and Prediction}\label{inference-about-association-and-prediction}}

In classical statistics, most problems are related to the significance of the regression coefficients. The p-values associated with corresponding regression coefficients are calculated based on certain assumptions. Prediction intervals are also constructed based on certain assumptions.

We can also use resampling methods to relax the assumptions about the distribution of the response variable to make inferences about the regression coefficients and construct prediction intervals.

In practice, the prediction problems usually involve both classical statistical models and machine learning algorithms. The model selection process and performance evaluation in classical statistical modeling is dependent on model assumptions, while in machine learning algorithms, model selection and evaluation use data-driven methods that are also valid for modern statistical modeling.

In the next few sections, we will introduce the data-driven methods for machine learning algorithms and statistical models.

\hypertarget{data-splitting-methods}{%
\section{Data Splitting Methods}\label{data-splitting-methods}}

When training and testing machine learning and statistical models without assuming strong assumptions (particularly the distributional information), we break down the data into three separate and distinct data sets: training data, validation data, and testing data. The basic idea is depicted in the following chart.

\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{img06/w06-ThreeWaySplitting} 

}

\caption{The-way data splitting method.}\label{fig:unnamed-chunk-118}
\end{figure}

\hypertarget{training-data}{%
\subsection{Training Data}\label{training-data}}

The training data set is the sample of data used to fit the model. In other words, the training data teaches the model how it's supposed to learn and think. To do this, training data is often presented in pairs: predictor variables (feature variables) and a response variable (also called a label).

Training data is the first set of data the model/algorithm is exposed to. During each stage of training, the model will be fit to the training data and estimate the parameters (also called weight in some machine learning algorithms such as neural networks).

Because the training data set is used to estimate the parameters (i.e., teaching the algorithm), it requires a certain amount of information to make the algorithms and models reliable. It makes up between 60\% and 80\% of the total data.

\hypertarget{validation-data}{%
\subsection{Validation Data}\label{validation-data}}

The validation data set is a sample of data held back from training the model. This data set provides an unbiased evaluation of a model fit on the training data set while tuning model hyperparameters. In more basic terms, validation data is an unused portion of your training data and helps determine if the initial model is accurate.

A model \textbf{hyperparameter} is a configuration that is external to the model and whose value cannot be estimated from data.

\begin{itemize}
\tightlist
\item
  They are often used in processes to help estimate model parameters.
\item
  They are often specified by the practitioner.
\item
  They can often be set using heuristics.
\item
  They are often tuned for a given predictive modeling problem.
\end{itemize}

We cannot know the best value for a model hyperparameter on a given problem. We may use rules of thumb, copy values used on other problems, or search for the best value by trial and error.

When a machine learning algorithm is tuned for a specific problem, such as the cut-off probability determination in the logistic prediction, then we are tuning the hyperparameters of the model or order to discover the parameters of the model that result in the most skillful predictions. The validation data helps tune a machine-learning model while checking for overfitting.

Overfitting happens when the model/algorithm is too closely fitted to the training data --- producing results tied to the specifics of that first data set. After validation, the team will often return to the training data and run it again, making adjustments to values and parameters to improve the model.

\hypertarget{test-data}{%
\subsection{Test data}\label{test-data}}

The test data set is a sample of data used to provide an unbiased evaluation of a final model fit on the training data set or to test the model. Put more simply, test data is a set of \textbf{unlabeled inputs} (i.e., the response value is removed from the data) that test whether the model is producing the correct outputs in the real world.

The key difference between a validation data set and a test data set is that the validation data set is used during model configuration, while the test data set is reserved to evaluate the final model.

Test data is about 20\% of the total data and should be completely separate from the training data --- which our model should know very well by this point.

In summary, the following chart gives an example of three-way splitting data with a sample configuration of the sub-set sizes.

\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{img06/w06-ThreeWaySplittingSize} 

}

\caption{The-way data splitting method with an example of sub-sample sizes.}\label{fig:unnamed-chunk-119}
\end{figure}

\hypertarget{cross-validation}{%
\section{Cross-validation}\label{cross-validation}}

Partitioning the available data into three sets drastically reduces the number of samples that can be used for training the model, and the results can depend on a particular random choice for the pair of (train, validation) sets. To address this reliability and stability, a solution to this problem is a procedure called cross-validation (CV for short). A test set should still be held out for final evaluation, but the validation set is no longer needed when doing a CV.

\hypertarget{k-fold-cross-validation}{%
\subsection{K-fold Cross-validation}\label{k-fold-cross-validation}}

Many different cross-validation methods are defined based on the following basic k-fold CV, the \textbf{training set} is split into k smaller sets (other approaches are described below, but generally follow the same principles). The following is the procedure that uses 5 ``folds'':

\begin{figure}[h]

{\centering \includegraphics[width=0.8\linewidth]{img06/w06-5-fold-CV} 

}

\caption{Figure 4.  Demonstration of a five-fold cross-validation.}\label{fig:unnamed-chunk-120}
\end{figure}

As an example, we explain the five-fold cross-validation for determining the optional cut-off probability in logistic prediction problems. Without loss of generality, we consider possible cut-off probabilities. For each cut-off probability, we calculate the prediction accuracy of the model based on the confusion matrix using the one-fold of validation data (see the following figure).

\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{img06/w06-CalculateConfusionMatrix} 

}

\caption{Confusion matrix and accuracy metrics.}\label{fig:unnamed-chunk-121}
\end{figure}

With the above process of calculation of the confusion matrix and the accuracy metrics based on the first iteration of the 5-fold CV with a given cut-off probability, we next will explain the logic of finding the optimal cut-off probability (also called hyperparameter) based on a set of given cut-off probabilities.

\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{img06/w06-cut-off-prob-CV} 

}

\caption{The pseudo-program of 5-fold CV for tuning hyperparameter - cut-off probability.}\label{fig:unnamed-chunk-122}
\end{figure}

From the above flow chart, we see that the performance measure reported by 5-fold cross-validation is the average of the values of accuracy computed in the loop. This approach can be computationally expensive but does not waste too much data (as is the case when fixing an arbitrary validation set), which is a major advantage in problems such as inverse inference where the number of samples is very small.

\hypertarget{other-cross-validation-methods}{%
\subsection{Other Cross-Validation Methods}\label{other-cross-validation-methods}}

There are many other cross-validation methods defined using the same logic in k-fold cross-validation. We will not detail these CV methods. Instead, we describe some of them that are occasionally used in practice.

\begin{itemize}
\item
  \textbf{Leave-one-out (LOO)} cross-validation is a simple cross-validation. Each training set is created by taking all the records (also called \textbf{samples} in machine learning) except one, the validating set being the record (also called \textbf{sample} in machine learning) left out. Therefore, this cross-validation procedure does not waste much data as only one sample is removed from the training set.
\item
  \textbf{Leave-P-Out} is very similar to Leave-One-Out as it creates all the possible training/test sets by removing samples (i.e., records) from the complete set. Unlike Leave-One-Out and K-Fold, the test sets will overlap for \(p > 1\).
\item
  \textbf{Stratified K-Fold} is a variation of k-fold that returns stratified folds: each set contains approximately the same percentage of samples of each target class as the complete set.
\item
  \textbf{Leave-One-Group-Out} is a cross-validation scheme that holds out the samples according to a third-party-provided array of integer groups. This group information can be used to encode arbitrary domain-specific pre-defined cross-validation folds. Each training set is thus constituted by all the samples except the ones related to a specific group.
\end{itemize}

\hypertarget{case-study-using-fraud-data}{%
\section{Case Study Using Fraud Data}\label{case-study-using-fraud-data}}

In this section, we use the fraud data at \url{https://pengdsci.github.io/datasets/FraudIndex/fraudidx.csv}. The data set has only two variables. The response variable is a binary fraud status variable with character values ``good'' and ``bad''. The predictor variable is an index variable. The sample size is 33236. The fraud proportion is about 23.34\%. The objective is to build a logistic regression model to predict fraud for future transactions.

\hypertarget{data-partition}{%
\subsection{Data Partition}\label{data-partition}}

Since the same size is large, we split the sample by 70\%:30\% with 70\% data for training and validating models and 30\% for testing purposes. The labels (value of the fraud status) of testing and validation data will be removed when calculating the accuracy measures.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fraud.data }\OtherTok{=} \FunctionTok{read.csv}\NormalTok{(}\StringTok{"https://pengdsci.github.io/datasets/FraudIndex/fraudidx.csv"}\NormalTok{)[,}\SpecialCharTok{{-}}\DecValTok{1}\NormalTok{]}
\DocumentationTok{\#\# recode status variable: bad = 1 and good = 0}
\NormalTok{good.id }\OtherTok{=} \FunctionTok{which}\NormalTok{(fraud.data}\SpecialCharTok{$}\NormalTok{status }\SpecialCharTok{==} \StringTok{" good"}\NormalTok{) }
\NormalTok{bad.id }\OtherTok{=} \FunctionTok{which}\NormalTok{(fraud.data}\SpecialCharTok{$}\NormalTok{status }\SpecialCharTok{==} \StringTok{"fraud"}\NormalTok{)}
\DocumentationTok{\#\#}
\NormalTok{fraud.data}\SpecialCharTok{$}\NormalTok{fraud.status }\OtherTok{=} \DecValTok{0}
\NormalTok{fraud.data}\SpecialCharTok{$}\NormalTok{fraud.status[bad.id] }\OtherTok{=} \DecValTok{1}
\NormalTok{nn }\OtherTok{=} \FunctionTok{dim}\NormalTok{(fraud.data)[}\DecValTok{1}\NormalTok{]}
\NormalTok{train.id }\OtherTok{=} \FunctionTok{sample}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\NormalTok{nn, }\FunctionTok{round}\NormalTok{(nn}\SpecialCharTok{*}\FloatTok{0.7}\NormalTok{), }\AttributeTok{replace =} \ConstantTok{FALSE}\NormalTok{) }
\NormalTok{training }\OtherTok{=}\NormalTok{ fraud.data[train.id,]}
\NormalTok{testing }\OtherTok{=}\NormalTok{ fraud.data[}\SpecialCharTok{{-}}\NormalTok{train.id,]}
\end{Highlighting}
\end{Shaded}

\hypertarget{finding-optimal-cut-off-probability}{%
\subsection{Finding Optimal Cut-off Probability}\label{finding-optimal-cut-off-probability}}

We define a sequence of 20 candidate cut-off probabilities and then use 5-fold cross-validation to identify the optimal cut-off probability for the final detection model.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{n0 }\OtherTok{=} \FunctionTok{dim}\NormalTok{(training)[}\DecValTok{1}\NormalTok{]}\SpecialCharTok{/}\DecValTok{5}
\NormalTok{cut}\FloatTok{.0}\NormalTok{ff.prob }\OtherTok{=} \FunctionTok{seq}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{1}\NormalTok{, }\AttributeTok{length =} \DecValTok{22}\NormalTok{)[}\SpecialCharTok{{-}}\FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{22}\NormalTok{)]        }\CommentTok{\# candidate cut off prob}
\NormalTok{pred.accuracy }\OtherTok{=} \FunctionTok{matrix}\NormalTok{(}\DecValTok{0}\NormalTok{,}\AttributeTok{ncol=}\DecValTok{20}\NormalTok{, }\AttributeTok{nrow=}\DecValTok{5}\NormalTok{, }\AttributeTok{byrow =}\NormalTok{ T)  }\CommentTok{\# null vector for storing prediction accuracy}
\DocumentationTok{\#\# 5{-}fold CV}
\ControlFlowTok{for}\NormalTok{ (i }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\DecValTok{5}\NormalTok{)\{}
\NormalTok{  valid.id }\OtherTok{=}\NormalTok{ ((i}\DecValTok{{-}1}\NormalTok{)}\SpecialCharTok{*}\NormalTok{n0 }\SpecialCharTok{+} \DecValTok{1}\NormalTok{)}\SpecialCharTok{:}\NormalTok{(i}\SpecialCharTok{*}\NormalTok{n0)}
\NormalTok{  valid.data }\OtherTok{=}\NormalTok{ training[valid.id,]}
\NormalTok{  train.data }\OtherTok{=}\NormalTok{ training[}\SpecialCharTok{{-}}\NormalTok{valid.id,]}
\NormalTok{  train.model }\OtherTok{=} \FunctionTok{glm}\NormalTok{(fraud.status }\SpecialCharTok{\textasciitilde{}}\NormalTok{ index, }\AttributeTok{family =} \FunctionTok{binomial}\NormalTok{(}\AttributeTok{link =}\NormalTok{ logit), }\AttributeTok{data =}\NormalTok{ train.data)}
\NormalTok{  newdata }\OtherTok{=} \FunctionTok{data.frame}\NormalTok{(}\AttributeTok{index=}\NormalTok{ valid.data}\SpecialCharTok{$}\NormalTok{index)}
\NormalTok{  pred.prob }\OtherTok{=} \FunctionTok{predict.glm}\NormalTok{(train.model, newdata, }\AttributeTok{type =} \StringTok{"response"}\NormalTok{)}
  \CommentTok{\# define confusion matrix and accuracy}
  \ControlFlowTok{for}\NormalTok{(j }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\DecValTok{20}\NormalTok{)\{}
\NormalTok{    pred.status }\OtherTok{=} \FunctionTok{rep}\NormalTok{(}\DecValTok{0}\NormalTok{,}\FunctionTok{length}\NormalTok{(pred.prob))}
\NormalTok{    valid.data}\SpecialCharTok{$}\NormalTok{pred.status }\OtherTok{=} \FunctionTok{as.numeric}\NormalTok{(pred.prob }\SpecialCharTok{\textgreater{}}\NormalTok{cut}\FloatTok{.0}\NormalTok{ff.prob[j])}
\NormalTok{    a11 }\OtherTok{=} \FunctionTok{sum}\NormalTok{(valid.data}\SpecialCharTok{$}\NormalTok{pred.status }\SpecialCharTok{==}\NormalTok{ valid.data}\SpecialCharTok{$}\NormalTok{fraud.status)}
\NormalTok{    pred.accuracy[i,j] }\OtherTok{=}\NormalTok{ a11}\SpecialCharTok{/}\FunctionTok{length}\NormalTok{(pred.prob)}
\NormalTok{  \}}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Warning: glm.fit:

## Warning: glm.fit:

## Warning: glm.fit:

## Warning: glm.fit:

## Warning: glm.fit:
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\DocumentationTok{\#\#\#  }
\NormalTok{avg.accuracy }\OtherTok{=} \FunctionTok{apply}\NormalTok{(pred.accuracy, }\DecValTok{2}\NormalTok{, mean)}
\NormalTok{max.id }\OtherTok{=} \FunctionTok{which}\NormalTok{(avg.accuracy }\SpecialCharTok{==}\FunctionTok{max}\NormalTok{(avg.accuracy ))}
\DocumentationTok{\#\#\# visual representation}
\NormalTok{tick.label }\OtherTok{=} \FunctionTok{as.character}\NormalTok{(}\FunctionTok{round}\NormalTok{(cut}\FloatTok{.0}\NormalTok{ff.prob,}\DecValTok{2}\NormalTok{))}
\FunctionTok{plot}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\DecValTok{20}\NormalTok{, avg.accuracy, }\AttributeTok{type =} \StringTok{"b"}\NormalTok{,}
     \AttributeTok{xlim=}\FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{20}\NormalTok{), }
     \AttributeTok{ylim=}\FunctionTok{c}\NormalTok{(}\FloatTok{0.5}\NormalTok{,}\DecValTok{1}\NormalTok{), }
     \AttributeTok{axes =} \ConstantTok{FALSE}\NormalTok{,}
     \AttributeTok{xlab =} \StringTok{"Cut{-}off Probability"}\NormalTok{,}
     \AttributeTok{ylab =} \StringTok{"Accuracy"}\NormalTok{,}
     \AttributeTok{main =} \StringTok{"5{-}fold CV performance"}
\NormalTok{     )}
\FunctionTok{axis}\NormalTok{(}\DecValTok{1}\NormalTok{, }\AttributeTok{at=}\DecValTok{1}\SpecialCharTok{:}\DecValTok{20}\NormalTok{, }\AttributeTok{label =}\NormalTok{ tick.label, }\AttributeTok{las =} \DecValTok{2}\NormalTok{)}
\FunctionTok{axis}\NormalTok{(}\DecValTok{2}\NormalTok{)}
\FunctionTok{segments}\NormalTok{(max.id, }\FloatTok{0.5}\NormalTok{, max.id, avg.accuracy[max.id], }\AttributeTok{col =} \StringTok{"red"}\NormalTok{)}
\FunctionTok{text}\NormalTok{(max.id, avg.accuracy[max.id]}\SpecialCharTok{+}\FloatTok{0.03}\NormalTok{, }\FunctionTok{as.character}\NormalTok{(}\FunctionTok{round}\NormalTok{(avg.accuracy[max.id],}\DecValTok{4}\NormalTok{)), }\AttributeTok{col =} \StringTok{"red"}\NormalTok{, }\AttributeTok{cex =} \FloatTok{0.8}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics{STA551EB_files/figure-latex/unnamed-chunk-124-1} 

}

\caption{Figure 7. 5-fold CV performance plot}\label{fig:unnamed-chunk-124}
\end{figure}

The above figure indicates that the optimal cut-off probability that yields the best accuracy is 0.57.

\hypertarget{reporting-test}{%
\subsection{Reporting Test}\label{reporting-test}}

This subsection reports the performance of the model using the test data set. Note that the model needs to be fit to the original training data to find the regression coefficients and then use the holdout testing sample to find the accuracy.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{test.model }\OtherTok{=} \FunctionTok{glm}\NormalTok{(fraud.status }\SpecialCharTok{\textasciitilde{}}\NormalTok{ index, }\AttributeTok{family =} \FunctionTok{binomial}\NormalTok{(}\AttributeTok{link =}\NormalTok{ logit), }\AttributeTok{data =}\NormalTok{ training)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Warning: glm.fit:
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{newdata }\OtherTok{=} \FunctionTok{data.frame}\NormalTok{(}\AttributeTok{index=}\NormalTok{ testing}\SpecialCharTok{$}\NormalTok{index)}
\NormalTok{pred.prob.test }\OtherTok{=} \FunctionTok{predict.glm}\NormalTok{(test.model, newdata, }\AttributeTok{type =} \StringTok{"response"}\NormalTok{)}
\NormalTok{testing}\SpecialCharTok{$}\NormalTok{test.status }\OtherTok{=} \FunctionTok{as.numeric}\NormalTok{(pred.prob.test }\SpecialCharTok{\textgreater{}} \FloatTok{0.57}\NormalTok{)}
\NormalTok{a11 }\OtherTok{=} \FunctionTok{sum}\NormalTok{(testing}\SpecialCharTok{$}\NormalTok{test.status }\SpecialCharTok{==}\NormalTok{ testing}\SpecialCharTok{$}\NormalTok{fraud.status)}
\NormalTok{test.accuracy }\OtherTok{=}\NormalTok{ a11}\SpecialCharTok{/}\FunctionTok{length}\NormalTok{(pred.prob.test)}
\FunctionTok{kable}\NormalTok{(}\FunctionTok{as.data.frame}\NormalTok{(test.accuracy), }\AttributeTok{align=}\StringTok{\textquotesingle{}c\textquotesingle{}}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{tabular}{c}
\hline
test.accuracy\\
\hline
0.9203691\\
\hline
\end{tabular}

The accuracy is 92.18\%. This accuracy indicates that there is no under-fitting. In models and algorithms with multiple feature variables, CV can help detect (and, hence, avoid )over and underfitting.

\hypertarget{concluding-remarks-1}{%
\section{Concluding Remarks}\label{concluding-remarks-1}}

We have used 5-fold cross-validation to find the hyperparameter (optimal cut-off probability). This is only one application for estimating a hyperparameter. In practice, we can use cross-validation to select the best models in different model families. For example, we have candidate models such as decision tree-based models, support vector machines, neural networks, and logistic regression models. Cross-validation is a powerful tool for selecting the best model.

\hfill\break

\hypertarget{perfomance-measures-of-algorithms-and-models}{%
\chapter{Perfomance Measures of Algorithms and Models}\label{perfomance-measures-of-algorithms-and-models}}

In this note, we summarize the performance measures that are used in data science projects (i.e.~in both machine learning and classical statistics). The focus is on the measures related to classification and regression models and algorithms.

Note that, in practice, looking at a single metric may not give us the whole picture of the problem we are trying to solve. In other words, we may want to use a set of metrics to have a concrete evaluation of the candidate models and algorithms.

\hypertarget{classification-performance-metrics}{%
\section{Classification Performance Metrics}\label{classification-performance-metrics}}

Since the performance measures for regression models are relatively simple, we focus on the commonly used performance measures of binary classification models and algorithms.

\hypertarget{confusion-matrix-for-binary-decision}{%
\subsection{Confusion Matrix for Binary Decision}\label{confusion-matrix-for-binary-decision}}

We have used the logistic regression model as an example to illustrate the cross-validation method. Most of the performance measures are defined based on the confusion matrix.

Consider a binary classification (prediction) model that passed all diagnostics and model selection processes. Any binary decision based on the model will inevitably result in two possible errors that are summarized in the following confusion matrix (as mentioned and used in the previous case study).

\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{img06/w06-GeneralConfusionMatrix} 

}

\caption{The layout of the binary decision confusion matrix.}\label{fig:unnamed-chunk-127}
\end{figure}

The following are a few probabilities that will be used as the element in the definition of performance measures.

\begin{itemize}
\item
  \textbf{True Positive (TP)} is the number of correct predictions that an example is positive which means positive class correctly identified as positive - P{[}Predicted Positive \textbar{} Actual Positive{]}.
\item
  \textbf{False Negative (FN)} is the number of incorrect predictions that an example is negative which means positive class incorrectly identified as negative - P{[}Predicted Negative \textbar{} Actual Positive{]}.
\item
  \textbf{False positive (FP)} is the number of incorrect predictions that an example is positive which means negative class incorrectly identified as positive - P{[}Predicted Positive \textbar{} Actual Negative{]}.
\item
  \textbf{True Negative (TN)} is the number of correct predictions that an example is negative which means negative class correctly identified as negative - P{[}Predicted Negative \textbar{} Actual Negative{]}.
\end{itemize}

The above conditional probabilities are defined by conditioning on the \textbf{actual status}. These probabilities are used to assess the model performance in the stage of model development.

One of the important steps in the data science process is to monitor the performance of the deployed models in the production environment. We can use a different set of conditional probabilities for this purpose.

\begin{itemize}
\item
  \textbf{Positive Predictive Value (PPV)} is the percentage of predictive positives that are confirmed to be positive - P{[} Confirmed Positive \textbar{} Predicted Positive{]}. In the clinical term, PPV is the percentage of patients with a positive test who actually have the disease.
\item
  \textbf{Negative Predictive Value (NPV)} is the percentage of predictive positives that are confirmed to be positive - P{[} Confirmed Negative \textbar{} Predicted Negative{]}. In the clinical term, NPV is the percentage of patients with a negative test who do not have the disease.
\end{itemize}

\hypertarget{local-performance-measures-for-model-development}{%
\subsection{Local Performance Measures (for Model Development)}\label{local-performance-measures-for-model-development}}

For ease of understanding, we use the following hypothetical confusion matrix based on the clinical binary decision.

\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{img06/w06-ClinicalConfusionMatrix} 

}

\caption{The layout of the clinical binary decision confusion matrix.}\label{fig:unnamed-chunk-128}
\end{figure}

The following performance measures are defined based on the above confusion matrix.

\begin{itemize}
\tightlist
\item
  \textbf{Classification Accuracy} measures the percentage of labels that are correctly predicted.
\end{itemize}

\[
\mbox{accuracy} = \frac{a + d}{a + b + c + d}
\]

\begin{itemize}
\tightlist
\item
  \textbf{Precision} is a valid performance measure for a class whose distribution is imbalanced (one class is more frequent than others). In this case, even if we predict all samples as the most frequent class, we would get a high accuracy rate. This does not make sense at all because your model is not learning anything, and is just predicting everything as the top class. \textbf{Precision} measures the percentage of true positives among all predictive positives.
\end{itemize}

\[
\mbox{precision} = \frac{a}{a + b}
\]

\begin{itemize}
\tightlist
\item
  \textbf{Recall} is another important metric, which is defined as the fraction of samples from a class that is correctly predicted by the model. More formally,
\end{itemize}

\[
\mbox{Recall} = P[\mbox{predict disaese} | \mbox{Actual disease}] = \frac{a}{a+c}
\]

\begin{itemize}
\tightlist
\item
  \textbf{F1 Score}: Depending on the application, we may want to give higher priority to recall or precision. But there are many applications in which both recall and precision are important. Therefore, it is natural to think of a way to combine these two into a single metric. One popular metric which combines precision and recall is called F1-score, which is the harmonic mean of precision and recall defined as
\end{itemize}

\[
F_1 = \frac{2 \times \mbox{precision}\times \mbox{recall}}{\mbox{precision}+\mbox{recall}}
\]

The generalized version of the F-score is defined below. As we can see F1-score is a special case of \(F_{\beta}\) when \(\beta= 1\).

\[
F_1 = \frac{(1+\beta^2) \times \mbox{precision}\times \mbox{recall}}{\beta^2(\mbox{precision}+\mbox{recall})}
\]

It is good to mention that there is always a trade-off between the precision and recall of a model. If we want to make the precision too high, we would end up seeing a drop in the recall rate, and vice versa.

\hypertarget{global-performance-measures}{%
\subsection{Global Performance Measures}\label{global-performance-measures}}

Sensitivity and specificity are two other popular metrics mostly used in medical and biology-related fields. They are used as building blocks for well-known global measures such as ROC and the area under the curve (AUC). They are defined in the forms of conditional probability in the following based on the above clinical confusion matrix.

\begin{itemize}
\tightlist
\item
  \textbf{Sensitivity (True Positive Rate, Recall)} - The probability of those who received a positive result on this test out of those who actually have a disease (when judged by the `Gold Standard'). It is the same as \textbf{recall}.
\end{itemize}

\[
\mbox{sensitivity} = \frac{a}{a+c}
\]

\begin{itemize}
\tightlist
\item
  \textbf{Specificity (True Negative Rate)} - The probability of those who received a negative result on this test out of those who do not actually have the disease (when judged by the `Gold Standard').
\end{itemize}

\[
\mbox{specificity} = \frac{d}{b + d}
\]

Next, we define a metric to assess the global performance measure for the binary decision models and algorithms. From the previous case study of cross-validation. Each candidate cut-off probability defines a confusion matrix and, consequently, sensitivity and specificity associated with the confusion matrix.

\begin{itemize}
\tightlist
\item
  \textbf{An ROC curve (receiver operating characteristic curve)} is a graph showing the performance of a classification model at all classification thresholds. This curve plots two parameters: sensitivity and (1 - specificity). Note that the (1 - specificity = false positive rate).
\end{itemize}

In other words, the ROC curve is the plot of the False Positive Rate (FPR) against the True Positive Rate (TPR) calculated from each decision boundary (such as the cut-off probability in logistic models).

\url{https://github.com/pengdsci/STA551/blob/main/w06/img/w06-Animated-ROC.gif}

The primary use of the ROC is to compare the global performance between candidate models (that are not necessarily to be within the same family). As an illustrative example, the following ROC curves are calculated based on a logistic regression model and a support vector machine (SVM). Both are binary classifiers.

\begin{figure}

{\centering \includegraphics[width=11.4in]{img06/w06-ROC-Model-Comparison} 

}

\caption{Figure 4. Using ROC for model selection.}\label{fig:unnamed-chunk-130}
\end{figure}

We can see that the SVM is globally better than the logistic regression. However, at some special decision boundaries, the logistic regression model is locally better than SVM.

\begin{itemize}
\tightlist
\item
  \textbf{Area Under The Curve (AUC)}
\end{itemize}

If two ROC curves intersect at least one point, we may want to report the area under the curves (AUC) to compare the global performance between the two corresponding models. See the illustrative example below.

\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{img06/w06-Equal-AUC-ROC} 

}

\caption{Using ROC for model selection.}\label{fig:unnamed-chunk-131}
\end{figure}

\hypertarget{case-study---logistic-regression-model-with-the-fraud-data}{%
\section{Case Study - Logistic regression model with the fraud data}\label{case-study---logistic-regression-model-with-the-fraud-data}}

This case study shows how to calculate the local and global performance metrics for logistic predictive models. We have used the confusion matrix in the case study in the previous note. Here we will use the optimal cut-off probability as the decision threshold to define a confusion matrix and then define the performance measure based on this matrix.

We reload the data and create the training and testing data sets. We pretend the optimal cut-off probability is based on what is obtained through the CV. The testing data set will be used to report the local and global performance measures.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fraud.data }\OtherTok{=} \FunctionTok{read.csv}\NormalTok{(}\StringTok{"https://pengdsci.github.io/datasets/FraudIndex/fraudidx.csv"}\NormalTok{)[,}\SpecialCharTok{{-}}\DecValTok{1}\NormalTok{]}
\DocumentationTok{\#\# recode status variable: bad = 1 and good = 0}
\NormalTok{good.id }\OtherTok{=} \FunctionTok{which}\NormalTok{(fraud.data}\SpecialCharTok{$}\NormalTok{status }\SpecialCharTok{==} \StringTok{" good"}\NormalTok{) }
\NormalTok{bad.id }\OtherTok{=} \FunctionTok{which}\NormalTok{(fraud.data}\SpecialCharTok{$}\NormalTok{status }\SpecialCharTok{==} \StringTok{"fraud"}\NormalTok{)}
\DocumentationTok{\#\#}
\NormalTok{fraud.data}\SpecialCharTok{$}\NormalTok{fraud.status }\OtherTok{=} \DecValTok{0}
\NormalTok{fraud.data}\SpecialCharTok{$}\NormalTok{fraud.status[bad.id] }\OtherTok{=} \DecValTok{1}
\NormalTok{nn }\OtherTok{=} \FunctionTok{dim}\NormalTok{(fraud.data)[}\DecValTok{1}\NormalTok{]}
\NormalTok{train.id }\OtherTok{=} \FunctionTok{sample}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\NormalTok{nn, }\FunctionTok{round}\NormalTok{(nn}\SpecialCharTok{*}\FloatTok{0.7}\NormalTok{), }\AttributeTok{replace =} \ConstantTok{FALSE}\NormalTok{) }
\NormalTok{training }\OtherTok{=}\NormalTok{ fraud.data[train.id,]}
\NormalTok{testing }\OtherTok{=}\NormalTok{ fraud.data[}\SpecialCharTok{{-}}\NormalTok{train.id,]}
\end{Highlighting}
\end{Shaded}

\hypertarget{local-performance-meaures}{%
\subsection{Local Performance Meaures}\label{local-performance-meaures}}

Since we have identified the optimal cut-off probability to be 0.57. Next, we will use the \textbf{testing data} set to report the local measures.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{test.model }\OtherTok{=} \FunctionTok{glm}\NormalTok{(fraud.status }\SpecialCharTok{\textasciitilde{}}\NormalTok{ index, }\AttributeTok{family =} \FunctionTok{binomial}\NormalTok{(}\AttributeTok{link =}\NormalTok{ logit), }\AttributeTok{data =}\NormalTok{ training)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Warning: glm.fit:
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{newdata }\OtherTok{=} \FunctionTok{data.frame}\NormalTok{(}\AttributeTok{index=}\NormalTok{ testing}\SpecialCharTok{$}\NormalTok{index)}
\NormalTok{pred.prob.test }\OtherTok{=} \FunctionTok{predict.glm}\NormalTok{(test.model, newdata, }\AttributeTok{type =} \StringTok{"response"}\NormalTok{)}
\NormalTok{testing}\SpecialCharTok{$}\NormalTok{test.status }\OtherTok{=} \FunctionTok{as.numeric}\NormalTok{(pred.prob.test }\SpecialCharTok{\textgreater{}} \FloatTok{0.57}\NormalTok{)}
\DocumentationTok{\#\#\# components for defining various measures}
\NormalTok{TN }\OtherTok{=} \FunctionTok{sum}\NormalTok{(testing}\SpecialCharTok{$}\NormalTok{test.status }\SpecialCharTok{==}\DecValTok{0} \SpecialCharTok{\&}\NormalTok{ testing}\SpecialCharTok{$}\NormalTok{fraud.status}\SpecialCharTok{==}\DecValTok{0}\NormalTok{)}
\NormalTok{FN }\OtherTok{=} \FunctionTok{sum}\NormalTok{(testing}\SpecialCharTok{$}\NormalTok{test.status }\SpecialCharTok{==}\DecValTok{0} \SpecialCharTok{\&}\NormalTok{ testing}\SpecialCharTok{$}\NormalTok{fraud.status }\SpecialCharTok{==}\DecValTok{1}\NormalTok{)}
\NormalTok{FP }\OtherTok{=} \FunctionTok{sum}\NormalTok{(testing}\SpecialCharTok{$}\NormalTok{test.status }\SpecialCharTok{==}\DecValTok{1} \SpecialCharTok{\&}\NormalTok{ testing}\SpecialCharTok{$}\NormalTok{fraud.status}\SpecialCharTok{==}\DecValTok{0}\NormalTok{)}
\NormalTok{TP }\OtherTok{=} \FunctionTok{sum}\NormalTok{(testing}\SpecialCharTok{$}\NormalTok{test.status }\SpecialCharTok{==}\DecValTok{1} \SpecialCharTok{\&}\NormalTok{ testing}\SpecialCharTok{$}\NormalTok{fraud.status }\SpecialCharTok{==}\DecValTok{1}\NormalTok{)}
\DocumentationTok{\#\#\#}
\NormalTok{sensitivity }\OtherTok{=}\NormalTok{ TP }\SpecialCharTok{/}\NormalTok{ (TP }\SpecialCharTok{+}\NormalTok{ FN)}
\NormalTok{specificity }\OtherTok{=}\NormalTok{ TN }\SpecialCharTok{/}\NormalTok{ (TN }\SpecialCharTok{+}\NormalTok{ FP)}
\DocumentationTok{\#\#\#}
\NormalTok{precision }\OtherTok{=}\NormalTok{ TP }\SpecialCharTok{/}\NormalTok{ (TP }\SpecialCharTok{+}\NormalTok{ FP)}
\NormalTok{recall }\OtherTok{=}\NormalTok{ sensitivity}
\NormalTok{F1 }\OtherTok{=} \DecValTok{2}\SpecialCharTok{*}\NormalTok{precision}\SpecialCharTok{*}\NormalTok{recall}\SpecialCharTok{/}\NormalTok{(precision }\SpecialCharTok{+}\NormalTok{ recall)}
\NormalTok{metric.list }\OtherTok{=} \FunctionTok{cbind}\NormalTok{(}\AttributeTok{sensitivity =}\NormalTok{ sensitivity, }
                    \AttributeTok{specificity =}\NormalTok{ specificity, }
                    \AttributeTok{precision =}\NormalTok{ precision,}
                    \AttributeTok{recall =}\NormalTok{ recall,}
                    \AttributeTok{F1 =}\NormalTok{ F1)}
\FunctionTok{kable}\NormalTok{(}\FunctionTok{as.data.frame}\NormalTok{(metric.list), }\AttributeTok{align=}\StringTok{\textquotesingle{}c\textquotesingle{}}\NormalTok{, }\AttributeTok{caption =} \StringTok{"Local performance metrics"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{table}

\caption{\label{tab:unnamed-chunk-133}Local performance metrics}
\centering
\begin{tabular}[t]{c|c|c|c|c}
\hline
sensitivity & specificity & precision & recall & F1\\
\hline
0.7171502 & 0.9761374 & 0.9023081 & 0.7171502 & 0.7991443\\
\hline
\end{tabular}
\end{table}

\hypertarget{global-measure-roc-and-auc}{%
\subsection{Global Measure: ROC and AUC}\label{global-measure-roc-and-auc}}

In order to create an ROC curve, we need to select a sequence of decision thresholds and calculate the corresponding sensitivity and specificity.

\textbf{CAUTION}: ROC and AUC are used for model selection, we still use the \textbf{training data} to construct the ROC and calculate the AUC.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{cut.off.seq }\OtherTok{=} \FunctionTok{seq}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, }\AttributeTok{length =} \DecValTok{100}\NormalTok{)}
\NormalTok{sensitivity.vec }\OtherTok{=} \ConstantTok{NULL}
\NormalTok{specificity.vec }\OtherTok{=} \ConstantTok{NULL}
\DocumentationTok{\#\#\# }
\NormalTok{training.model }\OtherTok{=} \FunctionTok{glm}\NormalTok{(fraud.status }\SpecialCharTok{\textasciitilde{}}\NormalTok{ index, }\AttributeTok{family =} \FunctionTok{binomial}\NormalTok{(}\AttributeTok{link =}\NormalTok{ logit), }\AttributeTok{data =}\NormalTok{ training)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Warning: glm.fit:
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{newdata }\OtherTok{=} \FunctionTok{data.frame}\NormalTok{(}\AttributeTok{index=}\NormalTok{ training}\SpecialCharTok{$}\NormalTok{index)}
\NormalTok{pred.prob.train }\OtherTok{=} \FunctionTok{predict.glm}\NormalTok{(training.model, newdata, }\AttributeTok{type =} \StringTok{"response"}\NormalTok{)}
\ControlFlowTok{for}\NormalTok{ (i }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\DecValTok{100}\NormalTok{)\{}
\NormalTok{  training}\SpecialCharTok{$}\NormalTok{train.status }\OtherTok{=} \FunctionTok{as.numeric}\NormalTok{(pred.prob.train }\SpecialCharTok{\textgreater{}}\NormalTok{ cut.off.seq[i])}
\DocumentationTok{\#\#\# components for defining various measures}
\NormalTok{TN }\OtherTok{=} \FunctionTok{sum}\NormalTok{(training}\SpecialCharTok{$}\NormalTok{train.status }\SpecialCharTok{==} \DecValTok{0} \SpecialCharTok{\&}\NormalTok{ training}\SpecialCharTok{$}\NormalTok{fraud.status }\SpecialCharTok{==} \DecValTok{0}\NormalTok{)}
\NormalTok{FN }\OtherTok{=} \FunctionTok{sum}\NormalTok{(training}\SpecialCharTok{$}\NormalTok{train.status }\SpecialCharTok{==} \DecValTok{0} \SpecialCharTok{\&}\NormalTok{ training}\SpecialCharTok{$}\NormalTok{fraud.status }\SpecialCharTok{==} \DecValTok{1}\NormalTok{)}
\NormalTok{FP }\OtherTok{=} \FunctionTok{sum}\NormalTok{(training}\SpecialCharTok{$}\NormalTok{train.status }\SpecialCharTok{==} \DecValTok{1} \SpecialCharTok{\&}\NormalTok{ training}\SpecialCharTok{$}\NormalTok{fraud.status }\SpecialCharTok{==} \DecValTok{0}\NormalTok{)}
\NormalTok{TP }\OtherTok{=} \FunctionTok{sum}\NormalTok{(training}\SpecialCharTok{$}\NormalTok{train.status }\SpecialCharTok{==} \DecValTok{1} \SpecialCharTok{\&}\NormalTok{ training}\SpecialCharTok{$}\NormalTok{fraud.status }\SpecialCharTok{==} \DecValTok{1}\NormalTok{)}
\DocumentationTok{\#\#\#}
\NormalTok{sensitivity.vec[i] }\OtherTok{=}\NormalTok{ TP }\SpecialCharTok{/}\NormalTok{ (TP }\SpecialCharTok{+}\NormalTok{ FN)}
\NormalTok{specificity.vec[i] }\OtherTok{=}\NormalTok{ TN }\SpecialCharTok{/}\NormalTok{ (TN }\SpecialCharTok{+}\NormalTok{ FP)}
\NormalTok{\}}
\NormalTok{one.minus.spec }\OtherTok{=} \DecValTok{1} \SpecialCharTok{{-}}\NormalTok{ specificity.vec}
\NormalTok{sens.vec }\OtherTok{=}\NormalTok{ sensitivity.vec}
\DocumentationTok{\#\# A better approx of ROC, need library \{pROC\}}
\NormalTok{  prediction }\OtherTok{=}\NormalTok{ pred.prob.train}
\NormalTok{  category }\OtherTok{=}\NormalTok{ training}\SpecialCharTok{$}\NormalTok{fraud.status }\SpecialCharTok{==} \DecValTok{1}
\NormalTok{  ROCobj }\OtherTok{\textless{}{-}} \FunctionTok{roc}\NormalTok{(category, prediction)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Setting levels: control = FALSE, case = TRUE
\end{verbatim}

\begin{verbatim}
## Setting direction: controls < cases
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{  AUC }\OtherTok{=} \FunctionTok{round}\NormalTok{(}\FunctionTok{auc}\NormalTok{(ROCobj),}\DecValTok{4}\NormalTok{)}
\DocumentationTok{\#\#}
\FunctionTok{par}\NormalTok{(}\AttributeTok{pty =} \StringTok{"s"}\NormalTok{)   }\CommentTok{\# make a square figure}
\FunctionTok{plot}\NormalTok{(one.minus.spec, sens.vec, }\AttributeTok{type =} \StringTok{"l"}\NormalTok{, }\AttributeTok{xlim =} \FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{1}\NormalTok{), }\AttributeTok{ylim =} \FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{1}\NormalTok{),}
     \AttributeTok{xlab =}\StringTok{"1 {-} specificity"}\NormalTok{,}
     \AttributeTok{ylab =} \StringTok{"sensitivity"}\NormalTok{,}
     \AttributeTok{main =} \StringTok{"ROC curve of Logistic Fraud Model"}\NormalTok{,}
     \AttributeTok{lwd =} \DecValTok{2}\NormalTok{,}
     \AttributeTok{col =} \StringTok{"blue"}\NormalTok{, )}
\FunctionTok{segments}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{0}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{, }\AttributeTok{col =} \StringTok{"red"}\NormalTok{, }\AttributeTok{lty =} \DecValTok{2}\NormalTok{, }\AttributeTok{lwd =} \DecValTok{2}\NormalTok{)}
\CommentTok{\#AUC = round(sum(sens.vec*(one.minus.spec[{-}101]{-}one.minus.spec[{-}1])),4)}
\FunctionTok{text}\NormalTok{(}\FloatTok{0.8}\NormalTok{, }\FloatTok{0.3}\NormalTok{, }\FunctionTok{paste}\NormalTok{(}\StringTok{"AUC = "}\NormalTok{, AUC), }\AttributeTok{col =} \StringTok{"blue"}\NormalTok{, }\AttributeTok{cex =} \FloatTok{0.8}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{STA551EB_files/figure-latex/unnamed-chunk-134-1.pdf}

\hfill\break

\hypertarget{from-statistics-models-to-ml-algorithms}{%
\chapter{From Statistics Models to ML Algorithms}\label{from-statistics-models-to-ml-algorithms}}

There are a lot of debates on the difference between statistics and machine learning in statistics and machine learning communities. It is sure that statistics and machine learning are not the same although there is an overlap. A major difference between machine learning and statistics is indeed their purpose.

\begin{itemize}
\item
  Statistics focuses on the inference and interpretability of the relationships between variables.
\item
  Machine learning focuses on the accuracy of the prediction of future values of (response) variables and detecting hidden patterns. Machine learning is traditionally considered to be a subfield of artificial intelligence, which is broadly defined as the capability of a machine to imitate intelligent human behavior.
\end{itemize}

A lot of statistical models can make predictions, but predictive accuracy is not their strength while machine learning models provide various degrees of interpretability sacrifice interpretability for predictive power. For example, regularized regressions as machine learning algorithms are interpretable but neural networks (particularly multi-layer networks ) are almost uninterpretable.

Statistics and machine learning are two of the key players in data science. As data science practitioners, our primary interest is to develop and select the right tools to build data solutions for real-world applications.

\hypertarget{some-technical-terms-and-ml-types}{%
\section{Some Technical Terms and ML Types}\label{some-technical-terms-and-ml-types}}

Before discussing machine learning algorithms, we first introduce some technical terms in ML and types of machine learning algorithms.

\hypertarget{machine-learning-problems-and-jargon}{%
\subsection{Machine Learning Problems and Jargon}\label{machine-learning-problems-and-jargon}}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.2386}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3068}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.4545}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Statistics
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Machine Learning
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Comments
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
data point, record, row of data & example, instance & Both domains also use ``observation,'' which can refer to a single measurement or an entire vector of attributes depending on context. \\
response variable, dependent variable & label, output & Both domains also use ``target.'' Since practically all variables depend on other variables, the term ``dependent variable'' is potentially misleading. \\
regressions & supervised learners, machines & Both estimate output(s) in terms of input(s). \\
regression intercept & bias & the default prediction of a linear model in the special case where all inputs are 0. \\
Maximize the likelihood to estimate model parameters & Minimize the entropy to derive the best parameters in categorical regression or maximize the likelihood for continuous regression. & For discrete distributions, maximizing the likelihood is equivalent to minimizing the entropy. \\
logistic/multinomial regression & maximum entropy, MaxEnt & They are equivalent except in special multinomial settings like ordinal logistic regression. \\
\end{longtable}

\hfill\break

\hypertarget{types-of-machine-learning-problems}{%
\subsection{Types of Machine Learning Problems}\label{types-of-machine-learning-problems}}

Based on the type of problems that we are trying to solve, we can classify the Machine learning problem into three different categories.

\textbf{Classification Problem}: Classification is a problem that requires machine learning algorithms to assign a class label (response value) to examples (vector of predictor values) from the problem domain. A very intuitive example is classifying credit card transactions into two labels ``fraud'' or ``not a fraud.''

\textbf{Regression Problem}: Regression is a problem that requires machine learning algorithms to predict continuous (response) variables. An elementary example will be to predict the temperature of the city. (Temperature can take any numeric value between -50 to +150 degrees.)

\textbf{Clustering Problem}: Clustering is a type of problem that requires the use of Machine Learning algorithms to group the given data records into a specified number of cohesive units. A simple example will be to group the credit card holders according to their monthly spending.

There are many machine learning algorithms and statistical models running in the practice. This note primarily overviews the commonly used methods for classification and prediction.

\hfill\break

\hypertarget{categories-of-machine-learning}{%
\section{Categories of Machine Learning}\label{categories-of-machine-learning}}

There are different ways to categorize machine learning algorithms using different criteria. But none of them are perfect although each has its own merits.

\hypertarget{supervised-learning}{%
\subsection{Supervised Learning}\label{supervised-learning}}

Input data is called training data and has a known label (i.e., the response in statistics) or result such as spam/not-spam (classification) or a stock price at a time (forecasting/prediction).

Supervised learning is a subcategory of machine learning. It is defined by its use of labeled data (i.e., the response is available in the data) sets to train algorithms that classify data or predict outcomes accurately. As input data is fed into the model, it adjusts its weights until the model has been fitted appropriately, which occurs as part of the \textbf{cross-validation} process.

Supervised learning helps organizations solve a variety of real-world problems at scale, such as classifying spam in a separate folder from an email inbox, classifying credit transactions into fraud or non-fraud categories, etc.

\begin{figure}

{\centering \includegraphics[width=16in,height=0.3\textheight]{img07/w07-superised} 

}

\caption{ Illustration of supervised machine learning.}\label{fig:unnamed-chunk-136}
\end{figure}

There are a lot of supervised machine learning algorithms. Example algorithms include \textbf{Logistic Regression}, Decision Trees, Support Vector Machines, Neural Networks, etc.

\hypertarget{unsupervised-learning}{%
\subsection{Unsupervised Learning}\label{unsupervised-learning}}

Unsupervised learning uses machine learning algorithms to analyze and cluster unlabeled (i.e., no response variable is used or required in) data sets. These algorithms discover hidden patterns or data groupings without the need for human intervention. Its ability to discover similarities and differences in information makes it the ideal solution for exploratory data analysis, cross-selling strategies, customer segmentation, and image recognition. We have briefly mentioned this clustering algorithm when discussing feature engineering.

\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{img07/w07-unsuperised} 

}

\caption{Illustration of unsupervised machine learning.}\label{fig:unnamed-chunk-137}
\end{figure}

Example problems are clustering, dimensionality reduction, and association rule learning.

Example algorithms include clustering analysis and K-Means.

\hypertarget{semi-supervised-learning}{%
\subsection{Semi-Supervised Learning}\label{semi-supervised-learning}}

Input data is a mixture of labeled and unlabeled examples, that is, some records have response values and some don't.

We can use either a predictive ML model trained based on the labeled data to predict the labels of the unlabeled data or a clustering algorithm such as k-means to assign labels to the unlabeled data to make a larger pseudo-labeled data for ML model with a better performance.

\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{img07/w07-semi-superised} 

}

\caption{Illustration of semi-supervised machine learning.}\label{fig:unnamed-chunk-138}
\end{figure}

The following chart gives a non-numerical example of how semi-supervised learning works.

\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{img07/w07-semi-superised} 

}

\caption{Illustration of how semi-supervised machine learning works.}\label{fig:unnamed-chunk-139}
\end{figure}

Semi-supervised learning and reinforcement learning are hot topics in the machine learning community. Example algorithms are extensions to other flexible methods that make assumptions about how to model the unlabeled data.

\hfill\break

\hypertarget{from-statistics-to-machine-learning}{%
\section{From Statistics to Machine Learning}\label{from-statistics-to-machine-learning}}

As mentioned earlier, most statistical models can be used as a learning algorithm. In this section, we use two examples to demonstrate the equivalent between some of the basic statistical models and their corresponding machine learning models.

\hypertarget{logistic-regression-model-revisited}{%
\subsection{Logistic Regression Model Revisited}\label{logistic-regression-model-revisited}}

Recall that the binary logistic regression model with n feature variables \(x_1, x_2, \cdots, x_n\) is given by

\[
P[Y = 1] = \frac{\exp(w_0 + w_1x_1 + w_2x_2 + \cdots + w_nx_n)}{1+\exp(w_0 + w_1x_1 + w_2x_2 + \cdots + w_nx_n)} = \frac{\exp(w_0+\sum_{i=1}^n w_ix_i)}{1 + \exp(w_0+\sum_{i=1}^n w_ix_i)}
\]

where \(w_0, w_1, \cdots, w_n\) are regression coefficients. The model can be rewritten as

\[
P[Y=1] = \frac{1}{1+ \exp[-(w_0+\sum_{i=1}^n w_ix_i)]}
\]

Let
\[
f(x) = \frac{1}{1+\exp(-x)}.
\]

This function is the well-known \textbf{logistic function}. The curve of the logistic function is given by

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x }\OtherTok{=} \FunctionTok{seq}\NormalTok{(}\SpecialCharTok{{-}}\DecValTok{5}\NormalTok{, }\DecValTok{5}\NormalTok{, }\AttributeTok{length =} \DecValTok{100}\NormalTok{)}
\NormalTok{y }\OtherTok{=} \DecValTok{1}\SpecialCharTok{/}\NormalTok{(}\DecValTok{1}\SpecialCharTok{+}\FunctionTok{exp}\NormalTok{(}\SpecialCharTok{{-}}\NormalTok{x))}
\FunctionTok{plot}\NormalTok{(x,y, }\AttributeTok{type =} \StringTok{"l"}\NormalTok{, }\AttributeTok{lwd =} \DecValTok{2}\NormalTok{, }\AttributeTok{xlab =}\StringTok{" "}\NormalTok{, }\AttributeTok{ylab =} \StringTok{" "}\NormalTok{, }
     \AttributeTok{main =} \StringTok{"Logistic Curve"}\NormalTok{, }\AttributeTok{col =} \StringTok{"blue"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics{STA551EB_files/figure-latex/unnamed-chunk-140-1} 

}

\caption{The curve of the logistic function.}\label{fig:unnamed-chunk-140}
\end{figure}

Using this logistic function, we can re-express the logistic model as

\[
P[Y = 1] = f\left(w_0+\sum_{i=1}^n w_ix_i\right)
\]

Next, we represent the above logistic regression model in the following diagram.

\begin{figure}

{\centering \includegraphics[width=0.7\linewidth]{img07/w07-logisticDiagram} 

}

\caption{Diagram Representation of logistic regression models.}\label{fig:unnamed-chunk-141}
\end{figure}

We will see that the above diagram of the logistic regression model is the architecture of the basic single layer \textbf{sigmoid} neural network model - perceptron.

\hypertarget{single-layer-neural-network---perceptron}{%
\subsection{Single Layer Neural Network - Perceptron}\label{single-layer-neural-network---perceptron}}

Perceptron is a type of artificial neural network, which is a fundamental concept in machine learning. Its architecture is the same as the diagram of the logistic regression model.

\begin{figure}

{\centering \includegraphics[width=0.7\linewidth]{img07/w07-PerceptronNetwork} 

}

\caption{Architecture of Single layer neural network models (perceptron).}\label{fig:unnamed-chunk-142}
\end{figure}

Each input \(x_i\) has an associated weight \(w_i\) (like regression coefficient). The sum of all weighted inputs, \(\sum_{i=1}^n w_ix_i\) , is then passed through a nonlinear activation function \(f()\), to transform the pre-activation level of the neuron to an output \(y_j\). For simplicity, the bias term is set to \textbf{b} which is equivalent to the intercept of a regression model.

To summarize, we explicitly list the major components of perceptron in the following.

\begin{itemize}
\item
  \textbf{Input Layer}: The input layer consists of one or more input neurons, which receive input signals from the external world or from other layers of the neural network.
\item
  \textbf{Weights}: Each input neuron is associated with a weight, which represents the strength of the connection between the input neuron and the output neuron.
\item
  \textbf{Bias}: A bias term is added to the input layer to provide the perceptron with additional flexibility in modeling complex patterns in the input data.
\item
  \textbf{Activation Function}: The activation function determines the output of the perceptron based on the weighted sum of the inputs and the bias term. Common activation functions used in perceptrons include the \texttt{step\ function}, \texttt{sigmoid\ function}, and \texttt{ReLU\ function}, etc.
\item
  \textbf{Output}: The output of the perceptron is a single binary value, either 0 or 1, which indicates the class or category to which the input data belongs.
\end{itemize}

Note that when the sigmoid (i.e., logistic) function

\[
f(x) = \frac{\exp(x)}{1 + \exp(x)} = \frac{1}{1+\exp(-x)}.
\]

is used in the perceptron. This means that the single-layer perception with logistic activation is equivalent to the binary logistic regression.

\hfill\break

\textbf{Remarks}:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  The output of the above perceptron network is binary, i.e., \(\hat{Y} = 0\) or \(1\) since an implicit decision boundary based on the sign of the value of the transfer function \(\sum_{i=1}^m w_ix_i + b\). In the sigmoid perceptron network, this is equivalent to setting the threshold probability to 0.5. To see this, not that, if \(\sum_{i=1}^m w_ix_i + b = 0\), then
  \[
  P\left[Y=1 \Bigg| \sum_{i=1}^m w_ix_i + b\right]=\frac{1}{1+\exp\left[-(\sum_{i=1}^m w_ix_i + b) \right]} = \frac{1}{1+\exp(0)} = \frac{1}{2}
  \]
  This means, if the cut-off probability \(0.5\) is used in the logistic predictive model, this logistic predictive model is equivalent to the perceptron with sigmoid being the activation function.
\item
  There are several other commonly used activation functions in perceptron. The sigmoid activation function is only one of them. This implies that the binary logistic regression model is a special perceptron network model.
\end{enumerate}

\hypertarget{multi-layer-perceptron}{%
\subsection{Multi-layer Perceptron}\label{multi-layer-perceptron}}

A Multi-Layer Perceptron (MLP) contains one or more hidden layers (apart from one input and one output layer). While a single-layer perceptron can only learn linear functions, a multi-layer perceptron can also learn non-linear functions. The following is an illustrative MLP.

\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{img07/w07-Multilayer-Perceptron} 

}

\caption{Multi-layer perceptron.}\label{fig:unnamed-chunk-143}
\end{figure}

The \texttt{major\ components} in the above MLP are described in the following.

\textbf{Input Layer}: The Input layer has three nodes. The Bias node has a value of 1. The other two nodes take \(X_1\) and \(X_2\) as external inputs (which are numerical values depending upon the input data set). No computation is performed in the Input layer, so the outputs from nodes in the Input layer are 1, \(X_1\), and \(X_2\) respectively, which are fed into the Hidden Layer.

\textbf{Hidden Layer}: The Hidden layer also has three nodes with the Bias node having an output of 1. The output of the other two nodes in the Hidden layer depends on the outputs from the Input layer (1, \(X_1\), \(X_2\)) as well as the weights associated with the connections (edges). Figure 16 shows the output calculations for the hidden nodes. Remember that \(f()\) refers to the activation function. These outputs are then fed to the nodes in the Output layer.

\textbf{Output Layer}: The Output layer has two nodes that take inputs from the Hidden layer and perform similar computations as shown in the above figure. The values calculated (\(Y_1\) and \(Y_2\)) as a result of these computations act as outputs of the Multi-Layer Perceptron.

\hypertarget{commonly-used-activation-functions}{%
\subsection{Commonly Used Activation Functions}\label{commonly-used-activation-functions}}

The sigmoid function is only one of the activation functions used in neural networks. The table below lists several other commonly used activation functions in neural network modeling.

\begin{figure}

{\centering \includegraphics[width=0.6\linewidth]{img07/w07-commonlyUsedActivationFuns} 

}

\caption{Popular activation functions in neural networks.}\label{fig:unnamed-chunk-144}
\end{figure}

\hypertarget{algorithms-for-estimating-weights}{%
\subsection{Algorithms for Estimating Weights}\label{algorithms-for-estimating-weights}}

We know that the estimation of the regression coefficient in logistic regression is to maximize the likelihood function defined based on the binomial distribution. Algorithms such as Newton and its variants, scoring methods, etc. are used to obtain the estimated regression coefficients.

In neural network models, the weights are estimated by minimizing the loss function (also called cost function) when training neural networks. The loss function could be defined as \textbf{mean square error (MSE)} for regression tasks and \textbf{cross-entropy (cs)} for classification tasks.

Learning algorithms \textbf{forward and backward propagation} that depend on each other are used in minimizing the underlying \textbf{loss function}.

\begin{itemize}
\item
  \textbf{Forward propagation} is where input data is fed through a network, in a forward direction, to generate an output. The data is accepted by hidden layers and processed, as per the activation function, and moves to the successive layer. During forward propagation, the activation function is applied, based on the weighted sum, to make the neural network flow non-linearly using bias. Forward propagation is the way data moves from left (input layer) to right (output layer) in the neural network.
\item
  \textbf{Backpropagation} is used to improve the prediction accuracy of a node is expressed as a loss function or error rate. Backpropagation calculates the slope of (gradient) a \textbf{loss function} of other weights in the neural network and updates the weights using gradient descent through the learning rate.
\end{itemize}

\begin{figure}

{\centering \includegraphics[width=0.6\linewidth]{img07/w07-backpropagationGradient} 

}

\caption{Updating weights with backpropagation algorithm.}\label{fig:unnamed-chunk-145}
\end{figure}

The general architecture of the backpropagation network model is depicted in the following diagram.

\begin{figure}

{\centering \includegraphics[width=0.6\linewidth]{img07/w07-backpropagationNN} 

}

\caption{The idea of backpropagation neural networks.}\label{fig:unnamed-chunk-146}
\end{figure}

The algorithm of backpropagation is not used in classical statistics. This is why the neural network model outperformed the classical logistic model in terms of predictive power.

The R library \texttt{neuralnet} has the following five algorithms:

\textbf{backprop} - traditional \texttt{backpropagation}.

\textbf{rprop+} - resilient backpropagation with weight backtracking.

\textbf{rprop-} - resilient backpropagation without weight backtracking.

\textbf{sag} - modified globally convergent algorithm (gr-prop) with the smallest absolute gradient.

\textbf{slr} - modified globally convergent algorithm (gr-prop) with the smallest learning rate.

\hfill\break

\hypertarget{implementing-nn-with-r}{%
\section{Implementing NN with R}\label{implementing-nn-with-r}}

Several R libraries can run neural network models. \texttt{nnet} is the simplest one that only implements single-layer networks. \texttt{neuralnet} can run both single-layer and multiple-layer neural networks. \texttt{RSNNS} (R Stuttgart Neural Network Simulator) is a wrapper of multiple R libraries that implements different network models.

\hypertarget{syntax-of-neuralnet}{%
\subsection{\texorpdfstring{Syntax of \texttt{neuralnet}}{Syntax of neuralnet}}\label{syntax-of-neuralnet}}

We use \texttt{neuralnet} library to run the neural network model in the example (code for installing and loading this library is placed in the setup code chunk).

The syntax of \texttt{neuralnet()} is given below

\begin{verbatim}
 neuralnet(formula, 
           data, 
           hidden = 1,
           threshold = 0.01, 
           stepmax = 1e+05, 
           rep = 1, 
           startweights = NULL,
           learningrate.limit = NULL,
           learningrate.factor =list(minus = 0.5, plus = 1.2),
           learningrate=NULL, 
           lifesign = "none",
           lifesign.step = 1000, 
           algorithm = "rprop+",
           err.fct = "sse", 
           act.fct = "logistic",
           linear.output = TRUE, 
           exclude = NULL,
           constant.weights = NULL, 
           likelihood = FALSE)
\end{verbatim}

The detailed \texttt{help\ document} can be found at \url{https://www.rdocumentation.org/packages/neuralnet/versions/1.44.2/topics/neuralnet}.

\hypertarget{feature-conversion-for-neuralnet}{%
\subsection{\texorpdfstring{Feature Conversion for \texttt{neuralnet}}{Feature Conversion for neuralnet}}\label{feature-conversion-for-neuralnet}}

\texttt{neuralnet()} requires all features to be in the numeric form (dummy variable for categorical features, normalization of numerical features). The model formula in \texttt{neuralnet()} requires dummy variables to be explicitly defined. It is also highly recommended to scale all numerical features before being included in the network model. The objective is to find all feature names (numeric and all dummy variables) and write them in the model formula like the one in \texttt{glm}: \texttt{response\ \textasciitilde{}\ var\_1\ +\ var\_2\ +\ ...\ +var\_k}

To explain the modeling process in detail, we will outline major steps in the following subsections.

\hypertarget{numeric-feature-scaling}{%
\subsection{Numeric Feature Scaling}\label{numeric-feature-scaling}}

There are different types of scaling and standardization. The one we use in the following has

\[
scaled.var = \frac{orig.var - \min(orig.var)}{\max(orig.var)-\min(orig.var)}
\]
The scaled numeric feature is unitless (similar to the well-known z-score transformation).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Pima }\OtherTok{=} \FunctionTok{read.csv}\NormalTok{(}\StringTok{"https://pengdsci.github.io/STA551/w03/AnalyticPimaDiabetes.csv"}\NormalTok{)[,}\SpecialCharTok{{-}}\DecValTok{1}\NormalTok{]}
\NormalTok{Pima}\SpecialCharTok{$}\NormalTok{pedigree }\OtherTok{=}\NormalTok{ (Pima}\SpecialCharTok{$}\NormalTok{pedigree}\SpecialCharTok{{-}}\FunctionTok{min}\NormalTok{(Pima}\SpecialCharTok{$}\NormalTok{pedigree))}\SpecialCharTok{/}\NormalTok{(}\FunctionTok{max}\NormalTok{(Pima}\SpecialCharTok{$}\NormalTok{pedigree)}\SpecialCharTok{{-}}\FunctionTok{min}\NormalTok{(Pima}\SpecialCharTok{$}\NormalTok{pedigree))}
\NormalTok{Pima}\SpecialCharTok{$}\NormalTok{impute.log.insulin }\OtherTok{=}\NormalTok{ (Pima}\SpecialCharTok{$}\NormalTok{impute.log.insulin}\SpecialCharTok{{-}}\FunctionTok{min}\NormalTok{(Pima}\SpecialCharTok{$}\NormalTok{impute.log.insulin))}\SpecialCharTok{/}\NormalTok{(}\FunctionTok{max}\NormalTok{(Pima}\SpecialCharTok{$}\NormalTok{impute.log.insulin)}\SpecialCharTok{{-}}\FunctionTok{min}\NormalTok{(Pima}\SpecialCharTok{$}\NormalTok{impute.log.insulin))}
\NormalTok{Pima}\SpecialCharTok{$}\NormalTok{impute.triceps }\OtherTok{=}\NormalTok{ (Pima}\SpecialCharTok{$}\NormalTok{impute.triceps}\SpecialCharTok{{-}}\FunctionTok{min}\NormalTok{(Pima}\SpecialCharTok{$}\NormalTok{impute.triceps))}\SpecialCharTok{/}\NormalTok{(}\FunctionTok{max}\NormalTok{(Pima}\SpecialCharTok{$}\NormalTok{impute.triceps)}\SpecialCharTok{{-}}\FunctionTok{min}\NormalTok{(Pima}\SpecialCharTok{$}\NormalTok{impute.triceps))}
\end{Highlighting}
\end{Shaded}

\hypertarget{extract-all-feature-names}{%
\subsection{Extract All Feature Names}\label{extract-all-feature-names}}

In practical applications, there may be many categorical features in the model and each category could have many categories. It is practically infeasible to write all resulting dummy features explicitly. We can use the R function to extract variables from a model formula that will be used in a model. Make sure, all categorical feature variables must be defined in a non-numerical form (i.e., should not be numerically encoded). We can also use the R function \texttt{relevel()} to change the baseline of an unordered categorical feature variable.

Next, we use the R function \texttt{model.matrix()} to extract the names of all feature variables (including implicitly defined dummy feature variables from \texttt{model.matrix()}).

\begin{verbatim}
PimaMtx0 = model.matrix(~ mass + pedigree + impute.log.insulin + impute.triceps + grp.glucose + grp.diastolic + grp.age + grp.pregnant + diabetes, data = Pima)
colnames(PimaMtx0)
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{PimaMtx }\OtherTok{=} \FunctionTok{model.matrix}\NormalTok{(}\SpecialCharTok{\textasciitilde{}}\NormalTok{ ., }\AttributeTok{data =}\NormalTok{ Pima)}
\FunctionTok{colnames}\NormalTok{(PimaMtx)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  [1] "(Intercept)"          "mass"                 "pedigree"             "impute.log.insulin"  
##  [5] "impute.triceps"       "grp.glucose[117,137]" "grp.glucose> 137"     "grp.diastolic[80,90]"
##  [9] "grp.diastolic> 90"    "grp.age[45, 64]"      "grp.age65+"           "grp.pregnant1"       
## [13] "grp.pregnant2"        "grp.pregnant3-4"      "grp.pregnant5-7"      "grp.pregnant8+"      
## [17] "diabetespos"
\end{verbatim}

There are some naming issues in the above dummy feature variables for network modeling (although they are good for regular linear and generalized linear regression models). We need to rename them by excluding special characters in order to build neural network models. These issues can be avoided at the stage of feature engineering (if we initially planned to build neural network models). Next, we clean up the variables before defining the network model formula.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{colnames}\NormalTok{(PimaMtx)[}\DecValTok{4}\NormalTok{] }\OtherTok{\textless{}{-}} \StringTok{"logInsulin"}
\FunctionTok{colnames}\NormalTok{(PimaMtx)[}\DecValTok{5}\NormalTok{] }\OtherTok{\textless{}{-}} \StringTok{"triceps"}
\FunctionTok{colnames}\NormalTok{(PimaMtx)[}\DecValTok{6}\NormalTok{] }\OtherTok{\textless{}{-}} \StringTok{"glucose117To137"}
\FunctionTok{colnames}\NormalTok{(PimaMtx)[}\DecValTok{7}\NormalTok{] }\OtherTok{\textless{}{-}} \StringTok{"glucoseGt137"}
\FunctionTok{colnames}\NormalTok{(PimaMtx)[}\DecValTok{8}\NormalTok{] }\OtherTok{\textless{}{-}} \StringTok{"diastolic80To90"}
\FunctionTok{colnames}\NormalTok{(PimaMtx)[}\DecValTok{9}\NormalTok{] }\OtherTok{\textless{}{-}} \StringTok{"diastolicGt90"}
\FunctionTok{colnames}\NormalTok{(PimaMtx)[}\DecValTok{10}\NormalTok{] }\OtherTok{\textless{}{-}} \StringTok{"age45To64"}
\FunctionTok{colnames}\NormalTok{(PimaMtx)[}\DecValTok{11}\NormalTok{] }\OtherTok{\textless{}{-}} \StringTok{"age65older"}
\FunctionTok{colnames}\NormalTok{(PimaMtx)[}\DecValTok{12}\NormalTok{] }\OtherTok{\textless{}{-}} \StringTok{"pregnant1"}
\FunctionTok{colnames}\NormalTok{(PimaMtx)[}\DecValTok{13}\NormalTok{] }\OtherTok{\textless{}{-}} \StringTok{"pregnant2"}
\FunctionTok{colnames}\NormalTok{(PimaMtx)[}\DecValTok{14}\NormalTok{] }\OtherTok{\textless{}{-}} \StringTok{"pregnant3To4"}
\FunctionTok{colnames}\NormalTok{(PimaMtx)[}\DecValTok{15}\NormalTok{] }\OtherTok{\textless{}{-}} \StringTok{"pregnant5To7"}
\FunctionTok{colnames}\NormalTok{(PimaMtx)[}\DecValTok{16}\NormalTok{] }\OtherTok{\textless{}{-}} \StringTok{"pregnant8Plus"}
\end{Highlighting}
\end{Shaded}

\hypertarget{define-model-formula}{%
\subsection{Define Model Formula}\label{define-model-formula}}

For convenience, we encourage you to use \textbf{CamelCase} notation (\texttt{CamelCase} is a way to separate the words in a phrase by making the first letter of each word capitalized and not using spaces) in naming feature variables.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{columnNames }\OtherTok{=} \FunctionTok{colnames}\NormalTok{(PimaMtx)}
\NormalTok{columnList }\OtherTok{=} \FunctionTok{paste}\NormalTok{(columnNames[}\SpecialCharTok{{-}}\FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\FunctionTok{length}\NormalTok{(columnNames))], }\AttributeTok{collapse =} \StringTok{"+"}\NormalTok{)}
\NormalTok{columnList }\OtherTok{=} \FunctionTok{paste}\NormalTok{(}\FunctionTok{c}\NormalTok{(columnNames[}\FunctionTok{length}\NormalTok{(columnNames)],}\StringTok{"\textasciitilde{}"}\NormalTok{,columnList), }\AttributeTok{collapse=}\StringTok{""}\NormalTok{)}
\NormalTok{modelFormula }\OtherTok{=} \FunctionTok{formula}\NormalTok{(columnList)}
\NormalTok{modelFormula}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## diabetespos ~ mass + pedigree + logInsulin + triceps + glucose117To137 + 
##     glucoseGt137 + diastolic80To90 + diastolicGt90 + age45To64 + 
##     age65older + pregnant1 + pregnant2 + pregnant3To4 + pregnant5To7 + 
##     pregnant8Plus
\end{verbatim}

\hfill\break

\hypertarget{training-and-testing-nn-model}{%
\subsection{Training and Testing NN Model}\label{training-and-testing-nn-model}}

We follow the routine steps for building a neural network model to predict diabetes.

\hypertarget{data-splitting}{%
\subsubsection{Data Splitting}\label{data-splitting}}

We split the data into 70\% for training the neural network and 30\% for testing.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{n }\OtherTok{=} \FunctionTok{dim}\NormalTok{(PimaMtx)[}\DecValTok{1}\NormalTok{]}
\NormalTok{testID }\OtherTok{=} \FunctionTok{sample}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\NormalTok{n, }\FunctionTok{round}\NormalTok{(n}\SpecialCharTok{*}\FloatTok{0.7}\NormalTok{), }\AttributeTok{replace =} \ConstantTok{FALSE}\NormalTok{)}
\NormalTok{testDat }\OtherTok{=}\NormalTok{ PimaMtx[testID,]}
\NormalTok{trainDat }\OtherTok{=}\NormalTok{ PimaMtx[}\SpecialCharTok{{-}}\NormalTok{testID,]}
\end{Highlighting}
\end{Shaded}

\hypertarget{build-nn-model}{%
\subsubsection{Build NN Model}\label{build-nn-model}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{NetworkModel }\OtherTok{=} \FunctionTok{neuralnet}\NormalTok{(modelFormula,}
                         \AttributeTok{data =}\NormalTok{ trainDat,}
                         \AttributeTok{hidden =} \DecValTok{1}\NormalTok{,               }\CommentTok{\# single layer NN}
                         \AttributeTok{rep =} \DecValTok{1}\NormalTok{,                  }\CommentTok{\# number of replicates in training NN}
                         \AttributeTok{threshold =} \FloatTok{0.01}\NormalTok{,         }\CommentTok{\# threshold for the partial derivatives as stopping criteria.}
                         \AttributeTok{learningrate =} \FloatTok{0.1}\NormalTok{,       }\CommentTok{\# user selected rate}
                         \AttributeTok{algorithm =} \StringTok{"rprop+"}
\NormalTok{                         )}
\FunctionTok{kable}\NormalTok{(NetworkModel}\SpecialCharTok{$}\NormalTok{result.matrix)}
\end{Highlighting}
\end{Shaded}

\begin{tabular}{l|r}
\hline
error & 14.0370974\\
\hline
reached.threshold & 0.0097069\\
\hline
steps & 21874.0000000\\
\hline
Intercept.to.1layhid1 & 90.1823029\\
\hline
mass.to.1layhid1 & -2.2395240\\
\hline
pedigree.to.1layhid1 & -42.0089980\\
\hline
logInsulin.to.1layhid1 & 23.6797205\\
\hline
triceps.to.1layhid1 & 38.6772974\\
\hline
glucose117To137.to.1layhid1 & -12.6254994\\
\hline
glucoseGt137.to.1layhid1 & -34.6507358\\
\hline
diastolic80To90.to.1layhid1 & 2.1930003\\
\hline
diastolicGt90.to.1layhid1 & -5.3138288\\
\hline
age45To64.to.1layhid1 & -10.9854888\\
\hline
age65older.to.1layhid1 & 36.4614114\\
\hline
pregnant1.to.1layhid1 & 9.7337152\\
\hline
pregnant2.to.1layhid1 & -16.5880762\\
\hline
pregnant3To4.to.1layhid1 & -7.5045887\\
\hline
pregnant5To7.to.1layhid1 & -10.2882513\\
\hline
pregnant8Plus.to.1layhid1 & -0.7988814\\
\hline
Intercept.to.diabetespos & 0.8903213\\
\hline
1layhid1.to.diabetespos & -0.7406113\\
\hline
\end{tabular}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{plot}\NormalTok{(NetworkModel, }\AttributeTok{rep=}\StringTok{"best"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics{STA551EB_files/figure-latex/unnamed-chunk-153-1} 

}

\caption{Single-layer backpropagation Neural network model for Pima Indian diabetes}\label{fig:unnamed-chunk-153}
\end{figure}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{logiModel }\OtherTok{=} \FunctionTok{glm}\NormalTok{(}\FunctionTok{factor}\NormalTok{(diabetes) }\SpecialCharTok{\textasciitilde{}}\NormalTok{., }\AttributeTok{family =}\NormalTok{ binomial, }\AttributeTok{data =}\NormalTok{ Pima)}
\FunctionTok{pander}\NormalTok{(}\FunctionTok{summary}\NormalTok{(logiModel)}\SpecialCharTok{$}\NormalTok{coefficients)}
\end{Highlighting}
\end{Shaded}

\begin{longtable}[]{@{}
  >{\centering\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.3649}}
  >{\centering\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.1486}}
  >{\centering\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.1757}}
  >{\centering\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.1486}}
  >{\centering\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.1622}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\centering
~
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
Estimate
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
Std. Error
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
z value
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
Pr(\textgreater\textbar z\textbar)
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{(Intercept)} & -5.303 & 0.7241 & -7.324 & 2.411e-13 \\
\textbf{mass} & 0.08627 & 0.01911 & 4.514 & 6.357e-06 \\
\textbf{pedigree} & 2.366 & 0.6969 & 3.395 & 0.000685 \\
\textbf{impute.log.insulin} & 0.3452 & 0.7145 & 0.4831 & 0.629 \\
\textbf{impute.triceps} & -0.06055 & 1.082 & -0.05595 & 0.9554 \\
\textbf{grp.glucose{[}117,137{]}} & 0.8903 & 0.2431 & 3.663 & 0.0002494 \\
\textbf{grp.glucose\textgreater{} 137} & 1.92 & 0.2639 & 7.277 & 3.414e-13 \\
\textbf{grp.diastolic{[}80,90{]}} & -0.1163 & 0.2245 & -0.5179 & 0.6045 \\
\textbf{grp.diastolic\textgreater{} 90} & -0.242 & 0.4281 & -0.5654 & 0.5718 \\
\textbf{grp.age{[}45, 64{]}} & 0.3513 & 0.2616 & 1.343 & 0.1793 \\
\textbf{grp.age65+} & -0.6649 & 0.6581 & -1.01 & 0.3124 \\
\textbf{grp.pregnant1} & -0.2712 & 0.3609 & -0.7517 & 0.4523 \\
\textbf{grp.pregnant2} & -0.2223 & 0.3913 & -0.568 & 0.5701 \\
\textbf{grp.pregnant3-4} & 0.435 & 0.3381 & 1.287 & 0.1982 \\
\textbf{grp.pregnant5-7} & 0.6543 & 0.3326 & 1.967 & 0.04919 \\
\textbf{grp.pregnant8+} & 1.107 & 0.3553 & 3.114 & 0.001844 \\
\end{longtable}

\hypertarget{about-cross-validation-in-neural-network}{%
\subsubsection{About Cross-validation in Neural Network}\label{about-cross-validation-in-neural-network}}

The algorithm of Cross-validation is primarily used for tuning hyper-parameters. For example, in the sigmoid perceptron, the optimal cut-off scores for the binary decision can be obtained through cross-validation. One of the important hyperparameters in the neural network model is the learning rate \(\alpha\) (in the backpropagation algorithm) that impacts the learning speed in training neural network models.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{n0 }\OtherTok{=} \FunctionTok{dim}\NormalTok{(trainDat)[}\DecValTok{1}\NormalTok{]}\SpecialCharTok{/}\DecValTok{5}
\NormalTok{cut.off.score }\OtherTok{=} \FunctionTok{seq}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{1}\NormalTok{, }\AttributeTok{length =} \DecValTok{22}\NormalTok{)[}\SpecialCharTok{{-}}\FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{22}\NormalTok{)]        }\CommentTok{\# candidate cut off prob}
\NormalTok{pred.accuracy }\OtherTok{=} \FunctionTok{matrix}\NormalTok{(}\DecValTok{0}\NormalTok{,}\AttributeTok{ncol=}\DecValTok{20}\NormalTok{, }\AttributeTok{nrow=}\DecValTok{5}\NormalTok{, }\AttributeTok{byrow =}\NormalTok{ T)  }\CommentTok{\# null vector for storing prediction accuracy}
\DocumentationTok{\#\#\#}
\ControlFlowTok{for}\NormalTok{ (i }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\DecValTok{5}\NormalTok{)\{}
\NormalTok{  valid.id }\OtherTok{=}\NormalTok{ ((i}\DecValTok{{-}1}\NormalTok{)}\SpecialCharTok{*}\NormalTok{n0 }\SpecialCharTok{+} \DecValTok{1}\NormalTok{)}\SpecialCharTok{:}\NormalTok{(i}\SpecialCharTok{*}\NormalTok{n0)}
\NormalTok{  valid.data }\OtherTok{=}\NormalTok{ trainDat[valid.id,]}
\NormalTok{  train.data }\OtherTok{=}\NormalTok{ trainDat[}\SpecialCharTok{{-}}\NormalTok{valid.id,]}
  \DocumentationTok{\#\#\#\#}
\NormalTok{  train.model }\OtherTok{=} \FunctionTok{neuralnet}\NormalTok{(modelFormula,}
                         \AttributeTok{data =}\NormalTok{ train.data,}
                         \AttributeTok{hidden =} \DecValTok{1}\NormalTok{,               }\CommentTok{\# single layer NN}
                         \AttributeTok{rep =} \DecValTok{1}\NormalTok{,                  }\CommentTok{\# number of replicates in training NN}
                         \AttributeTok{threshold =} \FloatTok{0.01}\NormalTok{,         }\CommentTok{\# threshold for the partial derivatives as stopping criteria.}
                         \AttributeTok{learningrate =} \FloatTok{0.1}\NormalTok{,       }\CommentTok{\# user selected rate}
                         \AttributeTok{algorithm =} \StringTok{"rprop+"}
\NormalTok{                         )}
\NormalTok{    pred.nn.score }\OtherTok{=} \FunctionTok{predict}\NormalTok{(train.model, valid.data)}
    \ControlFlowTok{for}\NormalTok{(j }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\DecValTok{20}\NormalTok{)\{}
    \CommentTok{\#pred.status = rep(0,length(pred.nn.score))}
\NormalTok{    pred.status }\OtherTok{=} \FunctionTok{as.numeric}\NormalTok{(pred.nn.score }\SpecialCharTok{\textgreater{}}\NormalTok{ cut.off.score[j])}
\NormalTok{    a11 }\OtherTok{=} \FunctionTok{sum}\NormalTok{(pred.status }\SpecialCharTok{==}\NormalTok{ valid.data[,}\DecValTok{17}\NormalTok{])}
\NormalTok{    pred.accuracy[i,j] }\OtherTok{=}\NormalTok{ a11}\SpecialCharTok{/}\FunctionTok{length}\NormalTok{(pred.nn.score)}
\NormalTok{  \}}
\NormalTok{\}}
\DocumentationTok{\#\#\#  }
\NormalTok{avg.accuracy }\OtherTok{=} \FunctionTok{apply}\NormalTok{(pred.accuracy, }\DecValTok{2}\NormalTok{, mean)}
\NormalTok{max.id }\OtherTok{=} \FunctionTok{which}\NormalTok{(avg.accuracy }\SpecialCharTok{==}\FunctionTok{max}\NormalTok{(avg.accuracy ))}
\DocumentationTok{\#\#\# visual representation}
\NormalTok{tick.label }\OtherTok{=} \FunctionTok{as.character}\NormalTok{(}\FunctionTok{round}\NormalTok{(cut.off.score,}\DecValTok{2}\NormalTok{))}
\FunctionTok{plot}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\DecValTok{20}\NormalTok{, avg.accuracy, }\AttributeTok{type =} \StringTok{"b"}\NormalTok{,}
     \AttributeTok{xlim=}\FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{20}\NormalTok{), }
     \AttributeTok{ylim=}\FunctionTok{c}\NormalTok{(}\FloatTok{0.5}\NormalTok{,}\DecValTok{1}\NormalTok{), }
     \AttributeTok{axes =} \ConstantTok{FALSE}\NormalTok{,}
     \AttributeTok{xlab =} \StringTok{"Cut{-}off Score"}\NormalTok{,}
     \AttributeTok{ylab =} \StringTok{"Accuracy"}\NormalTok{,}
     \AttributeTok{main =} \StringTok{"5{-}fold CV performance"}
\NormalTok{     )}
\FunctionTok{axis}\NormalTok{(}\DecValTok{1}\NormalTok{, }\AttributeTok{at=}\DecValTok{1}\SpecialCharTok{:}\DecValTok{20}\NormalTok{, }\AttributeTok{label =}\NormalTok{ tick.label, }\AttributeTok{las =} \DecValTok{2}\NormalTok{)}
\FunctionTok{axis}\NormalTok{(}\DecValTok{2}\NormalTok{)}
\FunctionTok{segments}\NormalTok{(max.id, }\FloatTok{0.5}\NormalTok{, max.id, avg.accuracy[max.id], }\AttributeTok{col =} \StringTok{"red"}\NormalTok{)}
\FunctionTok{text}\NormalTok{(max.id, avg.accuracy[max.id]}\SpecialCharTok{+}\FloatTok{0.03}\NormalTok{, }\FunctionTok{as.character}\NormalTok{(}\FunctionTok{round}\NormalTok{(avg.accuracy[max.id],}\DecValTok{4}\NormalTok{)), }\AttributeTok{col =} \StringTok{"red"}\NormalTok{, }\AttributeTok{cex =} \FloatTok{0.8}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{STA551EB_files/figure-latex/unnamed-chunk-155-1.pdf}

\hfill\break

\hypertarget{testing-model-performance}{%
\subsubsection{Testing Model Performance}\label{testing-model-performance}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#Test the resulting output}
\NormalTok{nn.results }\OtherTok{\textless{}{-}} \FunctionTok{predict}\NormalTok{(NetworkModel, testDat)}
\NormalTok{results }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(}\AttributeTok{actual =}\NormalTok{ testDat[,}\DecValTok{17}\NormalTok{], }\AttributeTok{prediction =}\NormalTok{ nn.results }\SpecialCharTok{\textgreater{}}\NormalTok{ .}\DecValTok{57}\NormalTok{)}
\NormalTok{confMatrix }\OtherTok{=} \FunctionTok{table}\NormalTok{(results}\SpecialCharTok{$}\NormalTok{prediction, results}\SpecialCharTok{$}\NormalTok{actual)               }\CommentTok{\# confusion matrix}
\NormalTok{accuracy}\OtherTok{=}\FunctionTok{sum}\NormalTok{(results}\SpecialCharTok{$}\NormalTok{actua }\SpecialCharTok{==}\NormalTok{ results}\SpecialCharTok{$}\NormalTok{prediction)}\SpecialCharTok{/}\FunctionTok{length}\NormalTok{(results}\SpecialCharTok{$}\NormalTok{prediction)}
\FunctionTok{list}\NormalTok{(}\AttributeTok{confusion.matrix =}\NormalTok{ confMatrix, }\AttributeTok{accuracy =}\NormalTok{ accuracy)       }
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## $confusion.matrix
##        
##           0   1
##   FALSE 276  94
##   TRUE   53  84
## 
## $accuracy
## [1] 0.7100592
\end{verbatim}

\hypertarget{roc-analysis}{%
\subsubsection{ROC Analysis}\label{roc-analysis}}

Recall that the ROC curve is the plot of sensitivity against (1 - specificity) calculated from the confusion matrix based on a sequence of selected cut-off scores. Definitions of sensitivity and specificity are given in the following confusion matrix

\begin{figure}

{\centering \includegraphics[width=0.95\linewidth]{img07/w07-SenSpec} 

}

\caption{Confusion matrix and sensitivity and specificity.}\label{fig:unnamed-chunk-157}
\end{figure}

Next, we construct a ROC for the above NN model based on the training data set.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{nn.results }\OtherTok{=} \FunctionTok{predict}\NormalTok{(NetworkModel, trainDat)  }\CommentTok{\# Keep in mind that trainDat is a matrix!}
\NormalTok{cut0 }\OtherTok{=} \FunctionTok{seq}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{1}\NormalTok{, }\AttributeTok{length =} \DecValTok{20}\NormalTok{)}
\NormalTok{SenSpe }\OtherTok{=} \FunctionTok{matrix}\NormalTok{(}\DecValTok{0}\NormalTok{, }\AttributeTok{ncol =} \FunctionTok{length}\NormalTok{(cut0), }\AttributeTok{nrow =} \DecValTok{2}\NormalTok{, }\AttributeTok{byrow =} \ConstantTok{FALSE}\NormalTok{)}
\ControlFlowTok{for}\NormalTok{ (i }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\FunctionTok{length}\NormalTok{(cut0))\{}
\NormalTok{    a }\OtherTok{=} \FunctionTok{sum}\NormalTok{(trainDat[,}\StringTok{"diabetespos"}\NormalTok{] }\SpecialCharTok{==} \DecValTok{1} \SpecialCharTok{\&}\NormalTok{ (nn.results }\SpecialCharTok{\textgreater{}}\NormalTok{ cut0[i]))}
\NormalTok{    d }\OtherTok{=} \FunctionTok{sum}\NormalTok{(trainDat[,}\StringTok{"diabetespos"}\NormalTok{] }\SpecialCharTok{==} \DecValTok{0} \SpecialCharTok{\&}\NormalTok{ (nn.results }\SpecialCharTok{\textless{}}\NormalTok{ cut0[i]))}
\NormalTok{    b }\OtherTok{=} \FunctionTok{sum}\NormalTok{(trainDat[,}\StringTok{"diabetespos"}\NormalTok{] }\SpecialCharTok{==} \DecValTok{0} \SpecialCharTok{\&}\NormalTok{ (nn.results }\SpecialCharTok{\textgreater{}}\NormalTok{ cut0[i]))    }
\NormalTok{    c }\OtherTok{=} \FunctionTok{sum}\NormalTok{(trainDat[,}\StringTok{"diabetespos"}\NormalTok{] }\SpecialCharTok{==} \DecValTok{1} \SpecialCharTok{\&}\NormalTok{ (nn.results }\SpecialCharTok{\textless{}}\NormalTok{ cut0[i]))   }
\NormalTok{    sen }\OtherTok{=}\NormalTok{ a}\SpecialCharTok{/}\NormalTok{(a }\SpecialCharTok{+}\NormalTok{ c)}
\NormalTok{    spe }\OtherTok{=}\NormalTok{ d}\SpecialCharTok{/}\NormalTok{(b }\SpecialCharTok{+}\NormalTok{ d)}
\NormalTok{    SenSpe[,i] }\OtherTok{=} \FunctionTok{c}\NormalTok{(sen, spe)}
\NormalTok{\}}
\CommentTok{\# plotting ROC}
\FunctionTok{plot}\NormalTok{(}\DecValTok{1}\SpecialCharTok{{-}}\NormalTok{SenSpe[}\DecValTok{2}\NormalTok{,], SenSpe[}\DecValTok{1}\NormalTok{,], }\AttributeTok{type =}\StringTok{"l"}\NormalTok{, }\AttributeTok{xlim=}\FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{1}\NormalTok{), }\AttributeTok{ylim=}\FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{1}\NormalTok{),}
     \AttributeTok{xlab =} \StringTok{"1 {-} specificity"}\NormalTok{, }\AttributeTok{ylab =} \StringTok{"Sensitivity"}\NormalTok{, }\AttributeTok{lty =} \DecValTok{1}\NormalTok{,}
     \AttributeTok{main =} \StringTok{"ROC Curve"}\NormalTok{, }\AttributeTok{col =} \StringTok{"blue"}\NormalTok{)}
\FunctionTok{abline}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{1}\NormalTok{, }\AttributeTok{lty =} \DecValTok{2}\NormalTok{, }\AttributeTok{col =} \StringTok{"red"}\NormalTok{)}

\DocumentationTok{\#\# A better approx of ROC, need library \{pROC\}}
\NormalTok{  prediction }\OtherTok{=} \FunctionTok{as.vector}\NormalTok{(nn.results)}
\NormalTok{  category }\OtherTok{=}\NormalTok{ trainDat[,}\StringTok{"diabetespos"}\NormalTok{] }\SpecialCharTok{==} \DecValTok{1}
\NormalTok{  ROCobj }\OtherTok{\textless{}{-}} \FunctionTok{roc}\NormalTok{(category, prediction)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Setting levels: control = FALSE, case = TRUE
\end{verbatim}

\begin{verbatim}
## Setting direction: controls < cases
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{  AUC }\OtherTok{=} \FunctionTok{auc}\NormalTok{(ROCobj)[}\DecValTok{1}\NormalTok{]}
  \DocumentationTok{\#\#}
\DocumentationTok{\#\#\#}
\FunctionTok{text}\NormalTok{(}\FloatTok{0.8}\NormalTok{, }\FloatTok{0.3}\NormalTok{, }\FunctionTok{paste}\NormalTok{(}\StringTok{"AUC = "}\NormalTok{, }\FunctionTok{round}\NormalTok{(AUC,}\DecValTok{4}\NormalTok{)), }\AttributeTok{col =} \StringTok{"purple"}\NormalTok{, }\AttributeTok{cex =} \FloatTok{0.9}\NormalTok{)}
\FunctionTok{legend}\NormalTok{(}\StringTok{"bottomright"}\NormalTok{, }\FunctionTok{c}\NormalTok{(}\StringTok{"ROC of the model"}\NormalTok{, }\StringTok{"Random guessing"}\NormalTok{), }\AttributeTok{lty=}\FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{),}
       \AttributeTok{col =} \FunctionTok{c}\NormalTok{(}\StringTok{"blue"}\NormalTok{, }\StringTok{"red"}\NormalTok{), }\AttributeTok{bty =} \StringTok{"n"}\NormalTok{, }\AttributeTok{cex =} \FloatTok{0.8}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics{STA551EB_files/figure-latex/unnamed-chunk-158-1} 

}

\caption{Figure 14: ROC Curve of the neural network model.}\label{fig:unnamed-chunk-158}
\end{figure}

The above ROC curve indicates that the underlying neural network is better than the random guess since the area under the curve is significantly greater than 0.5. In general, if the area under the ROC curve is greater than 0.65, we say the predictive power of the underlying model is acceptable.

\hfill\break

\hypertarget{about-deep-learning}{%
\subsection{About Deep Learning}\label{about-deep-learning}}

\emph{From Wikipedia, the free encyclopedia}

Deep learning is part of a broader family of machine learning methods, which is based on artificial neural networks with representation learning. The adjective ``deep'' in deep learning refers to the use of multiple layers in the network. Methods used can be either supervised, semi-supervised, or unsupervised.

Deep-learning architectures such as deep neural networks, deep belief networks, deep reinforcement learning, recurrent neural networks, convolutional neural networks, and transformers have been applied to fields including computer vision, speech recognition, natural language processing, machine translation, bioinformatics, drug design, medical image analysis, climate science, material inspection and board game programs, where they have produced results comparable to and in some cases surpassing human expert performance.

\hfill\break

\hypertarget{clustering-algorithms}{%
\section{Clustering Algorithms}\label{clustering-algorithms}}

Clustering and principal component analysis are two classical multivariate statistical methods. These two methods and their extensions are now the core methods in machine learning. We will briefly introduce both of them in this class.

\hfill\break

\hypertarget{an-overview-of-supervised-ml-algorithms}{%
\chapter{An Overview of Supervised ML Algorithms}\label{an-overview-of-supervised-ml-algorithms}}

When crunching data to model business decisions, you are most typically using supervised and unsupervised learning methods.

Algorithms are often grouped by similarity in terms of their function (how they work). Again, there will be no perfect classification of machine learning algorithms and there is also no way to exhaust all machine learning algorithms. We only list those most commonly used algorithms here based on similarity to keep things simple.

For illustrative purposes, I will use some examples from the well-known \textbf{Introduction to statistical learning: with Applications in R} \url{https://www.statlearning.com/}.

This note will outline the supervised learning algorithms including statistical models and those developed by the machine learning community. We will build a decision tree model for the diabetes data set in the case study.

\hypertarget{statistical-algorithms}{%
\section{Statistical Algorithms}\label{statistical-algorithms}}

Regression is concerned with modeling the relationship between variables that is iteratively refined using a performance measure defined based on errors in the predictions made by the model. Regression methods are a workhorse of statistics and have been the backbone of statistical machine learning.

The most popular regression algorithms are:

\begin{itemize}
\tightlist
\item
  Ordinary Least Squares Regression (OLSR)
\item
  Linear Regression
\item
  Logistic Regression
\item
  Step-wise Regression
\item
  LOcally Estimated Scatter-plot Smoothing (LOESS) - a nonparametric regression.
\end{itemize}

\hfill\break

\hypertarget{loess-regression}{%
\subsection{LOESS Regression}\label{loess-regression}}

The following visualization represents the LOESS regression based on two variables \textbf{Sales} and \textbf{Price} in the data set \emph{Carseats} in the book \textbf{ISLR}.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{data}\NormalTok{(}\StringTok{"Carseats"}\NormalTok{)}
\FunctionTok{kable}\NormalTok{(}\FunctionTok{head}\NormalTok{(Carseats))}
\end{Highlighting}
\end{Shaded}

\begin{tabular}{r|r|r|r|r|r|l|r|r|l|l}
\hline
Sales & CompPrice & Income & Advertising & Population & Price & ShelveLoc & Age & Education & Urban & US\\
\hline
9.50 & 138 & 73 & 11 & 276 & 120 & Bad & 42 & 17 & Yes & Yes\\
\hline
11.22 & 111 & 48 & 16 & 260 & 83 & Good & 65 & 10 & Yes & Yes\\
\hline
10.06 & 113 & 35 & 10 & 269 & 80 & Medium & 59 & 12 & Yes & Yes\\
\hline
7.40 & 117 & 100 & 4 & 466 & 97 & Medium & 55 & 14 & Yes & Yes\\
\hline
4.15 & 141 & 64 & 3 & 340 & 128 & Bad & 38 & 13 & Yes & No\\
\hline
10.81 & 124 & 113 & 13 & 501 & 72 & Bad & 78 & 16 & No & Yes\\
\hline
\end{tabular}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{lw1 }\OtherTok{=} \FunctionTok{loess}\NormalTok{(Sales }\SpecialCharTok{\textasciitilde{}}\NormalTok{ Price, }\AttributeTok{data =}\NormalTok{ Carseats)}
\FunctionTok{plot}\NormalTok{(Sales }\SpecialCharTok{\textasciitilde{}}\NormalTok{ Price, }\AttributeTok{data =}\NormalTok{ Carseats, }\AttributeTok{pch=}\DecValTok{19}\NormalTok{, }\AttributeTok{cex=}\FloatTok{0.8}\NormalTok{)}
\NormalTok{j }\OtherTok{=} \FunctionTok{order}\NormalTok{(Carseats}\SpecialCharTok{$}\NormalTok{Price)  }\CommentTok{\# sort the data vector and returns the index }
                           \CommentTok{\# of the values of the original data vector}
\FunctionTok{lines}\NormalTok{(Carseats}\SpecialCharTok{$}\NormalTok{Price[j],lw1}\SpecialCharTok{$}\NormalTok{fitted[j],}\AttributeTok{col=}\StringTok{"red"}\NormalTok{,}\AttributeTok{lwd=}\DecValTok{3}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics{STA551EB_files/figure-latex/unnamed-chunk-161-1} 

}

\caption{Figure 5. LOESS regression: Sales ~ Prices}\label{fig:unnamed-chunk-161}
\end{figure}

The \texttt{loess()} is a data-driven nonparametric regression (local polynomial regression), in other words, the explicit model parameters in an explicitly expressed model to estimate. \texttt{loess} regression is analogous to single variable regression such as simple linear and nonlinear regression models.

To plot the \texttt{smooth} fitted regression curve, we need to use function \texttt{order()} the indices of the original data vector after it was ordered. For example,

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x }\OtherTok{=} \FunctionTok{c}\NormalTok{(}\DecValTok{3}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{4}\NormalTok{, }\SpecialCharTok{{-}}\DecValTok{5}\NormalTok{)}
\FunctionTok{order}\NormalTok{(x)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 5 3 2 1 4
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\DocumentationTok{\#\# The above index can sort the data below}
\NormalTok{x[}\FunctionTok{order}\NormalTok{(x)]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] -5  0  1  3  4
\end{verbatim}

We can use the loess nonparametric regression model to predict as usual using the generic function \texttt{predict()} with an input data frame.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{lw1 }\OtherTok{=} \FunctionTok{loess}\NormalTok{(Sales }\SpecialCharTok{\textasciitilde{}}\NormalTok{ Price, }\AttributeTok{data =}\NormalTok{ Carseats)}
\FunctionTok{predict}\NormalTok{(lw1, }\FunctionTok{data.frame}\NormalTok{(}\AttributeTok{Price =}\FunctionTok{c}\NormalTok{(}\DecValTok{134}\NormalTok{,}\DecValTok{121}\NormalTok{)),}\AttributeTok{se =} \ConstantTok{TRUE}\NormalTok{)  }\CommentTok{\# new data must be within }
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## $fit
## [1] 6.751464 7.371896
## 
## $se.fit
## [1] 0.2261343 0.2216638
## 
## $residual.scale
## [1] 2.518948
## 
## $df
## [1] 393.7244
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
                                                       \CommentTok{\# Existing price range}
\end{Highlighting}
\end{Shaded}

\hypertarget{regularized-regression}{%
\subsection{Regularized Regression}\label{regularized-regression}}

We have discussed some degree of detail in linear and logistic regression models from both classical statistics perspective and machine learning perspective in terms of model training and performance evaluation.

The following \textbf{regularized regression} algorithms are recently developed learning algorithms modified from classical statistics.

\begin{itemize}
\tightlist
\item
  \textbf{Ridge Regression}
\end{itemize}

\textbf{Ridge regression} does not reduce the number of correlated feature variables. It brings bias to the estimated regression coefficient to reduce the impact of multi-correlated feature variables. In other words, it sacrifices the unbiasedness of the estimated regression to gain the stability of the estimation.

\begin{itemize}
\tightlist
\item
  \textbf{Least Absolute Shrinkage and Selection Operator (LASSO)}
\end{itemize}

\textbf{LASSO} filters the feature variables with a small magnitude of the absolute regression coefficients. All numerical feature variables must be standardized when using LASSO regression for prediction. Since some of the feature variables will be filtered out from the model. It is considered a dimension reduction method that becomes a popular tool in the machine learning community.

The following figure explains the relationships between regular least square regression, ridge regression, and LASSO.

\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{img07/w07-LASSO-Ridge-LSR} 

}

\caption{Relationship between least square, ridge, and LASSO regressions.}\label{fig:unnamed-chunk-164}
\end{figure}

Because both \textbf{ridge} and \textbf{LASSO} fall into the same theoretical framework (although functioning in very different ways), Stanford statisticians developed an R library, \texttt{glmnet} to implement various regularized regression including both of these two regularized regression methods.

\hfill\break

\hypertarget{instance-based-algorithms}{%
\subsection{Instance-based Algorithms}\label{instance-based-algorithms}}

First of all, observations/samples/instances all mean the same thing in machine learning.

The instance-based learning model is a decision problem with instances or examples of training data that are deemed important or required to the model. Such methods typically build up a database of example data and compare new data to the database using a similarity measure in order to find the best match and make a prediction. For this reason, instance-based methods are also called winner-take-all methods and memory-based learning (sometimes also called lazy learning. Focus is put on the representation of the stored instances and similarity measures used between instances.

The most popular instance-based algorithms are:

\begin{itemize}
\tightlist
\item
  k-Nearest Neighbor (kNN)
\item
  Support Vector Machines (SVM)
\end{itemize}

\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{img07/w07-Knn-SVM} 

}

\caption{Instance-based learning algorithms: KNN and SVM.}\label{fig:unnamed-chunk-165}
\end{figure}

Both kNN and SVM are intuitive. The more instances the more the accuracy. ISLR (2nd edition) has case studies using R for kNN (using \textbf{knn()} in library \textbf{\{class\}} in section 4.7.6, starting from page 181) and SVM (using \textbf{svm()} in library \textbf{\{e1071\}} in sections 9.6.1 and 9.6.2, starting from page 389).

\hfill\break

\hypertarget{nauxefve-bayes---a-bayesian-algorithm}{%
\subsection{Nave Bayes - A Bayesian Algorithm}\label{nauxefve-bayes---a-bayesian-algorithm}}

Bayesian methods are those that explicitly apply Bayes' Theorem for problems such as classification and regression. There several Bayesian algorithms have been developed so far. We only introduce the basic but commonly used in practice - Nave Bayes.

The Nave Bayes classifier is a simple probabilistic classifier that is based on the Bayes theorem but with strong assumptions regarding independence. Historically, this technique became popular with applications in email filtering, spam detection, and document categorization. Although it is often outperformed by other techniques, and despite the nave design and oversimplified assumptions, this classifier can perform well in many complex real-world problems.

The theory behind Nave Bayes is straightforward as depicted in the following.

\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{img07/w07-Naive-Bayes} 

}

\caption{Naive Bayes Classifier.}\label{fig:unnamed-chunk-166}
\end{figure}

There are several libraries in R that have the function to implement ave Bayes. ISLR has a lab on the application of ave Bayes (section 4.7.5, starting from page 180) using the \textbf{naiveBayes()} function in R library \textbf{\{e1071\}}.

\hfill\break

\hypertarget{decision-tree-algorithms}{%
\section{Decision Tree Algorithms}\label{decision-tree-algorithms}}

\url{https://github.com/pengdsci/STA551/blob/main/w07/img/w07.1-GIFtree.gif}

The Decision Tree (DT) algorithm is based on conditional probabilities. Unlike the other classification algorithms, decision trees generate rules. A rule is a conditional statement that can easily be understood by humans and easily used within a database to identify a set of records. It is easy to interpret and implement in real-world applications. Among several basic tree-based algorithms, Classification and Regression Tree (CART) is most frequently used in practice.

This subsection focuses on the basic decision tree with some technical description of steps in decision tree induction. The general structure of a decision tree algorithm is in the following example of predicting the survival of Titanic passengers.

\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{img07/w07-DT-Titanic} 

}

\caption{Illustration of decision tree algorithm: predicting Titanic survival.}\label{fig:unnamed-chunk-168}
\end{figure}

The above decision tree involves three variables: sex, age, and sibsp (sibling and spouse). We can easily convert the tree to a set of rules (conditional statements) to make a prediction of the survival status for a new incoming data point.

\hypertarget{structure-and-technical-terms}{%
\subsection{Structure and Technical Terms}\label{structure-and-technical-terms}}

The following diagram illustrates the basic structure of a decision tree.

\begin{figure}

{\centering \includegraphics[width=0.7\linewidth]{img07/w07.1-DecisionTreeStructure} 

}

\caption{Decision tree structure.}\label{fig:unnamed-chunk-169}
\end{figure}

\textbf{Root Node}: It represents the entire population or sample and this further gets divided into two or more homogeneous sets.

\textbf{Splitting}: It is a process of dividing a node into two or more sub-nodes.

\textbf{Decision Node}: When a sub-node splits into further sub-nodes, then it is called the decision node.

\textbf{Leaf / Terminal Node}: Nodes that do not split are called Leaf or Terminal Node.

\textbf{Pruning}: When we remove sub-nodes of a decision node, this process is called pruning. You can say the opposite process of splitting.

\textbf{Branch / Sub-Tree}: A subsection of the entire tree is called a branch or sub-tree.

\textbf{Parent and Child Node}: A node, which is divided into sub-nodes is called a parent node of sub-nodes whereas sub-nodes are the child of a parent node.

\hfill\break

The following example based on toy data illustrates how a decision grows and how to use a decision tree to make predictions.

The toy data set is given below.

\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{img07/w07-DT-ToyData} 

}

\caption{Decision tree structure using a toy data.}\label{fig:unnamed-chunk-170}
\end{figure}

The fully grown tree is given below (note the variable \textbf{class} is the binary response variable).

\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{img07/w07-DT-ToyExample} 

}

\caption{Fully grown decision tree using a toy data.}\label{fig:unnamed-chunk-171}
\end{figure}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{DataSet }\OtherTok{=} \FunctionTok{data.frame}\NormalTok{(}
\AttributeTok{Age =} \FunctionTok{c}\NormalTok{(}\StringTok{"Youth"}\NormalTok{, }\StringTok{"Youth"}\NormalTok{, }\StringTok{"Middle\_aged"}\NormalTok{, }\StringTok{"Senior"}\NormalTok{, }\StringTok{"Senior"}\NormalTok{,}\StringTok{"Senior"}\NormalTok{,}\StringTok{"Middle\_aged"}\NormalTok{,}\StringTok{"Youth"}\NormalTok{,}\StringTok{"Youth"}\NormalTok{,}\StringTok{"Senior"}\NormalTok{,}\StringTok{"Youth"}\NormalTok{,}\StringTok{"Middle\_aged"}\NormalTok{,}
        \StringTok{"Middle\_aged"}\NormalTok{,}\StringTok{"Senior"}\NormalTok{),}
\AttributeTok{Income =} \FunctionTok{c}\NormalTok{(}\StringTok{"High"}\NormalTok{,}\StringTok{"High"}\NormalTok{,}\StringTok{"High"}\NormalTok{,}\StringTok{"Medium"}\NormalTok{,}\StringTok{"Low"}\NormalTok{, }\StringTok{"Low"}\NormalTok{,}\StringTok{"Low"}\NormalTok{,}\StringTok{"Medium"}\NormalTok{,}\StringTok{"Low"}\NormalTok{,}\StringTok{"Medium"}\NormalTok{,}\StringTok{"Medium"}\NormalTok{,}\StringTok{"Medium"}\NormalTok{,}\StringTok{"High"}\NormalTok{,}\StringTok{"Medium"}\NormalTok{),}
\AttributeTok{Student =} \FunctionTok{c}\NormalTok{(}\StringTok{"No"}\NormalTok{, }\StringTok{"No"}\NormalTok{,}\StringTok{"No"}\NormalTok{,}\StringTok{"No"}\NormalTok{,}\StringTok{"Yes"}\NormalTok{,}\StringTok{"Yes"}\NormalTok{,}\StringTok{"Yes"}\NormalTok{,}\StringTok{"No"}\NormalTok{,}\StringTok{"Yes"}\NormalTok{,}\StringTok{"Yes"}\NormalTok{,}\StringTok{"Yes"}\NormalTok{,}\StringTok{"No"}\NormalTok{,}\StringTok{"Yes"}\NormalTok{,}\StringTok{"No"}\NormalTok{),}
\AttributeTok{CreditRating =} \FunctionTok{c}\NormalTok{(}\StringTok{"Fair"}\NormalTok{, }\StringTok{"Excellent"}\NormalTok{,}\StringTok{"Fair"}\NormalTok{,}\StringTok{"Fair"}\NormalTok{,}\StringTok{"Fair"}\NormalTok{,}\StringTok{"Excellent"}\NormalTok{,}\StringTok{"Excellent"}\NormalTok{,}
                 \StringTok{"Fair"}\NormalTok{,}\StringTok{"Fair"}\NormalTok{,}\StringTok{"Fair"}\NormalTok{,}\StringTok{"Excellent"}\NormalTok{,}\StringTok{"Excellent"}\NormalTok{,}\StringTok{"Fair"}\NormalTok{,}\StringTok{"Excellent"}\NormalTok{),}
\AttributeTok{Class =} \FunctionTok{c}\NormalTok{(}\StringTok{"No"}\NormalTok{, }\StringTok{"No"}\NormalTok{, }\StringTok{"Yes"}\NormalTok{, }\StringTok{"Yes"}\NormalTok{,}\StringTok{"Yes"}\NormalTok{,}\StringTok{"No"}\NormalTok{,}\StringTok{"Yes"}\NormalTok{, }\StringTok{"No"}\NormalTok{, }\StringTok{"Yes"}\NormalTok{,}\StringTok{"Yes"}\NormalTok{,}\StringTok{"Yes"}\NormalTok{,}\StringTok{"Yes"}\NormalTok{,}\StringTok{"Yes"}\NormalTok{,}\StringTok{"No"}\NormalTok{)}
\NormalTok{)}
\CommentTok{\#pander(DataSet)}
\end{Highlighting}
\end{Shaded}

\hypertarget{decision-tree-growing---impurity-measures}{%
\subsection{Decision Tree Growing - Impurity Measures}\label{decision-tree-growing---impurity-measures}}

Growing a decision tree is an iterative process of splitting the feature space into some sub-spaces according to certain criteria defined based on feature variables. The predictive performance of a decision is dependent on the size of the trained tree. A small size will cause underfitting issues and a large size will result in overfitting issues.

The questions are (1) how to control the size of a decision to obtain the best performance; (2) how to select the feature variables to define the root and subsequent child nodes; (3) how to split a feature variable.

\textbf{Gini index} and \textbf{entropy} are the two popular impurity measures commonly used in decision tree induction.

\hfill\break

\hypertarget{gini-index}{%
\subsubsection{Gini Index}\label{gini-index}}

\begin{itemize}
\tightlist
\item
  \textbf{Gini Index} considers a split for each attribute (for a continuous attribute, usually considers binary split). The Gini Index measures the impurity of subgroups (D) split by a feature variable.
\end{itemize}

\[
\mbox{Gini}(D)= \sum_{i=1}^m p_i(1-p_i)= 1 - \sum_{i=1}^m p_i^2
\]
Where \(p_i\) is the probability of an object that is being classified to a particular class.

For example, we calculate the Gini index using the above decision tree. The root node (age) has three child nodes. We show how to calculate the weighted Gini index of feature variable age in the following steps.

\begin{itemize}
\item
  \textbf{Weights}: P(youth) = 5/14, P(middle\_aged) = 4/14, P(senior) = 5/14.
\item
  \textbf{D = Youth}: \(p_1 =P(Yes) = 2/5, p_2 = P(No) = 3/5\), therefore, \({Gini}_{youth} = 1 -p_1^2 - p_2^2 = 1 - 4/25 - 9/25 = 12/25\)
\item
  \textbf{D = Middle\_aged}: \(p_1 =P(Yes) = 4/4, p_2 = P(No) = 0/45\), therefore, \({Gini}_{middle_aged} = 1 -p_1^2 - p_2^2 = 1 - 16/16 - 0/16 = 0\)
\item
  \textbf{D = Senior}: \(p_1 =P(Yes) = 3/5, p_2 = P(No) = 2/5\), therefore, \({Gini}_{senior} = 1 -p_1^2 - p_2^2 = 1 - 9/25 - 4/25 = 12/25\)
\end{itemize}

The \textbf{Gini index} of age is given by

\[
{Gini}_{age} =\frac{5}{14}\times\frac{12}{25} + \frac{4}{14}\times 0 +\frac{5}{14}\times \frac{12}{25} = \frac{5}{14}\times \frac{24}{25}  = \frac{12}{35} \approx  0.343.
\]

\hfill\break

\textbf{R Function for GINI Index}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{GINI.calc }\OtherTok{=} \ControlFlowTok{function}\NormalTok{(DatName, VarName, ClsName)\{}
   \CommentTok{\# }
\NormalTok{   freqTB0 }\OtherTok{=} \FunctionTok{table}\NormalTok{(DatName[,VarName], DatName[,ClsName])}
\NormalTok{   freqTB }\OtherTok{=} \FunctionTok{data.frame}\NormalTok{(}\AttributeTok{NO =}\NormalTok{ freqTB0[, }\DecValTok{1}\NormalTok{], }\AttributeTok{YES =}\NormalTok{ freqTB0[, }\DecValTok{2}\NormalTok{])}
\NormalTok{   freqTB}\SpecialCharTok{$}\NormalTok{Tot }\OtherTok{=}\NormalTok{ freqTB}\SpecialCharTok{$}\NormalTok{NO }\SpecialCharTok{+}\NormalTok{ freqTB}\SpecialCharTok{$}\NormalTok{YES}
\NormalTok{   freqTB}\SpecialCharTok{$}\NormalTok{P1 }\OtherTok{=}\NormalTok{ freqTB}\SpecialCharTok{$}\NormalTok{NO}\SpecialCharTok{/}\NormalTok{freqTB}\SpecialCharTok{$}\NormalTok{Tot}
\NormalTok{   freqTB}\SpecialCharTok{$}\NormalTok{P2 }\OtherTok{=}\NormalTok{ freqTB}\SpecialCharTok{$}\NormalTok{YES}\SpecialCharTok{/}\NormalTok{freqTB}\SpecialCharTok{$}\NormalTok{Tot}
\NormalTok{   freqTB}\SpecialCharTok{$}\NormalTok{CateGINI }\OtherTok{=} \DecValTok{1}\SpecialCharTok{{-}}\NormalTok{(freqTB}\SpecialCharTok{$}\NormalTok{P1)}\SpecialCharTok{\^{}}\DecValTok{2} \SpecialCharTok{{-}}\NormalTok{ (freqTB}\SpecialCharTok{$}\NormalTok{P2)}\SpecialCharTok{\^{}}\DecValTok{2}
\NormalTok{   freqTB}\SpecialCharTok{$}\NormalTok{ROWPER }\OtherTok{=}\NormalTok{ (freqTB}\SpecialCharTok{$}\NormalTok{NO }\SpecialCharTok{+}\NormalTok{ freqTB}\SpecialCharTok{$}\NormalTok{YES)}\SpecialCharTok{/}\FunctionTok{sum}\NormalTok{(freqTB}\SpecialCharTok{$}\NormalTok{Tot)}
\NormalTok{   freqTB}\SpecialCharTok{$}\NormalTok{ComponentGini }\OtherTok{=}\NormalTok{ (freqTB}\SpecialCharTok{$}\NormalTok{CateGINI) }\SpecialCharTok{*}\NormalTok{ (freqTB}\SpecialCharTok{$}\NormalTok{ROWPER)}
\NormalTok{   GINI.idx }\OtherTok{=} \FunctionTok{sum}\NormalTok{(freqTB}\SpecialCharTok{$}\NormalTok{ComponentGini)}
\NormalTok{   GINI.idx}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

\hfill\break

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{giniAge }\OtherTok{=} \FunctionTok{GINI.calc}\NormalTok{(}\AttributeTok{DatName=}\NormalTok{DataSet, }\AttributeTok{VarName=}\StringTok{"Age"}\NormalTok{, }\AttributeTok{ClsName =} \StringTok{"Class"}\NormalTok{) }
\NormalTok{giniIncome }\OtherTok{=} \FunctionTok{GINI.calc}\NormalTok{(}\AttributeTok{DatName=}\NormalTok{DataSet, }\AttributeTok{VarName=}\StringTok{"Income"}\NormalTok{, }\AttributeTok{ClsName =} \StringTok{"Class"}\NormalTok{) }
\NormalTok{giniStudent }\OtherTok{=} \FunctionTok{GINI.calc}\NormalTok{(}\AttributeTok{DatName=}\NormalTok{DataSet, }\AttributeTok{VarName=}\StringTok{"Student"}\NormalTok{, }\AttributeTok{ClsName =} \StringTok{"Class"}\NormalTok{) }
\NormalTok{giniCreditRating }\OtherTok{=} \FunctionTok{GINI.calc}\NormalTok{(}\AttributeTok{DatName=}\NormalTok{DataSet, }\AttributeTok{VarName=}\StringTok{"CreditRating"}\NormalTok{, }
                             \AttributeTok{ClsName =} \StringTok{"Class"}\NormalTok{) }
\FunctionTok{pander}\NormalTok{(}\FunctionTok{cbind}\NormalTok{(}\AttributeTok{giniAge =}\NormalTok{ giniAge, }\AttributeTok{giniIncome =}\NormalTok{ giniIncome, }
             \AttributeTok{giniStudent =}\NormalTok{ giniStudent, }\AttributeTok{giniCreditRating =}\NormalTok{ giniCreditRating))}
\end{Highlighting}
\end{Shaded}

\begin{longtable}[]{@{}
  >{\centering\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.1389}}
  >{\centering\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.1806}}
  >{\centering\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.1944}}
  >{\centering\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.2639}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\centering
giniAge
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
giniIncome
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
giniStudent
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
giniCreditRating
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
0.3429 & 0.4405 & 0.3673 & 0.4286 \\
\end{longtable}

We can similarly calculate the Gini index for other feature variables in the data set. \textbf{When we choose a feature variable to define the root node, we choose the feature with smallest Gini index}. The Gini index is used in the classic CART algorithm and is very easy to calculate.

\hypertarget{entropy-and-information-gain}{%
\subsubsection{Entropy and Information Gain}\label{entropy-and-information-gain}}

\textbf{Entropy} is another impurity measure that is defined by

\[
E = \sum_{i = 1}^m (-p_i \log_2p_i).
\]
Where \(p_i\) is the same as that defined in Gini index. We can find the entropy at the root node and each child node based on the above tree based on the toy data. A low Entropy indicates that the data labels are quite uniform.

\begin{itemize}
\item
  \textbf{root (parent) node entropy (before splitting)}: \(E(\mbox{D}) = -(5/14)\log_2(5/14) - (9/14)\log_2(9/14) = 0.940286\)
\item
  \textbf{child node: youth}: \(E(\mbox{D}) = -(2/5)\log_2(2/5) - (3/5)\log_2(3/5) = 0.9709506\)
\item
  \textbf{child node: middle\_aged}: perfectly pure node had entropy 0.
\item
  \textbf{child node: senior}: \(E(\mbox{D}) = -(3/5)\log_3(2/5) - (2/5)\log_2(3/5) = 0.9709506\)
\item
  \textbf{Weighted average of entropy at child nodes}: \(E(\mbox{child}) = \frac{5}{14}\times 0.9709506 + \frac{4}{14}\times 0 +\frac{5}{14}\times 0.9709506 = \frac{5}{14}\times 0.9709506 \approx 0.3467681\)
\end{itemize}

\hfill\break

\begin{itemize}
\tightlist
\item
  \textbf{Information Gain}: \(\mbox{InfoGain} = E(\mbox{Parent Node}) - E(\mbox{Child Nodes}) = 0.940286 - 0.3467681 = 0.5935179.\)
\end{itemize}

\hfill\break

\textbf{Information gain} measures whether a further split is worthwhile.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{infoGain.calc }\OtherTok{=} \ControlFlowTok{function}\NormalTok{(DatName, VarName, ClsName)\{}
\NormalTok{   freqTB0 }\OtherTok{=} \FunctionTok{table}\NormalTok{(DatName[,VarName], DatName[,ClsName])}
\NormalTok{   freqTB }\OtherTok{=} \FunctionTok{data.frame}\NormalTok{(}\AttributeTok{NO =}\NormalTok{ freqTB0[, }\DecValTok{1}\NormalTok{], }\AttributeTok{YES =}\NormalTok{ freqTB0[, }\DecValTok{2}\NormalTok{])}
\NormalTok{   freqTB}\SpecialCharTok{$}\NormalTok{Tot }\OtherTok{=}\NormalTok{ freqTB}\SpecialCharTok{$}\NormalTok{NO }\SpecialCharTok{+}\NormalTok{ freqTB}\SpecialCharTok{$}\NormalTok{YES}
\NormalTok{   freqTB}\SpecialCharTok{$}\NormalTok{P1 }\OtherTok{=}\NormalTok{ freqTB}\SpecialCharTok{$}\NormalTok{NO}\SpecialCharTok{/}\NormalTok{freqTB}\SpecialCharTok{$}\NormalTok{Tot}
\NormalTok{   freqTB}\SpecialCharTok{$}\NormalTok{P2 }\OtherTok{=}\NormalTok{ freqTB}\SpecialCharTok{$}\NormalTok{YES}\SpecialCharTok{/}\NormalTok{freqTB}\SpecialCharTok{$}\NormalTok{Tot}
   \DocumentationTok{\#\#\#}
\NormalTok{   freqTB}\SpecialCharTok{$}\NormalTok{ROWPER }\OtherTok{=}\NormalTok{ (freqTB}\SpecialCharTok{$}\NormalTok{NO }\SpecialCharTok{+}\NormalTok{ freqTB}\SpecialCharTok{$}\NormalTok{YES)}\SpecialCharTok{/}\FunctionTok{sum}\NormalTok{(freqTB}\SpecialCharTok{$}\NormalTok{Tot)}
   \DocumentationTok{\#\#\# Delete zero cell prob to calculate the entropy}
\NormalTok{   pNO }\OtherTok{=} \FunctionTok{sum}\NormalTok{(freqTB}\SpecialCharTok{$}\NormalTok{NO)}\SpecialCharTok{/}\FunctionTok{sum}\NormalTok{(freqTB}\SpecialCharTok{$}\NormalTok{Tot)}
\NormalTok{   pYES }\OtherTok{=} \FunctionTok{sum}\NormalTok{(freqTB}\SpecialCharTok{$}\NormalTok{YES)}\SpecialCharTok{/}\FunctionTok{sum}\NormalTok{(freqTB}\SpecialCharTok{$}\NormalTok{Tot)}
\NormalTok{   propYES }\OtherTok{=}\NormalTok{ freqTB}\SpecialCharTok{$}\NormalTok{YES}\SpecialCharTok{/}\FunctionTok{sum}\NormalTok{(freqTB}\SpecialCharTok{$}\NormalTok{Tot)}
\NormalTok{   ParentEnt }\OtherTok{=} \SpecialCharTok{{-}}\NormalTok{pNO}\SpecialCharTok{*}\FunctionTok{log2}\NormalTok{(pNO) }\SpecialCharTok{{-}}\NormalTok{pYES }\SpecialCharTok{*}\FunctionTok{log2}\NormalTok{(pYES) }
   \DocumentationTok{\#\#\# entropy of child nodes}
\NormalTok{   P1 }\OtherTok{=}\NormalTok{ freqTB}\SpecialCharTok{$}\NormalTok{P1}
\NormalTok{   P2 }\OtherTok{=}\NormalTok{ freqTB}\SpecialCharTok{$}\NormalTok{P2}
\NormalTok{   logP1 }\OtherTok{=} \FunctionTok{log2}\NormalTok{(freqTB}\SpecialCharTok{$}\NormalTok{P1)}
\NormalTok{   logP2 }\OtherTok{=} \FunctionTok{log2}\NormalTok{(freqTB}\SpecialCharTok{$}\NormalTok{P2)}
\NormalTok{   logP1[}\FunctionTok{which}\NormalTok{(}\SpecialCharTok{!}\FunctionTok{is.finite}\NormalTok{(logP1))] }\OtherTok{=} \DecValTok{0}
\NormalTok{   logP2[}\FunctionTok{which}\NormalTok{(}\SpecialCharTok{!}\FunctionTok{is.finite}\NormalTok{(logP2))] }\OtherTok{=} \DecValTok{0}
\NormalTok{   ChildEnt }\OtherTok{=}\NormalTok{ (}\SpecialCharTok{{-}}\NormalTok{P1}\SpecialCharTok{*}\NormalTok{logP1 }\SpecialCharTok{{-}}\NormalTok{ P2}\SpecialCharTok{*}\NormalTok{logP2)}
   \DocumentationTok{\#\#\#}
\NormalTok{   infoGain }\OtherTok{=}\NormalTok{ParentEnt }\SpecialCharTok{{-}} \FunctionTok{sum}\NormalTok{(ChildEnt}\SpecialCharTok{*}\NormalTok{propYES)}
   \CommentTok{\#info.Gain = sum(freqTB$infoGain)}
   \FunctionTok{list}\NormalTok{(}\AttributeTok{ParentEnt =}\NormalTok{ ParentEnt, }\AttributeTok{ChildEnt =}\NormalTok{ ChildEnt, }\AttributeTok{propYES =}\NormalTok{ propYES, }
        \AttributeTok{infoGain =}\NormalTok{ infoGain)  }
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{infoGain.calc}\NormalTok{(}\AttributeTok{DatName=}\NormalTok{DataSet, }\AttributeTok{VarName=}\StringTok{"Age"}\NormalTok{, }\AttributeTok{ClsName =} \StringTok{"Class"}\NormalTok{) }
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## $ParentEnt
## [1] 0.940286
## 
## $ChildEnt
## [1] 0.0000000 0.9709506 0.9709506
## 
## $propYES
## [1] 0.2857143 0.2142857 0.1428571
## 
## $infoGain
## [1] 0.5935179
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{entAge }\OtherTok{=} \FunctionTok{infoGain.calc}\NormalTok{(}\AttributeTok{DatName=}\NormalTok{DataSet, }\AttributeTok{VarName=}\StringTok{"Age"}\NormalTok{, }
                       \AttributeTok{ClsName =} \StringTok{"Class"}\NormalTok{)}\SpecialCharTok{$}\NormalTok{infoGain }
\NormalTok{entIncome }\OtherTok{=} \FunctionTok{infoGain.calc}\NormalTok{(}\AttributeTok{DatName=}\NormalTok{DataSet, }\AttributeTok{VarName=}\StringTok{"Income"}\NormalTok{, }
                          \AttributeTok{ClsName =} \StringTok{"Class"}\NormalTok{)}\SpecialCharTok{$}\NormalTok{infoGain }
\NormalTok{entStudent }\OtherTok{=} \FunctionTok{infoGain.calc}\NormalTok{(}\AttributeTok{DatName=}\NormalTok{DataSet, }\AttributeTok{VarName=}\StringTok{"Student"}\NormalTok{, }
                           \AttributeTok{ClsName =} \StringTok{"Class"}\NormalTok{)}\SpecialCharTok{$}\NormalTok{infoGain }
\NormalTok{entCreditRating }\OtherTok{=} \FunctionTok{infoGain.calc}\NormalTok{(}\AttributeTok{DatName=}\NormalTok{DataSet, }\AttributeTok{VarName=}\StringTok{"CreditRating"}\NormalTok{, }
                                \AttributeTok{ClsName =} \StringTok{"Class"}\NormalTok{)}\SpecialCharTok{$}\NormalTok{infoGain }
\FunctionTok{kable}\NormalTok{(}\FunctionTok{cbind}\NormalTok{(}\AttributeTok{infoGainAge =}\NormalTok{ entAge, }\AttributeTok{infoGainIncome =}\NormalTok{ entIncome, }
            \AttributeTok{infoGainStudent =}\NormalTok{ entStudent, }
            \AttributeTok{infoGainCreditRating =}\NormalTok{ entCreditRating))}
\end{Highlighting}
\end{Shaded}

\begin{tabular}{r|r|r|r}
\hline
infoGainAge & infoGainIncome & infoGainStudent & infoGainCreditRating\\
\hline
0.5935179 & 0.3612133 & 0.4755916 & 0.3783096\\
\hline
\end{tabular}

\hfill\break

\hypertarget{binary-v.s.-multi-way-splits}{%
\subsection{Binary v.s. Multi-way Splits}\label{binary-v.s.-multi-way-splits}}

In principle, trees are not restricted to binary splits but can also be grown with multi-way splits - based on the Gini index or other selection criteria. However, the (locally optimal) search for multi-way splits in numeric variables would become much more burdensome. Hence, tree algorithms often rely on the greedy forward selection of binary splits where subsequent binary splits in the same variable can also represent multi-way splits.

\hfill\break

\hypertarget{boosted-trees---ensemble-algorithms}{%
\subsection{Boosted Trees - Ensemble Algorithms}\label{boosted-trees---ensemble-algorithms}}

Ensemble methods are models composed of multiple weaker models that are independently trained and whose predictions are combined in some way to make the overall prediction.

Much effort is put into what types of weak learners to combine and the ways in which to combine them. This is a very powerful class of techniques and as such is very popular.

\hypertarget{bootstrapped-aggregation-bagging}{%
\subsubsection{Bootstrapped Aggregation (Bagging)}\label{bootstrapped-aggregation-bagging}}

With the understanding of regular decision, we can

\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{img07/w07-BootstrapAggregation} 

}

\caption{Demonstration of bootstrap aggregation algorithm.}\label{fig:unnamed-chunk-178}
\end{figure}

\hypertarget{random-forest}{%
\subsubsection{Random Forest}\label{random-forest}}

Random forest (RF) algorithms make output predictions by combining outcomes from a sequence of decision trees. Each tree is constructed independently and depends on a random vector sampled from the input data, with all the trees in the forest having the same distribution. The predictions from the forests are averaged using bootstrap aggregation and random feature selection. RF models have been demonstrated to be robust predictors for both small sample sizes and high dimensional data

The following diagram illustrates how RF was constructed and how the decision is made based on the set of individual trees.

\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{img07/w07-RandomForests} 

}

\caption{Demonstration of random forest algorithm.}\label{fig:unnamed-chunk-179}
\end{figure}

\hypertarget{case-study---predicting-diabetes}{%
\section{Case Study - Predicting Diabetes}\label{case-study---predicting-diabetes}}

This is a new model that is different from logistic and neural network models. We load the analytic data set.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Pima }\OtherTok{=} \FunctionTok{read.csv}\NormalTok{(}\StringTok{"https://pengdsci.github.io/STA551/w03/AnalyticPimaDiabetes.csv"}\NormalTok{)[,}\SpecialCharTok{{-}}\DecValTok{1}\NormalTok{]}
\CommentTok{\# We use a random split approach}
\NormalTok{n }\OtherTok{=} \FunctionTok{dim}\NormalTok{(Pima)[}\DecValTok{1}\NormalTok{]  }\CommentTok{\# sample size}
\CommentTok{\# caution: using without replacement}
\NormalTok{train.id }\OtherTok{=} \FunctionTok{sample}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\NormalTok{n, }\FunctionTok{round}\NormalTok{(}\FloatTok{0.7}\SpecialCharTok{*}\NormalTok{n), }\AttributeTok{replace =} \ConstantTok{FALSE}\NormalTok{)  }
\NormalTok{train }\OtherTok{=}\NormalTok{ Pima[train.id, ]    }\CommentTok{\# training data}
\NormalTok{test }\OtherTok{=}\NormalTok{ Pima[}\SpecialCharTok{{-}}\NormalTok{train.id, ]    }\CommentTok{\# testing data}
\end{Highlighting}
\end{Shaded}

\hypertarget{rpart-library}{%
\subsection{\texorpdfstring{\texttt{rpart} Library}{rpart Library}}\label{rpart-library}}

we will \texttt{rpart()} to write a wrapper so we can pass the arguments of purity measures and penalty measures to construct different decision trees. The cross-validation method will be used to select the optimal decision tree as the candidate predictive to compare with the logistic model in the previous section.

\begin{quote}
Note that \texttt{rpart()} has a control option that allows users to set up various control parameters (so-called hyper-parameters) to allow the function to identify the optimal tree. One of those control parameters is the number of cross-validation when pruning the decision. Once \texttt{xval} is specified, \texttt{rpart()} prunes the tree automatically based on the given control parameters. This is internal k-fold cross-validation for identifying an optimal tree based on the information provided in the argument \texttt{parms} in which the purity measures and penalty matrix. More information can be found in the article \url{https://cran.r-project.org/web/packages/rpart/vignettes/longintro.pdf}
\end{quote}

\hfill\break

\textbf{rpart()} syntax

\begin{verbatim}
 tree = rpart(modelFormula,          # model formula similar to that in the logistic models
             data , 
             na.action  = na.rpart,  # By default, deleted if the outcome is missing, 
                                     # kept if features are missing
             method = "class",       # Classification form factor
             model  = FALSE,         # keep a copy of the model frame in the result? I
                  x = FALSE,         # keep a copy of the x matrix in the result.
                  y = TRUE,          # keep a copy of the dependent variable in the result. 
                                     # If missing and model is supplied this defaults to FALSE
              parms = list( # loss matrix. Penalize false positive or negative more heavily
                         loss = matrix(c(0,b,c,0), ncol = 2),  # b = FP, c = FN
                         split = purity),   # "gini" or "information"
             
             ## rpart algorithm options (These are defaults)
             control = rpart.control(
                       minsplit = 20,       # minimum number of observations required before split
                       minbucket= 10,       # minimum number of observations in any terminal node, default = minsplit/3
                       cp  = 0.01,          # complexity parameter used as the stopping rule,  0.02 -> small tree
                       maxcompete  = 4,     # number of competitor splits retained in the output
                       maxsurrogate   = 5,  # number of surrogate splits retained in the output
                       usesurrogate   = 2,  # how to use surrogates in the splitting process
                       xval   = 10,         # number of cross-validations
                       surrogatestyle = 0,  # controls the selection of the best surrogate
                       maxdepth  = 30       # maximum depth of any node of the final tree)
      )
\end{verbatim}

\hfill\break

\texttt{rpart()} has a lot of flexibility to construct decision trees as it has user controls. It is particularly useful in applications where the costs of \texttt{false\ positive} and \texttt{false\ negative} are different.

Next, we write a wrapper so we can build different decision trees conveniently.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# arguments to pass into rpart():}
\CommentTok{\# 1. data set (training /testing); }
\CommentTok{\# 2. Penalty coefficients}
\CommentTok{\# 3. Impurity measure}
\DocumentationTok{\#\# }
\NormalTok{tree.builder }\OtherTok{=} \ControlFlowTok{function}\NormalTok{(in.data, fp, fn, purity)\{}
\NormalTok{   tree }\OtherTok{=} \FunctionTok{rpart}\NormalTok{(diabetes }\SpecialCharTok{\textasciitilde{}}\NormalTok{ .,                }\CommentTok{\# including all features}
                \AttributeTok{data =}\NormalTok{ in.data, }
                \AttributeTok{na.action  =}\NormalTok{ na.rpart,       }\CommentTok{\# By default, deleted if the outcome is missing, }
                                             \CommentTok{\# kept if predictors are missing}
                \AttributeTok{method =} \StringTok{"class"}\NormalTok{,            }\CommentTok{\# Classification form factor}
                \AttributeTok{model  =} \ConstantTok{FALSE}\NormalTok{,}
                \AttributeTok{x =} \ConstantTok{FALSE}\NormalTok{,}
                \AttributeTok{y =} \ConstantTok{TRUE}\NormalTok{,}
            \AttributeTok{parms =} \FunctionTok{list}\NormalTok{( }\CommentTok{\# loss matrix. Penalize false positives or negatives more heavily}
                         \AttributeTok{loss =} \FunctionTok{matrix}\NormalTok{(}\FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, fp, fn, }\DecValTok{0}\NormalTok{), }\AttributeTok{ncol =} \DecValTok{2}\NormalTok{, }\AttributeTok{byrow =} \ConstantTok{TRUE}\NormalTok{),   }
                         \AttributeTok{split =}\NormalTok{ purity),          }\CommentTok{\# "gini" or "information"}
             \DocumentationTok{\#\# rpart algorithm options (These are defaults)}
             \AttributeTok{control =} \FunctionTok{rpart.control}\NormalTok{(}
                        \AttributeTok{minsplit =} \DecValTok{10}\NormalTok{,  }\CommentTok{\# minimum number of observations required before split}
                        \AttributeTok{minbucket=} \DecValTok{10}\NormalTok{,  }\CommentTok{\# minimum number of observations in any terminal node, default = minsplit/3}
                        \AttributeTok{cp  =} \FloatTok{0.01}\NormalTok{,  }\CommentTok{\# complexity parameter for stopping rule, 0.02 {-}\textgreater{} small tree }
                       \AttributeTok{xval =} \DecValTok{10}     \CommentTok{\# number of cross{-}validation )}
\NormalTok{                        )}
\NormalTok{             )}
\NormalTok{  \}}
\end{Highlighting}
\end{Shaded}

Using the above function, we define six different decision tree models in the following.

\begin{itemize}
\item
  Model 1: \texttt{gini.tree.11} is based on the Gini index without penalizing false positives and false negatives.
\item
  Model 2: \texttt{info.tree.11} is based on entropy without penalizing false positives and false negatives.
\item
  Model 3: \texttt{gini.tree.110} is based on the Gini index: cost of false negatives is 10 times the positives.
\item
  Model 4: \texttt{info.tree.110} is based on entropy: cost of false negatives is 10 times the positives.
\item
  Model 5: \texttt{gini.tree.101} is based on the Gini index: cost of false positive is 10 times the negatives.
\item
  Model 6: \texttt{info.tree.101} is based on entropy: cost of false positive is 10 times the negatives.
\end{itemize}

The tree diagram of the above two regular decision models is given below.

\begin{Shaded}
\begin{Highlighting}[]
\DocumentationTok{\#\# Call the tree model wrapper.}
\NormalTok{gini.tree.}\FloatTok{1.1} \OtherTok{=} \FunctionTok{tree.builder}\NormalTok{(}\AttributeTok{in.data =}\NormalTok{ train, }\AttributeTok{fp =} \DecValTok{1}\NormalTok{, }\AttributeTok{fn =} \DecValTok{1}\NormalTok{, }\AttributeTok{purity =} \StringTok{"gini"}\NormalTok{)}
\NormalTok{info.tree.}\FloatTok{1.1} \OtherTok{=} \FunctionTok{tree.builder}\NormalTok{(}\AttributeTok{in.data =}\NormalTok{ train, }\AttributeTok{fp =} \DecValTok{1}\NormalTok{, }\AttributeTok{fn =} \DecValTok{1}\NormalTok{, }\AttributeTok{purity =} \StringTok{"information"}\NormalTok{)}
\NormalTok{gini.tree.}\FloatTok{1.10} \OtherTok{=} \FunctionTok{tree.builder}\NormalTok{(}\AttributeTok{in.data =}\NormalTok{ train, }\AttributeTok{fp =} \DecValTok{1}\NormalTok{, }\AttributeTok{fn =} \DecValTok{10}\NormalTok{, }\AttributeTok{purity =} \StringTok{"gini"}\NormalTok{)}
\NormalTok{info.tree.}\FloatTok{1.10} \OtherTok{=} \FunctionTok{tree.builder}\NormalTok{(}\AttributeTok{in.data =}\NormalTok{ train, }\AttributeTok{fp =} \DecValTok{1}\NormalTok{, }\AttributeTok{fn =} \DecValTok{10}\NormalTok{, }\AttributeTok{purity =} \StringTok{"information"}\NormalTok{)}
\DocumentationTok{\#\# tree plots}
\FunctionTok{par}\NormalTok{(}\AttributeTok{mfrow=}\FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{))}
\FunctionTok{rpart.plot}\NormalTok{(gini.tree.}\FloatTok{1.1}\NormalTok{, }\AttributeTok{main =} \StringTok{"Tree with Gini index: non{-}penalization"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Warning: Cannot retrieve the data used to build the model (so cannot determine roundint and is.binary for the variables).
## To silence this warning:
##     Call rpart.plot with roundint=FALSE,
##     or rebuild the rpart model with model=TRUE.
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{rpart.plot}\NormalTok{(info.tree.}\FloatTok{1.1}\NormalTok{, }\AttributeTok{main =} \StringTok{"Tree with entropy: non{-}penalization"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Warning: Cannot retrieve the data used to build the model (so cannot determine roundint and is.binary for the variables).
## To silence this warning:
##     Call rpart.plot with roundint=FALSE,
##     or rebuild the rpart model with model=TRUE.
\end{verbatim}

\begin{figure}

{\centering \includegraphics{STA551EB_files/figure-latex/unnamed-chunk-182-1} 

}

\caption{Non-penalized decision tree models using Gini index (left) and entropy (right).}\label{fig:unnamed-chunk-182}
\end{figure}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{par}\NormalTok{(}\AttributeTok{mfrow=}\FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{))}
\FunctionTok{rpart.plot}\NormalTok{(gini.tree.}\FloatTok{1.10}\NormalTok{, }\AttributeTok{main =} \StringTok{"Tree with Gini index: penalization"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Warning: Cannot retrieve the data used to build the model (so cannot determine roundint and is.binary for the variables).
## To silence this warning:
##     Call rpart.plot with roundint=FALSE,
##     or rebuild the rpart model with model=TRUE.
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{rpart.plot}\NormalTok{(info.tree.}\FloatTok{1.10}\NormalTok{, }\AttributeTok{main =} \StringTok{"Tree with entropy: penalization"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Warning: Cannot retrieve the data used to build the model (so cannot determine roundint and is.binary for the variables).
## To silence this warning:
##     Call rpart.plot with roundint=FALSE,
##     or rebuild the rpart model with model=TRUE.
\end{verbatim}

\begin{figure}

{\centering \includegraphics{STA551EB_files/figure-latex/unnamed-chunk-183-1} 

}

\caption{penalized decision tree models using Gini index (left) and entropy (right).}\label{fig:unnamed-chunk-183}
\end{figure}

\hypertarget{roc-for-model-selection}{%
\subsection{ROC for Model Selection}\label{roc-for-model-selection}}

We built 4 different decision tree models previously. Next, we use ROC analysis to select the best among the four candidate models.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# function returning a sensitivity and specificity matrix}
\NormalTok{SenSpe }\OtherTok{=} \ControlFlowTok{function}\NormalTok{(in.data, fp, fn, purity)\{}
\NormalTok{  cutoff }\OtherTok{=} \FunctionTok{seq}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{1}\NormalTok{, }\AttributeTok{length =} \DecValTok{20}\NormalTok{)   }\CommentTok{\# 20 cut{-}offs including 0 and 1. }
\NormalTok{  model }\OtherTok{=} \FunctionTok{tree.builder}\NormalTok{(in.data, fp, fn, purity) }
  \DocumentationTok{\#\# Caution: decision tree returns both "success" and "failure" probabilities.}
  \DocumentationTok{\#\# We need only "success" probability to define sensitivity and specificity!!! }
\NormalTok{  pred }\OtherTok{=} \FunctionTok{predict}\NormalTok{(model, }\AttributeTok{newdata =}\NormalTok{ in.data, }\AttributeTok{type =} \StringTok{"prob"}\NormalTok{) }\CommentTok{\# two{-}column matrix.}
\NormalTok{  senspe.mtx }\OtherTok{=} \FunctionTok{matrix}\NormalTok{(}\DecValTok{0}\NormalTok{, }\AttributeTok{ncol =} \FunctionTok{length}\NormalTok{(cutoff), }\AttributeTok{nrow=} \DecValTok{2}\NormalTok{, }\AttributeTok{byrow =} \ConstantTok{FALSE}\NormalTok{)}
  \ControlFlowTok{for}\NormalTok{ (i }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\FunctionTok{length}\NormalTok{(cutoff))\{}
  \CommentTok{\# }\AlertTok{CAUTION}\CommentTok{: "pos" and "neg" are values of the label in this data set!}
  \CommentTok{\# The following line uses only "pos" probability: pred[, "pos"] !!!!}
\NormalTok{  pred.out }\OtherTok{=}  \FunctionTok{ifelse}\NormalTok{(pred[,}\StringTok{"pos"}\NormalTok{] }\SpecialCharTok{\textgreater{}=}\NormalTok{ cutoff[i], }\StringTok{"pos"}\NormalTok{, }\StringTok{"neg"}\NormalTok{)  }
\NormalTok{  TP }\OtherTok{=} \FunctionTok{sum}\NormalTok{(pred.out }\SpecialCharTok{==}\StringTok{"pos"} \SpecialCharTok{\&}\NormalTok{ in.data}\SpecialCharTok{$}\NormalTok{diabetes }\SpecialCharTok{==} \StringTok{"pos"}\NormalTok{)}
\NormalTok{  TN }\OtherTok{=} \FunctionTok{sum}\NormalTok{(pred.out }\SpecialCharTok{==}\StringTok{"neg"} \SpecialCharTok{\&}\NormalTok{ in.data}\SpecialCharTok{$}\NormalTok{diabetes }\SpecialCharTok{==} \StringTok{"neg"}\NormalTok{)}
\NormalTok{  FP }\OtherTok{=} \FunctionTok{sum}\NormalTok{(pred.out }\SpecialCharTok{==}\StringTok{"pos"} \SpecialCharTok{\&}\NormalTok{ in.data}\SpecialCharTok{$}\NormalTok{diabetes }\SpecialCharTok{==} \StringTok{"neg"}\NormalTok{)}
\NormalTok{  FN }\OtherTok{=} \FunctionTok{sum}\NormalTok{(pred.out }\SpecialCharTok{==}\StringTok{"neg"} \SpecialCharTok{\&}\NormalTok{ in.data}\SpecialCharTok{$}\NormalTok{diabetes }\SpecialCharTok{==} \StringTok{"pos"}\NormalTok{)}
\NormalTok{  senspe.mtx[}\DecValTok{1}\NormalTok{,i] }\OtherTok{=}\NormalTok{ TP}\SpecialCharTok{/}\NormalTok{(TP }\SpecialCharTok{+}\NormalTok{ FN)}
\NormalTok{  senspe.mtx[}\DecValTok{2}\NormalTok{,i] }\OtherTok{=}\NormalTok{ TN}\SpecialCharTok{/}\NormalTok{(TN }\SpecialCharTok{+}\NormalTok{ FP)}
\NormalTok{  \}}
  \DocumentationTok{\#\# A better approx of ROC, need library \{pROC\}}
\NormalTok{  prediction }\OtherTok{=}\NormalTok{ pred[, }\StringTok{"pos"}\NormalTok{]}
\NormalTok{  category }\OtherTok{=}\NormalTok{ in.data}\SpecialCharTok{$}\NormalTok{diabetes }\SpecialCharTok{==} \StringTok{"pos"}
\NormalTok{  ROCobj }\OtherTok{\textless{}{-}} \FunctionTok{roc}\NormalTok{(category, prediction)}
\NormalTok{  AUC }\OtherTok{=} \FunctionTok{auc}\NormalTok{(ROCobj)}
  \DocumentationTok{\#\#}
  \FunctionTok{list}\NormalTok{(}\AttributeTok{senspe.mtx=}\NormalTok{ senspe.mtx, }\AttributeTok{AUC =} \FunctionTok{round}\NormalTok{(AUC,}\DecValTok{3}\NormalTok{))}
\NormalTok{ \}}
\end{Highlighting}
\end{Shaded}

The above function has three arguments for users to choose different types of decision trees including the 4 trees discussed in the previous subsection. Next we use this function to build 6 different trees and plot their corresponding ROC curves so we can see the global performance of these tree algorithms.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{giniROC11 }\OtherTok{=} \FunctionTok{SenSpe}\NormalTok{(}\AttributeTok{in.data =}\NormalTok{ train, }\AttributeTok{fp=}\DecValTok{1}\NormalTok{, }\AttributeTok{fn=}\DecValTok{1}\NormalTok{, }\AttributeTok{purity=}\StringTok{"gini"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Setting levels: control = FALSE, case = TRUE
\end{verbatim}

\begin{verbatim}
## Setting direction: controls < cases
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{infoROC11 }\OtherTok{=} \FunctionTok{SenSpe}\NormalTok{(}\AttributeTok{in.data =}\NormalTok{ train, }\AttributeTok{fp=}\DecValTok{1}\NormalTok{, }\AttributeTok{fn=}\DecValTok{1}\NormalTok{, }\AttributeTok{purity=}\StringTok{"information"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Setting levels: control = FALSE, case = TRUE
## Setting direction: controls < cases
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{giniROC110 }\OtherTok{=} \FunctionTok{SenSpe}\NormalTok{(}\AttributeTok{in.data =}\NormalTok{ train, }\AttributeTok{fp=}\DecValTok{1}\NormalTok{, }\AttributeTok{fn=}\DecValTok{10}\NormalTok{, }\AttributeTok{purity=}\StringTok{"gini"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Setting levels: control = FALSE, case = TRUE
## Setting direction: controls < cases
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{infoROC110 }\OtherTok{=} \FunctionTok{SenSpe}\NormalTok{(}\AttributeTok{in.data =}\NormalTok{ train, }\AttributeTok{fp=}\DecValTok{1}\NormalTok{, }\AttributeTok{fn=}\DecValTok{10}\NormalTok{, }\AttributeTok{purity=}\StringTok{"information"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Setting levels: control = FALSE, case = TRUE
## Setting direction: controls < cases
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{giniROC101 }\OtherTok{=} \FunctionTok{SenSpe}\NormalTok{(}\AttributeTok{in.data =}\NormalTok{ train, }\AttributeTok{fp=}\DecValTok{10}\NormalTok{, }\AttributeTok{fn=}\DecValTok{1}\NormalTok{, }\AttributeTok{purity=}\StringTok{"gini"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Setting levels: control = FALSE, case = TRUE
## Setting direction: controls < cases
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{infoROC101 }\OtherTok{=} \FunctionTok{SenSpe}\NormalTok{(}\AttributeTok{in.data =}\NormalTok{ train, }\AttributeTok{fp=}\DecValTok{10}\NormalTok{, }\AttributeTok{fn=}\DecValTok{1}\NormalTok{, }\AttributeTok{purity=}\StringTok{"information"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Setting levels: control = FALSE, case = TRUE
## Setting direction: controls < cases
\end{verbatim}

Next, we plot the ROC curves and calculate the areas under the ROC curves for Individual decision tree models.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{par}\NormalTok{(}\AttributeTok{pty=}\StringTok{"s"}\NormalTok{)      }\CommentTok{\# set up square plot through graphic parameter}
\FunctionTok{plot}\NormalTok{(}\DecValTok{1}\SpecialCharTok{{-}}\NormalTok{giniROC11}\SpecialCharTok{$}\NormalTok{senspe.mtx[}\DecValTok{2}\NormalTok{,], giniROC11}\SpecialCharTok{$}\NormalTok{senspe.mtx[}\DecValTok{1}\NormalTok{,], }\AttributeTok{type =} \StringTok{"l"}\NormalTok{, }\AttributeTok{xlim=}\FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{1}\NormalTok{), }\AttributeTok{ylim=}\FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{1}\NormalTok{), }
     \AttributeTok{xlab=}\StringTok{"1 {-} specificity: FPR"}\NormalTok{, }\AttributeTok{ylab=}\StringTok{"Sensitivity: TPR"}\NormalTok{, }\AttributeTok{col =} \StringTok{"blue"}\NormalTok{, }\AttributeTok{lwd =} \DecValTok{2}\NormalTok{,}
     \AttributeTok{main=}\StringTok{"ROC Curves of Decision Trees"}\NormalTok{, }\AttributeTok{cex.main =} \FloatTok{0.9}\NormalTok{, }\AttributeTok{col.main =} \StringTok{"navy"}\NormalTok{)}
\FunctionTok{abline}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{1}\NormalTok{, }\AttributeTok{lty =} \DecValTok{2}\NormalTok{, }\AttributeTok{col =} \StringTok{"orchid4"}\NormalTok{, }\AttributeTok{lwd =} \DecValTok{2}\NormalTok{)}
\FunctionTok{lines}\NormalTok{(}\DecValTok{1}\SpecialCharTok{{-}}\NormalTok{infoROC11}\SpecialCharTok{$}\NormalTok{senspe.mtx[}\DecValTok{2}\NormalTok{,], infoROC11}\SpecialCharTok{$}\NormalTok{senspe.mtx[}\DecValTok{1}\NormalTok{,], }\AttributeTok{col =} \StringTok{"firebrick2"}\NormalTok{, }\AttributeTok{lwd =} \DecValTok{2}\NormalTok{, }\AttributeTok{lty=}\DecValTok{2}\NormalTok{)}
\FunctionTok{lines}\NormalTok{(}\DecValTok{1}\SpecialCharTok{{-}}\NormalTok{giniROC110}\SpecialCharTok{$}\NormalTok{senspe.mtx[}\DecValTok{2}\NormalTok{,], giniROC110}\SpecialCharTok{$}\NormalTok{senspe.mtx[}\DecValTok{1}\NormalTok{,], }\AttributeTok{col =} \StringTok{"olivedrab"}\NormalTok{, }\AttributeTok{lwd =} \DecValTok{2}\NormalTok{)}
\FunctionTok{lines}\NormalTok{(}\DecValTok{1}\SpecialCharTok{{-}}\NormalTok{infoROC110}\SpecialCharTok{$}\NormalTok{senspe.mtx[}\DecValTok{2}\NormalTok{,], infoROC110}\SpecialCharTok{$}\NormalTok{senspe.mtx[}\DecValTok{1}\NormalTok{,], }\AttributeTok{col =} \StringTok{"skyblue"}\NormalTok{, }\AttributeTok{lwd =} \DecValTok{2}\NormalTok{)}
\FunctionTok{lines}\NormalTok{(}\DecValTok{1}\SpecialCharTok{{-}}\NormalTok{giniROC101}\SpecialCharTok{$}\NormalTok{senspe.mtx[}\DecValTok{2}\NormalTok{,], giniROC101}\SpecialCharTok{$}\NormalTok{senspe.mtx[}\DecValTok{1}\NormalTok{,], }\AttributeTok{col =} \StringTok{"red"}\NormalTok{, }\AttributeTok{lwd =} \DecValTok{2}\NormalTok{, }\AttributeTok{lty =} \DecValTok{4}\NormalTok{)}
\FunctionTok{lines}\NormalTok{(}\DecValTok{1}\SpecialCharTok{{-}}\NormalTok{infoROC101}\SpecialCharTok{$}\NormalTok{senspe.mtx[}\DecValTok{2}\NormalTok{,], infoROC101}\SpecialCharTok{$}\NormalTok{senspe.mtx[}\DecValTok{1}\NormalTok{,], }\AttributeTok{col =} \StringTok{"sienna3"}\NormalTok{, }\AttributeTok{lwd =} \DecValTok{2}\NormalTok{)}
\FunctionTok{legend}\NormalTok{(}\StringTok{"bottomright"}\NormalTok{, }\FunctionTok{c}\NormalTok{(}\FunctionTok{paste}\NormalTok{(}\StringTok{"gini.1.1,  AUC ="}\NormalTok{, giniROC11}\SpecialCharTok{$}\NormalTok{AUC), }
                        \FunctionTok{paste}\NormalTok{(}\StringTok{"info.1.1,   AUC ="}\NormalTok{,infoROC11}\SpecialCharTok{$}\NormalTok{AUC), }
                        \FunctionTok{paste}\NormalTok{(}\StringTok{"gini.1.10, AUC ="}\NormalTok{,giniROC110}\SpecialCharTok{$}\NormalTok{AUC), }
                        \FunctionTok{paste}\NormalTok{(}\StringTok{"info.1.10, AUC ="}\NormalTok{,infoROC110}\SpecialCharTok{$}\NormalTok{AUC),}
                        \FunctionTok{paste}\NormalTok{(}\StringTok{"gini.10.1, AUC ="}\NormalTok{,giniROC101}\SpecialCharTok{$}\NormalTok{AUC), }
                        \FunctionTok{paste}\NormalTok{(}\StringTok{"info.10.1, AUC ="}\NormalTok{,infoROC101}\SpecialCharTok{$}\NormalTok{AUC)),}
                        \AttributeTok{col=}\FunctionTok{c}\NormalTok{(}\StringTok{"blue"}\NormalTok{,}\StringTok{"firebrick2"}\NormalTok{,}\StringTok{"olivedrab"}\NormalTok{,}\StringTok{"skyblue"}\NormalTok{,}\StringTok{"red"}\NormalTok{,}\StringTok{"sienna3"}\NormalTok{), }
                        \AttributeTok{lty=}\FunctionTok{rep}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{6}\NormalTok{), }\AttributeTok{lwd=}\FunctionTok{rep}\NormalTok{(}\DecValTok{2}\NormalTok{,}\DecValTok{6}\NormalTok{), }\AttributeTok{cex =} \FloatTok{0.5}\NormalTok{, }\AttributeTok{bty =} \StringTok{"n"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics{STA551EB_files/figure-latex/unnamed-chunk-186-1} 

}

\caption{Comparison of ROC curves}\label{fig:unnamed-chunk-186}
\end{figure}

The above ROC curves represent various decision trees and their corresponding AUC. The model with the largest AUC is considered the best decision tree among the existing ones.

\hfill\break

\hypertarget{optimal-cut-off-score-determination}{%
\subsection{Optimal Cut-off Score Determination}\label{optimal-cut-off-score-determination}}

As usual, once the final model is determined, we need to find the optimal cut-off score for reporting the predictive performance of the final model with the test data. Please keep in mind the optimal cut-off determination through cross-validation must be based on the training data set.

In practical applications, one may end up with two or more \textbf{final models} with similar AUCs. In this case, we need to report the performance of all \emph{final models} based on the test data and let clients choose one to deploy (and possibly leave the rest as challengers). For this reason, we write a function to determine the optimal cut-off for a given decision tree (based on this project) since different decision trees have their own optimal cut-off.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Optm.cutoff }\OtherTok{=} \ControlFlowTok{function}\NormalTok{(in.data, fp, fn, purity)\{}
\NormalTok{  n0 }\OtherTok{=} \FunctionTok{dim}\NormalTok{(in.data)[}\DecValTok{1}\NormalTok{]}\SpecialCharTok{/}\DecValTok{5}
\NormalTok{  cutoff }\OtherTok{=} \FunctionTok{seq}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{1}\NormalTok{, }\AttributeTok{length =} \DecValTok{20}\NormalTok{)               }\CommentTok{\# candidate cut off prob}
  \DocumentationTok{\#\# accuracy for each candidate cut{-}off}
\NormalTok{  accuracy.mtx }\OtherTok{=} \FunctionTok{matrix}\NormalTok{(}\DecValTok{0}\NormalTok{, }\AttributeTok{ncol=}\DecValTok{20}\NormalTok{, }\AttributeTok{nrow=}\DecValTok{5}\NormalTok{)    }\CommentTok{\# 20 candidate cutoffs and gini.11}
  \DocumentationTok{\#\#}
  \ControlFlowTok{for}\NormalTok{ (k }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\DecValTok{5}\NormalTok{)\{}
\NormalTok{     valid.id }\OtherTok{=}\NormalTok{ ((k}\DecValTok{{-}1}\NormalTok{)}\SpecialCharTok{*}\NormalTok{n0 }\SpecialCharTok{+} \DecValTok{1}\NormalTok{)}\SpecialCharTok{:}\NormalTok{(k}\SpecialCharTok{*}\NormalTok{n0)}
\NormalTok{     valid.dat }\OtherTok{=}\NormalTok{ in.data[valid.id,]}
\NormalTok{     train.dat }\OtherTok{=}\NormalTok{ in.data[}\SpecialCharTok{{-}}\NormalTok{valid.id,] }
     \DocumentationTok{\#\# tree model}
\NormalTok{     tree.model }\OtherTok{=} \FunctionTok{tree.builder}\NormalTok{(in.data, fp, fn, purity)}
     \DocumentationTok{\#\# prediction }
\NormalTok{     pred }\OtherTok{=} \FunctionTok{predict}\NormalTok{(tree.model, }\AttributeTok{newdata =}\NormalTok{ valid.dat, }\AttributeTok{type =} \StringTok{"prob"}\NormalTok{)[,}\DecValTok{2}\NormalTok{]}
     \DocumentationTok{\#\# for{-}loop}
     \ControlFlowTok{for}\NormalTok{ (i }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\DecValTok{20}\NormalTok{)\{}
        \DocumentationTok{\#\# predicted probabilities}
\NormalTok{        pc}\FloatTok{.1} \OtherTok{=} \FunctionTok{ifelse}\NormalTok{(pred }\SpecialCharTok{\textgreater{}}\NormalTok{ cutoff[i], }\StringTok{"pos"}\NormalTok{, }\StringTok{"neg"}\NormalTok{)}
        \DocumentationTok{\#\# accuracy}
\NormalTok{        a1 }\OtherTok{=} \FunctionTok{mean}\NormalTok{(pc}\FloatTok{.1} \SpecialCharTok{==}\NormalTok{ valid.dat}\SpecialCharTok{$}\NormalTok{diabetes)}
\NormalTok{        accuracy.mtx[k,i] }\OtherTok{=}\NormalTok{ a1}
\NormalTok{       \}}
\NormalTok{      \}}
\NormalTok{   avg.acc }\OtherTok{=} \FunctionTok{apply}\NormalTok{(accuracy.mtx, }\DecValTok{2}\NormalTok{, mean)}
   \DocumentationTok{\#\# plots}
\NormalTok{   n }\OtherTok{=} \FunctionTok{length}\NormalTok{(avg.acc)}
\NormalTok{   idx }\OtherTok{=} \FunctionTok{which}\NormalTok{(avg.acc }\SpecialCharTok{==} \FunctionTok{max}\NormalTok{(avg.acc))}
\NormalTok{   tick.label }\OtherTok{=} \FunctionTok{as.character}\NormalTok{(}\FunctionTok{round}\NormalTok{(cutoff,}\DecValTok{2}\NormalTok{))}
   \DocumentationTok{\#\#}
   \FunctionTok{plot}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\NormalTok{n, avg.acc, }\AttributeTok{xlab=}\StringTok{"cut{-}off score"}\NormalTok{, }\AttributeTok{ylab=}\StringTok{"average accuracy"}\NormalTok{, }
        \AttributeTok{ylim=}\FunctionTok{c}\NormalTok{(}\FunctionTok{min}\NormalTok{(avg.acc), }\DecValTok{1}\NormalTok{), }
        \AttributeTok{axes =} \ConstantTok{FALSE}\NormalTok{,}
        \AttributeTok{main=}\FunctionTok{paste}\NormalTok{(}\StringTok{"5{-}fold CV optimal cut{-}off }\SpecialCharTok{\textbackslash{}n}\StringTok{ "}\NormalTok{,purity,}\StringTok{"(fp, fn) = ("}\NormalTok{, fp, }\StringTok{","}\NormalTok{, fn,}\StringTok{")"}\NormalTok{ , }\AttributeTok{collapse =} \StringTok{""}\NormalTok{),}
        \AttributeTok{cex.main =} \FloatTok{0.9}\NormalTok{,}
        \AttributeTok{col.main =} \StringTok{"navy"}\NormalTok{)}
        \FunctionTok{axis}\NormalTok{(}\DecValTok{1}\NormalTok{, }\AttributeTok{at=}\DecValTok{1}\SpecialCharTok{:}\DecValTok{20}\NormalTok{, }\AttributeTok{label =}\NormalTok{ tick.label, }\AttributeTok{las =} \DecValTok{2}\NormalTok{)}
        \FunctionTok{axis}\NormalTok{(}\DecValTok{2}\NormalTok{)}
        \FunctionTok{points}\NormalTok{(idx, avg.acc[idx], }\AttributeTok{pch=}\DecValTok{19}\NormalTok{, }\AttributeTok{col =} \StringTok{"red"}\NormalTok{)}
        \FunctionTok{segments}\NormalTok{(idx , }\FunctionTok{min}\NormalTok{(avg.acc), idx , avg.acc[idx ], }\AttributeTok{col =} \StringTok{"red"}\NormalTok{)}
       \FunctionTok{text}\NormalTok{(idx, avg.acc[idx]}\SpecialCharTok{+}\FloatTok{0.03}\NormalTok{, }\FunctionTok{as.character}\NormalTok{(}\FunctionTok{round}\NormalTok{(avg.acc[idx],}\DecValTok{4}\NormalTok{)), }\AttributeTok{col =} \StringTok{"red"}\NormalTok{, }\AttributeTok{cex =} \FloatTok{0.8}\NormalTok{) }
\NormalTok{   \}}
\end{Highlighting}
\end{Shaded}

For demonstration, we use the above function to calculate the optimal cut-off of 6 decision trees constructed earlier in the following.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{par}\NormalTok{(}\AttributeTok{mfrow=}\FunctionTok{c}\NormalTok{(}\DecValTok{3}\NormalTok{,}\DecValTok{2}\NormalTok{))}
\FunctionTok{Optm.cutoff}\NormalTok{(}\AttributeTok{in.data =}\NormalTok{ train, }\AttributeTok{fp=}\DecValTok{1}\NormalTok{, }\AttributeTok{fn=}\DecValTok{1}\NormalTok{, }\AttributeTok{purity=}\StringTok{"gini"}\NormalTok{)}
\FunctionTok{Optm.cutoff}\NormalTok{(}\AttributeTok{in.data =}\NormalTok{ train, }\AttributeTok{fp=}\DecValTok{1}\NormalTok{, }\AttributeTok{fn=}\DecValTok{1}\NormalTok{, }\AttributeTok{purity=}\StringTok{"information"}\NormalTok{)}
\FunctionTok{Optm.cutoff}\NormalTok{(}\AttributeTok{in.data =}\NormalTok{ train, }\AttributeTok{fp=}\DecValTok{1}\NormalTok{, }\AttributeTok{fn=}\DecValTok{10}\NormalTok{, }\AttributeTok{purity=}\StringTok{"gini"}\NormalTok{)}
\FunctionTok{Optm.cutoff}\NormalTok{(}\AttributeTok{in.data =}\NormalTok{ train, }\AttributeTok{fp=}\DecValTok{1}\NormalTok{, }\AttributeTok{fn=}\DecValTok{10}\NormalTok{, }\AttributeTok{purity=}\StringTok{"information"}\NormalTok{)}
\FunctionTok{Optm.cutoff}\NormalTok{(}\AttributeTok{in.data =}\NormalTok{ train, }\AttributeTok{fp=}\DecValTok{10}\NormalTok{, }\AttributeTok{fn=}\DecValTok{1}\NormalTok{, }\AttributeTok{purity=}\StringTok{"gini"}\NormalTok{)}
\FunctionTok{Optm.cutoff}\NormalTok{(}\AttributeTok{in.data =}\NormalTok{ train, }\AttributeTok{fp=}\DecValTok{10}\NormalTok{, }\AttributeTok{fn=}\DecValTok{1}\NormalTok{, }\AttributeTok{purity=}\StringTok{"information"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics{STA551EB_files/figure-latex/unnamed-chunk-188-1} 

}

\caption{Plot of optimal cut-off determination}\label{fig:unnamed-chunk-188}
\end{figure}

As anticipated, different trees have their own optimal cut-off. Please keep in mind that the cut-off is random (based on the randomly split training data), there may have different cut-offs in different runs. It is dependent on the tree size, sometimes, we may end up with multiple optimal cut-offs. Technically speaking, we choose any one of them for implementation. A better recommendation is to choose the average of these multiple cut-offs and the final cut-off to be used on the testing data set.

\hfill\break

\hypertarget{an-overview-of-unsupervised-ml-algorithms}{%
\chapter{An Overview of Unsupervised ML Algorithms}\label{an-overview-of-unsupervised-ml-algorithms}}

This note overviews the basic unsupervised machine learning algorithms (also known as knowledge discovery) in which models are not supervised using a training data set. Instead, models themselves find the \textbf{hidden patterns} and insights from the given data.

The goal of unsupervised learning is to find the underlying structure of the data set and group that data according to similarities. Common algorithms used in unsupervised learning include clustering, anomaly detection, neural networks, and approaches for learning latent variable models.

The following types of unsupervised machine learning algorithms are commonly used in practice.

\begin{itemize}
\tightlist
\item
  K-means Clustering
\item
  Hierarchical Clustering
\item
  Anomaly Detection
\item
  Principal Component Analysis
\end{itemize}

There are many methods to calculate this distance information; the choice of distance measures is a critical step in clustering. It defines how the similarity of two data points (x, y) is calculated and it will influence the shape of the clusters. The choice of distance measures is a critical step in clustering. It defines how the similarity of two data points (x, y) is calculated and it will influence the shape of the clusters.

\url{https://github.com/pengdsci/STA551/blob/main/w08/img/w08-kMeans-gif.gif}

Here are a few sites you check for these ``distances''.

\begin{itemize}
\tightlist
\item
  \url{https://elki-project.github.io/algorithms/distances}
\item
  \url{https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6853338}
\item
  \url{https://cran.r-project.org/web/packages/SimilarityMeasures/SimilarityMeasures.pdf}
\end{itemize}

In the next few sections, we will describe each of these algorithms.

\hypertarget{k-means-clustering}{%
\section{K-means Clustering}\label{k-means-clustering}}

K-means algorithm is an iterative algorithm that partitions the data set into K \emph{pre-defined}, distinct, and non-overlapping subgroups (clusters) where each data point belongs to only one group. Data points within each subgroup are \textbf{similar} while data points across the subgroups are ``different** according to a selected dissimilarity measure used in the algorithm.

In other words, k-means clustering consists of defining clusters so that the total intra-cluster variation (known as a total within-cluster variation) is minimized. There are several k-means algorithms available. The standard algorithm is to define the total within-cluster variation as the sum of squared (SS) distances (Euclidean distances) between data points and the corresponding centroid. To be more specific, let \(x_i\) be the data point in cluster k, denoted by \(C_k\) and \(\mu_k\) is the center of cluster \(C_k\) (i.e.~the mean value of the points when Euclidean distance is used). The within-cluster SS is defined by

\[
W(C_k) = \sum_{x_i\in C_k} (x_i - \mu_i)^2
\]

\begin{quote}
Each observation (\(x_i\)) is assigned to a given cluster such that the sum of squares (SS) distance of the observation to their assigned cluster centers \(\mu_k\) is a minimum.
\end{quote}

We define the total within-cluster variation as follows

\[
\mbox{TW} = \sum_{k=1}^k W(C_l) = \sum_{i=1}^k\sum_{x_k\in C_k}(x_i - \mu_k)^2
\]

\begin{quote}
The total within-cluster sum of square measures the compactness (i.e.~goodness) of the clustering and we want it to be as small as possible.
\end{quote}

Two fundamental questions to answer are: (1) how many initial clusters should be selected; (2) how to choose the initial ``centers''.

\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{img08/w08-k-Means} 

}

\caption{Illustration of cluster analysis.}\label{fig:unnamed-chunk-191}
\end{figure}

K-means clustering algorithm works in the following three steps.

\hypertarget{select-a-pre-determined-number-of-classes}{%
\subsection{Select A Pre-determined Number of Classes}\label{select-a-pre-determined-number-of-classes}}

Several algorithms can be used to find the optimal number of clusters. Elbow and Silhouette algorithms are commonly used and are implemented in R.

\textbf{Elbow Method}

The Elbow method gives us an idea of what a good k number of clusters would be based on the sum of squared distance (SSE) between data points and their assigned clusters' centroids. We pick k at the spot where SSE starts to flatten out and form an elbow. We'll use the geyser dataset and evaluate SSE for different values of k and see where the curve might form an elbow and flatten out.

Elbow is one of the most famous methods for selecting the right value of k. We also perform the hyper-parameter tuning to choose the best value of k. The Elbow method is an empirical method to find out the best value of k.

\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{img08/w08-elbow} 

}

\caption{Figure 3. (a) A visual curve with an explicit elbow point. (b) A visual curve being fairly smooth with an ambiguous elbow point.}\label{fig:unnamed-chunk-192}
\end{figure}

\textbf{Silhouette Method}

Silhouette Method uses a similarity measure (Silhouette coefficient) that is defined in the following

\[
S_i = \frac{b_i-a_i}{\max{ \{a_i, b_i\}}}
\]

where \(S_i\) is the silhouette coefficient of the data point \(i\); \(a_i\) is the average distance between \(i\) and all the other data points in the cluster to which \(i\) belongs, and \(b_i\) is the average distance from \(i\) to all clusters to which \(i\) does not belong.

We can plot the Silhouette coefficient against the pre-determined clusters \(k\). The plot of the silhouette is between \(-1\) to \(1\).

A high average silhouette width indicates good clustering. The average silhouette method computes the average silhouette of observations for different values of k. The optimal number of clusters k is the one that maximizes the average silhouette over a range of possible values for k.

Observe the plot and choose the k value that is closer to 1 as the optimal number of clusters.

\hypertarget{initialize-centroids.}{%
\subsection{Initialize Centroids.}\label{initialize-centroids.}}

Initialize centroids by first shuffling the data set and then randomly selecting K data points for the centroids without replacement.

\hypertarget{update-centroids}{%
\subsection{Update Centroids}\label{update-centroids}}

Updating centroids is an iterative process:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Compute the sum of the squared distance between data points and all (initial) centroids. Assign each data point to the closest cluster (centroid).
\item
  Compute the (updating) centroids for the clusters by taking the average of all data points that belong to each cluster.
\end{enumerate}

Keep iterating until there is no change to the centroids. i.e., the assignment of data points to clusters isn't changing.

The following figure illustrates how to find the updated centroids immediately after the initial centroids.

\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{img08/w08-K-means-iteration} 

}

\caption{Updating centroids in the process of finding the final centroids}\label{fig:unnamed-chunk-193}
\end{figure}

\hypertarget{some-remarks-on-k-means}{%
\subsection{Some Remarks on K-means}\label{some-remarks-on-k-means}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  K-means clustering assumes numerical features since the Euclidean distance is used to define similarity measures.
\item
  K-Means clustering performs well only for a convex set of clusters and not for non-convex sets.
\item
  Recent development allows categorical feature variables with non-Euclidean distance.
\item
  The k-means algorithm does not guarantee finding the optimal solution. k-means is a fairly simple sequence of tasks and its clustering quality depends a lot on two factors: the number of k clusters and initial centroids.
\end{enumerate}

\hypertarget{case-study}{%
\subsection{Case Study}\label{case-study}}

For the illustrative purpose, we only use two numerical variables in a simple data set that is publically available in Github \url{https://raw.githubusercontent.com/satishgunjal/datasets/master/Mall_Customers.csv}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{df }\OtherTok{=} \FunctionTok{read.csv}\NormalTok{(}\StringTok{"https://raw.githubusercontent.com/satishgunjal/datasets/master/Mall\_Customers.csv"}\NormalTok{)}
\DocumentationTok{\#\# rename the two variables and then subset the data}
\FunctionTok{names}\NormalTok{(df)[}\FunctionTok{names}\NormalTok{(df)}\SpecialCharTok{==}\StringTok{"Annual.Income..k.."}\NormalTok{] }\OtherTok{=} \StringTok{"AnnualIncome"}
\FunctionTok{names}\NormalTok{(df)[}\FunctionTok{names}\NormalTok{(df)}\SpecialCharTok{==}\StringTok{"Spending.Score..1.100."}\NormalTok{] }\OtherTok{=} \StringTok{"SpendingScore"}
\NormalTok{clust.data }\OtherTok{=}\NormalTok{ df[, }\FunctionTok{c}\NormalTok{(}\StringTok{"AnnualIncome"}\NormalTok{, }\StringTok{"SpendingScore"}\NormalTok{)]}
\NormalTok{scaled.data }\OtherTok{=} \FunctionTok{as.data.frame}\NormalTok{(}\FunctionTok{scale}\NormalTok{(clust.data)[,}\DecValTok{1}\SpecialCharTok{:}\DecValTok{2}\NormalTok{])}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{distance }\OtherTok{=} \FunctionTok{get\_dist}\NormalTok{(scaled.data)}
\FunctionTok{fviz\_dist}\NormalTok{(distance, }\AttributeTok{gradient =} \FunctionTok{list}\NormalTok{(}\AttributeTok{low =} \StringTok{"yellow"}\NormalTok{, }\AttributeTok{mid =} \StringTok{"orange"}\NormalTok{, }\AttributeTok{high =} \StringTok{"darkred"}\NormalTok{), }\AttributeTok{show\_labels =} \ConstantTok{FALSE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics{STA551EB_files/figure-latex/unnamed-chunk-195-1} 

}

\caption{Heatmap representation of potential clusters}\label{fig:unnamed-chunk-195}
\end{figure}

The above heatmap indicates that different clusters exist in this data (based on the two numerical variables).

\begin{itemize}
\tightlist
\item
  The syntax of \textbf{kmeans()} is given in the following code chunk.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{k2 }\OtherTok{\textless{}{-}} \FunctionTok{kmeans}\NormalTok{(}\AttributeTok{x =}\NormalTok{ scaled.data, }
             \AttributeTok{centers =} \DecValTok{2}\NormalTok{, }
             \AttributeTok{iter.max =} \DecValTok{10}\NormalTok{,}
             \AttributeTok{nstart =} \DecValTok{25}\NormalTok{,}
             \AttributeTok{algorithm =} \StringTok{"Hartigan{-}Wong"}\NormalTok{,}
             \AttributeTok{trace =} \ConstantTok{FALSE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{itemize}
\tightlist
\item
  Determination of optimal class.
\end{itemize}

We use the elbow method to find the optimal number of clusters.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{wss }\OtherTok{=} \ConstantTok{NULL}
\NormalTok{K }\OtherTok{=} \DecValTok{15}
\ControlFlowTok{for}\NormalTok{ (i }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\NormalTok{K)\{}
\NormalTok{  wss[i] }\OtherTok{=} \FunctionTok{kmeans}\NormalTok{(scaled.data, i, }\DecValTok{1}\NormalTok{ )}\SpecialCharTok{$}\NormalTok{tot.withinss}
\NormalTok{ \}}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Warning: 1

## Warning: 1

## Warning: 1

## Warning: 1

## Warning: 1

## Warning: 1

## Warning: 1

## Warning: 1

## Warning: 1

## Warning: 1

## Warning: 1

## Warning: 1

## Warning: 1
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\DocumentationTok{\#\# elbow plot}
\FunctionTok{plot}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\NormalTok{K, wss, }\AttributeTok{type =}\StringTok{"b"}\NormalTok{,}
          \AttributeTok{col=} \StringTok{"blue"}\NormalTok{,}
          \AttributeTok{xlab=}\StringTok{"Number of Clusters"}\NormalTok{,}
          \AttributeTok{ylab =} \StringTok{"WSS"}\NormalTok{,}
          \AttributeTok{main =} \StringTok{"Elbow Plot for Selecting Optimal Number of Clusters"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics{STA551EB_files/figure-latex/unnamed-chunk-197-1} 

}

\caption{Elbow plot for optimal number of clusters.}\label{fig:unnamed-chunk-197}
\end{figure}

From the above elbow plot, it seems that the optimal number of clusters is 5. So select k - 5 hereafter.

\begin{itemize}
\tightlist
\item
  Cluster the data with 5 centroids
\end{itemize}

We will cluster the data into 5 groups and then add the cluster ID to the data. Since only two continuous feature variables were used to cluster the data. After we added the cluster ID to the data, we use color coding to make a scatter plot and view the clusters.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{k5 }\OtherTok{\textless{}{-}} \FunctionTok{kmeans}\NormalTok{(}\AttributeTok{x =}\NormalTok{ scaled.data, }
             \AttributeTok{centers =} \DecValTok{5}\NormalTok{, }
             \AttributeTok{iter.max =} \DecValTok{10}\NormalTok{,}
             \AttributeTok{nstart =} \DecValTok{25}\NormalTok{,}
             \AttributeTok{algorithm =} \StringTok{"Hartigan{-}Wong"}\NormalTok{,}
             \AttributeTok{trace =} \ConstantTok{FALSE}\NormalTok{)}
\NormalTok{scaled.data}\SpecialCharTok{$}\NormalTok{group }\OtherTok{=}\NormalTok{ k5}\SpecialCharTok{$}\NormalTok{cluster}
\DocumentationTok{\#\#\# Plot the clusters}
\CommentTok{\# Scatter plot}
\FunctionTok{plot}\NormalTok{(scaled.data}\SpecialCharTok{$}\NormalTok{AnnualIncome, scaled.data}\SpecialCharTok{$}\NormalTok{SpendingScore,}
     \AttributeTok{pch =} \DecValTok{19}\NormalTok{,}
     \AttributeTok{col =} \FunctionTok{factor}\NormalTok{(scaled.data}\SpecialCharTok{$}\NormalTok{group),}
     \AttributeTok{xlab =}\StringTok{"Spending Score"}\NormalTok{,}
     \AttributeTok{ylab =} \StringTok{"Annual Income"}\NormalTok{,}
     \AttributeTok{main =} \StringTok{"Clustering Performance Visual Check"}\NormalTok{)}

\CommentTok{\# Legend}
\FunctionTok{legend}\NormalTok{(}\StringTok{"topright"}\NormalTok{,}
       \AttributeTok{legend =} \FunctionTok{levels}\NormalTok{(}\FunctionTok{factor}\NormalTok{(scaled.data}\SpecialCharTok{$}\NormalTok{group)),}
       \AttributeTok{pch =} \DecValTok{19}\NormalTok{,}
       \AttributeTok{col =} \FunctionTok{factor}\NormalTok{(}\FunctionTok{levels}\NormalTok{(}\FunctionTok{factor}\NormalTok{(scaled.data}\SpecialCharTok{$}\NormalTok{group))))}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics{STA551EB_files/figure-latex/unnamed-chunk-198-1} 

}

\caption{Final cluster results: visual inspection}\label{fig:unnamed-chunk-198}
\end{figure}

\hypertarget{hierarchical-clustering}{%
\section{Hierarchical Clustering}\label{hierarchical-clustering}}

provided a solid introduction to one of the most popular clustering methods. Hierarchical clustering is an alternative approach to k-means clustering for identifying groups in the dataset. It does not require us to pre-specify the number of clusters to be generated as is required by the k-means approach. Furthermore, hierarchical clustering has an added advantage over K-means clustering in that it results in an attractive tree-based representation of the observations, called a dendrogram.

\hypertarget{types-of-hierarchical-clustering}{%
\subsection{Types of Hierarchical Clustering}\label{types-of-hierarchical-clustering}}

There are two types of hierarchical clustering: agglomerative and divisive.

\begin{itemize}
\item
  \textbf{Agglomerative}*: An agglomerative approach begins with each observation in a distinct (singleton) cluster, and successively merges clusters until a stopping criterion is satisfied.
\item
  \textbf{Divisive}: A divisive method begins with all patterns in a single cluster and performs splitting until a stopping criterion is met.
\end{itemize}

\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{img08/w08-types-pf-hirarchical-clustering} 

}

\caption{Illustration of types of hierarchical clustering.}\label{fig:unnamed-chunk-199}
\end{figure}

As an example, we look at how agglomerative clustering works using five data points in the following figure.

\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{img08/w08-Agllomerative-Clustering} 

}

\caption{Illustration of steps of agglomerative hierarchical clustering.}\label{fig:unnamed-chunk-200}
\end{figure}

\hfill\break

\hypertarget{case-study-i-clustering-with-two-features}{%
\subsection{Case Study I -- Clustering with Two Features}\label{case-study-i-clustering-with-two-features}}

We still use the same data set that we used in the previous case study of K-means clustering but will include \textbf{age} variable in the data frame for following hierarchical clustering.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{df }\OtherTok{=} \FunctionTok{read.csv}\NormalTok{(}\StringTok{"https://raw.githubusercontent.com/satishgunjal/datasets/master/Mall\_Customers.csv"}\NormalTok{)}
\DocumentationTok{\#\# Rename the two variables and then subset the data}
\FunctionTok{names}\NormalTok{(df)[}\FunctionTok{names}\NormalTok{(df)}\SpecialCharTok{==}\StringTok{"Annual.Income..k.."}\NormalTok{] }\OtherTok{=} \StringTok{"AnnualIncome"}
\FunctionTok{names}\NormalTok{(df)[}\FunctionTok{names}\NormalTok{(df)}\SpecialCharTok{==}\StringTok{"Spending.Score..1.100."}\NormalTok{] }\OtherTok{=} \StringTok{"SpendingScore"}
\NormalTok{hierarch.data }\OtherTok{=}\NormalTok{ df[, }\FunctionTok{c}\NormalTok{(}\StringTok{"Age"}\NormalTok{, }\StringTok{"AnnualIncome"}\NormalTok{, }\StringTok{"SpendingScore"}\NormalTok{)]}
\end{Highlighting}
\end{Shaded}

\hfill\break

\hypertarget{pre-processing-operations-for-clustering}{%
\subsubsection{Pre-processing Operations for Clustering}\label{pre-processing-operations-for-clustering}}

There are a couple of things you should take care of before starting.

\textbf{Scaling} is imperative that we normalize the scale of feature values in order to start with the clustering process. This is because each observation's feature values are represented as coordinates in n-dimensional space (n is the number of features) and then the distances between these coordinates are calculated. If these coordinates are not normalized, then it may lead to false results. R has functions \textbf{scale()} and \textbf{normalize()}.

\textbf{Missing Value imputation} is also important to deal with missing/null/inf values in your data set beforehand. There are many ways to deal with such values, one is to either remove them or impute them with mean, median, mode, or use some advanced regression techniques. R has many packages and functions to deal with missing value imputations like \textbf{impute()}.

\hypertarget{hierarchical-clustering-with-r}{%
\subsubsection{Hierarchical Clustering with R}\label{hierarchical-clustering-with-r}}

There are different functions available in R for computing hierarchical clustering. The commonly used functions are:

\begin{itemize}
\item
  \textbf{hclust()} {[}in stats package{]} and \textbf{agnes()} {[}in cluster package{]} for agglomerative hierarchical clustering (HC).
\item
  \textbf{diana()} {[}in cluster package{]} for divisive HC.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{scales.hierarch }\OtherTok{=} \FunctionTok{as.data.frame}\NormalTok{(hierarch.data)}
\NormalTok{distance }\OtherTok{\textless{}{-}} \FunctionTok{dist}\NormalTok{(scales.hierarch, }\AttributeTok{method =} \StringTok{"euclidean"}\NormalTok{)}
\CommentTok{\# Hierarchical clustering using Complete Linkage}
\NormalTok{hc1 }\OtherTok{\textless{}{-}} \FunctionTok{hclust}\NormalTok{(distance, }\AttributeTok{method =} \StringTok{"complete"}\NormalTok{ )}
\CommentTok{\# Plot the obtained dendrogram}
\FunctionTok{plot}\NormalTok{(hc1, }\AttributeTok{cex =} \FloatTok{0.6}\NormalTok{, }\AttributeTok{labels =} \ConstantTok{FALSE}\NormalTok{, }\AttributeTok{hang =} \SpecialCharTok{{-}}\DecValTok{1}\NormalTok{, }\AttributeTok{xlab =} \StringTok{""}\NormalTok{, }
     \AttributeTok{main =} \StringTok{"Dendrogram: hierarchical clustering"}\NormalTok{)}
\FunctionTok{rect.hclust}\NormalTok{(hc1, }\AttributeTok{k =} \DecValTok{5}\NormalTok{, }\AttributeTok{border =} \DecValTok{2}\SpecialCharTok{:}\DecValTok{9}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics{STA551EB_files/figure-latex/unnamed-chunk-202-1} 

}

\caption{Dendrogram: hierarchical clustering}\label{fig:unnamed-chunk-202}
\end{figure}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Figured this out by coloring the labels white to the background}
\NormalTok{avg\_dend\_obj }\OtherTok{\textless{}{-}} \FunctionTok{as.dendrogram}\NormalTok{(hc1)}
\FunctionTok{labels\_colors}\NormalTok{(avg\_dend\_obj) }\OtherTok{\textless{}{-}} \StringTok{"white"}
\FunctionTok{plot}\NormalTok{(avg\_dend\_obj, }\AttributeTok{cex =} \FloatTok{0.6}\NormalTok{, }
     \AttributeTok{labels =} \ConstantTok{FALSE}\NormalTok{, }
     \AttributeTok{hang =} \SpecialCharTok{{-}}\DecValTok{1}\NormalTok{, }
     \AttributeTok{xlab =} \StringTok{""}\NormalTok{, }
     \AttributeTok{ylab=} \StringTok{"Height"}\NormalTok{,}
     \AttributeTok{main =} \StringTok{"Dendrogram: hierarchical clustering: No X{-}Labels"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Warning in plot.window(...): "labels"
\end{verbatim}

\begin{verbatim}
## Warning in plot.window(...): "hang"
\end{verbatim}

\begin{verbatim}
## Warning in plot.xy(xy, type, ...): "labels"
\end{verbatim}

\begin{verbatim}
## Warning in plot.xy(xy, type, ...): "hang"
\end{verbatim}

\begin{verbatim}
## Warning in axis(side = side, at = at, labels = labels, ...): "hang"

## Warning in axis(side = side, at = at, labels = labels, ...): "hang"
\end{verbatim}

\begin{verbatim}
## Warning in title(...): "labels"
\end{verbatim}

\begin{verbatim}
## Warning in title(...): "hang"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{rect.hclust}\NormalTok{(hc1, }\AttributeTok{k =} \DecValTok{5}\NormalTok{, }\AttributeTok{border =} \DecValTok{2}\SpecialCharTok{:}\DecValTok{9}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics{STA551EB_files/figure-latex/unnamed-chunk-203-1} 

}

\caption{Dendrogram: hierarchical clustering: No X-Labels}\label{fig:unnamed-chunk-203}
\end{figure}

\hypertarget{determination-optimal-number-of-clusters}{%
\subsubsection{Determination Optimal Number of Clusters}\label{determination-optimal-number-of-clusters}}

The determination of the optimal number of clusters is an important and challenging problem. In hierarchical clustering, different similarity measures impact the number of optimal clusters. We will not discuss this topic in detail. To know more about this topic, you are referred to the following article with examples in R \url{https://www.jstatsoft.org/article/view/2194/798}.

We can use the same elbow and silhouette methods to plot

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{fviz\_nbclust}\NormalTok{(scales.hierarch, }\AttributeTok{FUN =}\NormalTok{ hcut, }\AttributeTok{method =} \StringTok{"wss"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics{STA551EB_files/figure-latex/unnamed-chunk-204-1} 

}

\caption{Elbow plot: Optimal number of clusters}\label{fig:unnamed-chunk-204}
\end{figure}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{fviz\_nbclust}\NormalTok{(scales.hierarch, }\AttributeTok{FUN =}\NormalTok{ hcut, }\AttributeTok{method =} \StringTok{"silhouette"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics{STA551EB_files/figure-latex/unnamed-chunk-205-1} 

}

\caption{Silhouette plot: Optimal number of clusters}\label{fig:unnamed-chunk-205}
\end{figure}

\hypertarget{extracting-cluster-id}{%
\subsubsection{Extracting Cluster ID}\label{extracting-cluster-id}}

The above-elbow plot indicates that choosing 4 clusters is appropriate. Next, we perform a 4-cluster analysis and extract the cluster ID to add them to the data frame.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{hc4 }\OtherTok{\textless{}{-}} \FunctionTok{hclust}\NormalTok{(distance, }\AttributeTok{method =} \StringTok{"complete"}\NormalTok{ )}
\NormalTok{group }\OtherTok{=} \FunctionTok{cutree}\NormalTok{(hc4, }\AttributeTok{k =} \DecValTok{5}\NormalTok{)}
\NormalTok{scales.hierarch}\SpecialCharTok{$}\NormalTok{group }\OtherTok{=}\NormalTok{ group}
\DocumentationTok{\#\# }
\FunctionTok{plot}\NormalTok{(scales.hierarch}\SpecialCharTok{$}\NormalTok{AnnualIncome, scales.hierarch}\SpecialCharTok{$}\NormalTok{SpendingScore,}
     \AttributeTok{pch =} \DecValTok{19}\NormalTok{,}
     \AttributeTok{col =} \FunctionTok{factor}\NormalTok{(scales.hierarch}\SpecialCharTok{$}\NormalTok{group),}
     \AttributeTok{xlab =}\StringTok{"Spending Score"}\NormalTok{,}
     \AttributeTok{ylab =} \StringTok{"Annual Income"}\NormalTok{,}
     \AttributeTok{main =} \StringTok{"Hierarchical Clustering Performance Visual Check"}\NormalTok{)}

\DocumentationTok{\#\# Legend}
\FunctionTok{legend}\NormalTok{(}\StringTok{"topright"}\NormalTok{,}
       \AttributeTok{legend =} \FunctionTok{levels}\NormalTok{(}\FunctionTok{factor}\NormalTok{(scales.hierarch}\SpecialCharTok{$}\NormalTok{group)),}
       \AttributeTok{pch =} \DecValTok{19}\NormalTok{,}
       \AttributeTok{col =} \FunctionTok{factor}\NormalTok{(}\FunctionTok{levels}\NormalTok{(}\FunctionTok{factor}\NormalTok{(scales.hierarch}\SpecialCharTok{$}\NormalTok{group))))}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics{STA551EB_files/figure-latex/unnamed-chunk-206-1} 

}

\caption{Visual check the resulting clusters obtained from agglomerative hierarchical clustering.}\label{fig:unnamed-chunk-206}
\end{figure}

\hfill\break

\hypertarget{case-study-ii-multi-feature-clustering}{%
\section{Case Study II: Multi-feature Clustering}\label{case-study-ii-multi-feature-clustering}}

The Iris Dataset contains four features (length and width of sepals and petals) of 50 samples of three species of Iris (Iris setosa, Iris virginica, and Iris versicolor). These measures were used to create a linear discriminant model to classify the species. The dataset is often used in data mining, classification, and clustering examples and to test algorithms.

\begin{figure}

{\centering \includegraphics[width=0.6\linewidth]{img08/w08-iris} 

}

\caption{Iris data set: variables illustration - pedal and sepal}\label{fig:unnamed-chunk-207}
\end{figure}

This 100-year-old data set has been included in the R base package. The first few records of the data set are displayed in the following table.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{pander}\NormalTok{(}\FunctionTok{head}\NormalTok{(iris))}
\end{Highlighting}
\end{Shaded}

\begin{longtable}[]{@{}
  >{\centering\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.1944}}
  >{\centering\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.1806}}
  >{\centering\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.1944}}
  >{\centering\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.1806}}
  >{\centering\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.2361}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\centering
SepalLength
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
SepalWidth
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
PetalLength
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
PetalWidth
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
Classification
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
5.1 & 3.5 & 1.4 & 0.2 & Iris-setosa \\
4.9 & 3 & 1.4 & 0.2 & Iris-setosa \\
4.7 & 3.2 & 1.3 & 0.2 & Iris-setosa \\
4.6 & 3.1 & 1.5 & 0.2 & Iris-setosa \\
5 & 3.6 & 1.4 & 0.2 & Iris-setosa \\
5.4 & 3.9 & 1.7 & 0.4 & Iris-setosa \\
\end{longtable}

We use \textbf{k-means} method to perform cluster analysis on the \textbf{iris data} with the four numerical feature variables.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{myClusteredIris }\OtherTok{=}\NormalTok{ iris}
\CommentTok{\# We start with 3 clusters since we know there are 3 species in the data.}
\CommentTok{\# In practice, we need relatively try different number of clusters and then}
\CommentTok{\# use the elbow plot to determine the best number of clusters.}
\NormalTok{km.iris }\OtherTok{\textless{}{-}} \FunctionTok{kmeans}\NormalTok{( }\AttributeTok{x =}\NormalTok{ myClusteredIris[, }\SpecialCharTok{{-}}\DecValTok{5}\NormalTok{] , }\AttributeTok{centers =} \DecValTok{3}\NormalTok{)  }
\NormalTok{clust.ID }\OtherTok{\textless{}{-}}\NormalTok{ km.iris}\SpecialCharTok{$}\NormalTok{cluster        }\CommentTok{\# extracting cluster IDs}

\FunctionTok{table}\NormalTok{(clust.ID)                    }\CommentTok{\# frequency of clusters}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## clust.ID
##  1  2  3 
## 50 62 38
\end{verbatim}

Since this clustering task involves 4 numerical feature variables, we cannot create a 2D plot to show the clustering performance with all four original feature variables. However, we can so-called PCA (to be discussed in the next section) to create two new feature variables and then plot the new features to show the performance of the clustering algorithm.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{clusplot}\NormalTok{(iris[, }\SpecialCharTok{{-}}\DecValTok{5}\NormalTok{],}
\NormalTok{ clust.ID,}
 \AttributeTok{lines =} \DecValTok{0}\NormalTok{,}
 \AttributeTok{shade =} \ConstantTok{TRUE}\NormalTok{,}
 \AttributeTok{color =} \ConstantTok{TRUE}\NormalTok{,}
 \AttributeTok{labels =} \DecValTok{1}\NormalTok{,}
 \AttributeTok{plotchar =} \ConstantTok{FALSE}\NormalTok{,}
 \AttributeTok{span =} \ConstantTok{TRUE}\NormalTok{,}
 \AttributeTok{main =} \FunctionTok{paste}\NormalTok{(}\StringTok{"Clusters of Iris Flowers"}\NormalTok{)}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{STA551EB_files/figure-latex/unnamed-chunk-210-1.pdf}

\hypertarget{memory-usage-of-clustering}{%
\subsection{Memory Usage of Clustering}\label{memory-usage-of-clustering}}

One of the potential issues in clustering analysis is the use of memory. If the data size is too large,

\hfill\break

\hypertarget{dimensionality-reduction-algorithms}{%
\section{Dimensionality Reduction Algorithms}\label{dimensionality-reduction-algorithms}}

Like clustering methods, dimension reduction seeks and explores the inherent structure in the data, but in this case in an unsupervised manner or to summarize or describe data using less information.

This can be useful to visualize high-dimensional data or to simplify data which can then be used in a supervised learning method. Many of these methods can be adapted for use in classification and regression. The following are the frequently used algorithms.

\begin{itemize}
\item
  Principal Component Analysis (PCA)
\item
  Linear Discriminant Analysis (LDA)
\item
  Quadratic Discriminant Analysis (QDA)
\item
\begin{verbatim}
       Independent Component Analysis (ICA)
\end{verbatim}
\end{itemize}

In this section, we introduce the most commonly used PCA.

\hypertarget{the-logic-of-pca}{%
\subsection{The Logic of PCA}\label{the-logic-of-pca}}

We use a two-variable animation as an example to illustrate the idea of principal component analysis (PCA).

\url{https://github.com/pengdsci/STA551/blob/main/w08/img/w08-PCA-Animation-01.gif}

The above animated graph shows that two or more numerical feature variables are highly correlated, the PCA can be used to aggregate the information in the correlated feature variables by transforming them to a set of uncorrelated \textbf{new feature variables} such that the majority of the total information is captured by the first few new feature variables.

\begin{figure}

{\centering \includegraphics[width=0.6\linewidth]{img08/w08-2VarPCAExample} 

}

\caption{Graphical interpretation of PCA with two correlated variables}\label{fig:unnamed-chunk-212}
\end{figure}

\hypertarget{case-study---iris-data}{%
\subsection{Case Study - Iris Data}\label{case-study---iris-data}}

We have used the well-known Iris Data set in clustering algorithms. The data set has 4 correlated numerical variables(sepal width and length, petal width and length) and a categorical variable. The four variables measure the size of the flowers. We use PCA to see whether reducing the number of feature variables for related modeling.

\hypertarget{fitting-pca-model-to-iris-data}{%
\subsubsection{Fitting PCA model to Iris data}\label{fitting-pca-model-to-iris-data}}

We want to PCA method to reduce the dimensions from 4 (numerical variables) to a smaller number. The R function \textbf{prcomp()} to the factor loadings associated with the four numerical variables.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{log.iris }\OtherTok{=} \FunctionTok{log}\NormalTok{(iris[,}\SpecialCharTok{{-}}\DecValTok{5}\NormalTok{])   }\CommentTok{\# drop the categorical variable in the original }
                            \CommentTok{\# data set and transform all numerical to the}
                            \CommentTok{\# log{-}scale}
\NormalTok{ir.pca }\OtherTok{\textless{}{-}} \FunctionTok{prcomp}\NormalTok{(log.iris, }\AttributeTok{center =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{scale =} \ConstantTok{TRUE}\NormalTok{)}
\CommentTok{\# summary(ir.pca)[6]   \# use the command to explore the possible information}
                       \CommentTok{\# available in the output of the summary.}
\end{Highlighting}
\end{Shaded}

In the above R function, three arguments are explained in the following

\begin{verbatim}
log.iris = log of the four variables
cater = TRUE, this means the variables are centered, i.e.,  you move the origin of the original coordinate system to the center of the data cloud.
scale = TRUE, divide the difference between the value of each variable and its mean by the standard deviation of the corresponding variable. 
\end{verbatim}

Next, we find the factor loading of the above fitted PCA. We can write an explicit system of linear transformation by using the loadings.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{kable}\NormalTok{(}\FunctionTok{round}\NormalTok{(ir.pca}\SpecialCharTok{$}\NormalTok{rotation, }\DecValTok{2}\NormalTok{), }\AttributeTok{caption=}\StringTok{"Factor loadings of the PCA"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{table}

\caption{\label{tab:unnamed-chunk-214}Factor loadings of the PCA}
\centering
\begin{tabular}[t]{l|r|r|r|r}
\hline
  & PC1 & PC2 & PC3 & PC4\\
\hline
SepalLength & 0.51 & -0.45 & 0.71 & 0.21\\
\hline
SepalWidth & -0.30 & -0.89 & -0.32 & -0.10\\
\hline
PetalLength & 0.58 & -0.03 & -0.20 & -0.79\\
\hline
PetalWidth & 0.57 & -0.04 & -0.59 & 0.57\\
\hline
\end{tabular}
\end{table}

The explicit expression of the predictive system of PC is given by

\[
 \begin{aligned}
PC_1 & = 0.50 Sepal.Length -0.30 Sepal.Width + 0.58 Petal.Length + 0.57 Petal.Width  \\
PC_2 & = -0.45 Sepal.Length - 0.89 Sepal.Width -0.03 Petal.Length - 0.04 Petal.Width \\
PC_3 & = 0.71 Sepal.Length -0.33 Sepal.Width - 0.22  Petal.Length - 0.58 Petal.Width \\
PC_4 & = 0.19 Sepal.Length -0.09 Sepal.Width -0.79 Petal.Length + 0.58 Petal.Width   \\
\end{aligned}
\]

The magnitude of factor loadings indicates the amount of information that original variables contribute to the corresponding principal components. For example, the absolute value of loadings associated with petal width and length and sepal length in \(PC_1\) is greater than or equal to 0.5. We can simply call \(PC_1\) the \textbf{size} of iris flowers. Similarly, sepal length and width are major contributors to \(PC_2\), we can name \(PC_2\) as \textbf{sepal size}.

\hypertarget{optimal-number-of-pcs-to-be-retained}{%
\subsubsection{Optimal number of PCs to be retained}\label{optimal-number-of-pcs-to-be-retained}}

The object of PCA is to reduce the dimension without losing a significant amount of information. In PCA, we look at how much total variation is captured by each principal component. Most of the libraries that are capable of performing PCA automatically rank the PCA based on the variation captured by each principal component.

The following summary table gives the importance of the principal components.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{kable}\NormalTok{(}\FunctionTok{summary}\NormalTok{(ir.pca)}\SpecialCharTok{$}\NormalTok{importance, }\AttributeTok{caption=}\StringTok{"The importance of each principal component"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{table}

\caption{\label{tab:unnamed-chunk-215}The importance of each principal component}
\centering
\begin{tabular}[t]{l|r|r|r|r}
\hline
  & PC1 & PC2 & PC3 & PC4\\
\hline
Standard deviation & 1.708715 & 0.9563817 & 0.3700768 & 0.1693209\\
\hline
Proportion of Variance & 0.729930 & 0.2286700 & 0.0342400 & 0.0071700\\
\hline
Cumulative Proportion & 0.729930 & 0.9585900 & 0.9928300 & 1.0000000\\
\hline
\end{tabular}
\end{table}

From the above table, we can see that the first PC explains about \(73.33\%\) of the variation. But we first two principal components explain about \(96\%\) of the total variation. In the data analysis, you only need to use the first two PCs that lose about \(4\%\) of the information.

We can also make a scree plot as a visual tool to show the number of principal components to retain for future analysis.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{screeplot}\NormalTok{(ir.pca, }
          \AttributeTok{type =} \StringTok{"lines"}\NormalTok{,}
          \AttributeTok{main =} \StringTok{"Scree Plot of PCA Iris Flower Sizes"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics{STA551EB_files/figure-latex/unnamed-chunk-216-1} 

}

\caption{Scree plot of PCA on Iris Data}\label{fig:unnamed-chunk-216}
\end{figure}

Note that the vertical axis in the above scree plot uses the variances of PCs. The standard deviation was used in the above summary table.

\hypertarget{extracting-pc-scores}{%
\subsubsection{Extracting PC Scores}\label{extracting-pc-scores}}

The predictive principle scores are values of the newly transformed variables. We can choose the first few principal components to use as response variables to do relevant modeling.

The command \texttt{ir.pcs\$x} extracts the PC scores from the PCA procedure. These scores are the values of the new transformed variables. They can be used as response or predictor variables in statistical models. The following table shows the

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{kable}\NormalTok{(ir.pca}\SpecialCharTok{$}\NormalTok{x[}\DecValTok{1}\SpecialCharTok{:}\DecValTok{15}\NormalTok{,], }\AttributeTok{caption =} \StringTok{"The first 15 PC scores transformed from the original variable. In the analysis, you want to either the first PC or the first two PCs."}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{table}

\caption{\label{tab:unnamed-chunk-217}The first 15 PC scores transformed from the original variable. In the analysis, you want to either the first PC or the first two PCs.}
\centering
\begin{tabular}[t]{r|r|r|r}
\hline
PC1 & PC2 & PC3 & PC4\\
\hline
-2.399323 & -0.4230132 & 0.1808988 & 0.0148983\\
\hline
-2.222208 & 0.6710008 & 0.3295067 & 0.0675933\\
\hline
-2.578374 & 0.4014949 & -0.0005677 & 0.0597146\\
\hline
-2.449092 & 0.6619725 & -0.0853303 & -0.1406184\\
\hline
-2.528843 & -0.5370943 & 0.0176988 & -0.0342582\\
\hline
-1.832157 & -1.3192660 & -0.2595561 & 0.1570885\\
\hline
-2.476608 & 0.0698647 & -0.5143622 & 0.1176323\\
\hline
-2.342356 & -0.1817091 & 0.1237008 & -0.0855882\\
\hline
-2.537388 & 1.2245319 & -0.1337017 & -0.0648975\\
\hline
-2.619625 & 0.4888347 & 0.6474674 & -0.4456031\\
\hline
-2.242377 & -0.9560140 & 0.3182291 & -0.0342892\\
\hline
-2.425334 & -0.0555452 & -0.1031337 & -0.2316215\\
\hline
-2.693003 & 0.7631906 & 0.6418480 & -0.3595538\\
\hline
-3.323662 & 1.1230276 & 0.1721670 & -0.1966056\\
\hline
-2.367240 & -1.6608246 & 0.5762004 & 0.3128922\\
\hline
\end{tabular}
\end{table}

As the final step, we rename the two PCs and then add the two new variables to the original data set for future analysis. Since \(PC_1\) captures variation of both sepal and pedal, we rename \(PC_1\) as \textbf{iris.size}. Similarly, we rename \(PC_2\) as \textbf{sepal.size}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{my.final.iris.data }\OtherTok{=}\NormalTok{ iris}
\NormalTok{my.final.iris.data}\SpecialCharTok{$}\NormalTok{iris.size }\OtherTok{=}\NormalTok{ ir.pca}\SpecialCharTok{$}\NormalTok{x[, }\DecValTok{1}\NormalTok{]}
\NormalTok{my.final.iris.data}\SpecialCharTok{$}\NormalTok{sepal.size }\OtherTok{=}\NormalTok{ ir.pca}\SpecialCharTok{$}\NormalTok{x[, }\DecValTok{2}\NormalTok{]}
\DocumentationTok{\#\# write the final data set to a local folder}
\CommentTok{\#write.csv(my.final.iris.data, file = "C:\textbackslash{}\textbackslash{}Users\textbackslash{}\textbackslash{}75CPENG\textbackslash{}\textbackslash{}OneDrive {-} West Chester University of PA\textbackslash{}\textbackslash{}Desktop\textbackslash{}\textbackslash{}cpeng\textbackslash{}\textbackslash{}WCU{-}Teaching\textbackslash{}\textbackslash{}2023Summer\textbackslash{}\textbackslash{}STA551\textbackslash{}\textbackslash{}w08\textbackslash{}\textbackslash{}Final{-}Iris{-}Data.csv")}
\end{Highlighting}
\end{Shaded}

The following scree shot shows the final data file was saved in a local folder and the two renamed principal components were added to the final data set.

\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{img08/w08-write2local-folder} 

}

\caption{Screenshot of the final iris data set with new variables defined based on the principal components}\label{fig:unnamed-chunk-219}
\end{figure}

\hfill\break

\hypertarget{bootstrap-algorithms-and-applicatications}{%
\chapter{Bootstrap Algorithms and Applicatications}\label{bootstrap-algorithms-and-applicatications}}

The bootstrap method is a data-based simulation method for statistical inference. The method assumes that

\begin{itemize}
\item
  The sample is a random sample representing the population;
\item
  The sample size is large enough such that the empirical distribution can be close to the true distribution.
\end{itemize}

\hypertarget{basic-idea-of-bootstrap-method.-1}{%
\section{Basic Idea of Bootstrap Method.}\label{basic-idea-of-bootstrap-method.-1}}

The objective is to estimate a population parameter such as mean, variance, correlation coefficient, regression coefficients, etc. from a random sample without assuming any probability distribution of the underlying distribution of the population.

For convenience, we assume that the population of interest has a cumulative distribution function \(F(x: \theta)\), where \(\theta\) is a vector of the population. For example, You can think about the following distributions

\begin{itemize}
\item
  \textbf{Normal distribution}: \(N(\mu, \sigma^2)\), the distribution function is given by

  \[f(x:\theta) = \frac{1}{\sqrt{2\pi}\sigma}\exp\left(-\frac{(x-\mu)^2}{2\sigma^2}\right)\]
  where \(\theta = (\mu, \sigma)\). Since the normal distribution is so fundamental in statistics, we use the special notation for the cumulative distribution \(\phi_{\mu, \sigma^2}(x)\) or simply \(\phi(x)\). The corresponding probability function
\item
  \textbf{Binomial distribution}: \(Binom(n, p)\), the probability distribution is given by
\end{itemize}

\[ P(x) = \frac{n!}{x!(n-x)!}p^x(1-p)^{n-x}, x = 0, 1, 2, \cdots, n-1, n.\]
where \(\theta = p\). \emph{Caution}: \(n\) is NOT a parameter!

We have already learned how to make inferences about population means and variances under various assumptions in elementary statistics. In this note, we introduce a \textbf{new approach} to making inferences only based on a given random sample taken from the underlying population.

As an example, we focus on the population mean. For other parameters, we can follow the same idea to make bootstrap inferences.

\hypertarget{sampling-true-population---monte-carlo-sampling}{%
\subsection{Sampling True Population - Monte Carlo Sampling}\label{sampling-true-population---monte-carlo-sampling}}

We have introduced various study designs and sampling plans to obtain random samples from a given population with the distribution function \(F(x:\theta)\). Let \(\mu\) be the population mean.

\begin{itemize}
\tightlist
\item
  \textbf{Random Sample}. Let
\end{itemize}

\[\{x_1, x_2, \cdots, x_n\} \to F(x:\theta)\]
be a random sample from population \(F(x:\theta)\).

\begin{itemize}
\tightlist
\item
  \textbf{Sample Mean}. The point estimate is given by
\end{itemize}

\[\hat{\mu} = \frac{\sum_{i=1}^n x_i}{n}\]

\begin{itemize}
\item
  \textbf{Sampling Distribution of \(\hat{\mu}\)}. In order to construct the confidence interval of \(\mu\) or make hypothesis testing about \(\mu\), we need to know the sampling distribution of \(\hat{\mu}\). From elementary statistics, we have the following results.

  \begin{itemize}
  \item
    \(\hat{\mu}\) is normally distributed if (1). \(n\) is large; or (2). the population is normal and population variance is known.
  \item
    the standardized \(\hat{\mu}\) follows a t-distribution if the population is normal and population variance is unknown.
  \item
    \(\hat{\mu}\) is \textbf{unknown} of the population is not normal and the sample size is not large enough.
  \end{itemize}
\item
  In the last case of the previous bullet point, we don't have the theory to derive the sampling distribution based on a \textbf{single} sample. However, if the sampling is not too expensive and time-consuming, we take following the sample study design and sampling plan to repeatedly take a large number, 1000, samples of the same size from the population. We calculate the mean of each of the 1000 samples and obtain 1000 sample means \(\{\hat{\mu}_1, \hat{\mu}_2, \cdots, \hat{\mu}_{1000}\}\). Then the empirical distribution of \(\hat{\mu}\).
\end{itemize}

The following figure depicts the process of how to sample the true population and approximate the sampling distribution of the point estimator of the population parameter.

\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{img09/w09-ApproxSamplingDist} 

}

\caption{Steps for estimating the sampling distribution of a point estimator of the population parameter}\label{fig:unnamed-chunk-221}
\end{figure}

\hfill\break

\hypertarget{sampling-random-sample---bootstrap-sampling}{%
\subsection{Sampling Random Sample - Bootstrap Sampling}\label{sampling-random-sample---bootstrap-sampling}}

Sampling the true population can be very expensive in practice! That means the method of Monte Carlo sampling is infeasible from a practical perspective. The question is whether there are ways to estimate the sampling distribution of the sample means from \textbf{a single given random sample}? The answer is YES under the assumption the sample yields a valid estimation of the original population distribution.

\begin{itemize}
\item
  \textbf{Bootstrap Sampling} With the assumption that the sample yields a good approximation of the population distribution, we can take bootstrap samples from the \textbf{actual} sample. Let
  \[\{x_1, x_2, \cdots, x_n\} \to F(x:\theta)\] be the actual random sample taken from the population. A \textbf{bootstrap sample} is obtained by taking a sample \textbf{with replacement} from the original data set (not the population!) with the same size as the original sample. Because \textbf{with replacement} was used, some values in the bootstrap sample appear once, some twice, and so on, and some do not appear at all.
\item
  \textbf{Notation of Bootstrap Sample}. We use \(\{x_1^{(i*)}, x_2^{(i*)}, \cdots, x_n^{(i*)}\}\) to denote the \(i^{th}\) bootstrap sample. Then the corresponding mean is called bootstrap sample mean and denoted by \(\hat{\mu}_i^*\), for \(i = 1, 2, ..., n\).
\item
  \textbf{Bootstrap sampling distribution} of the sample mean can be estimated by taking a large number, say B, of bootstrap samples. The resulting B bootstrap sample means are used to estimate the sampling distribution. Note that, in practice, B is bigger than 1000.
\end{itemize}

The above Bootstrap sampling process is illustrated in the following figure.

\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{img09/w09-BootSamplingDist} 

}

\caption{Steps for the Bootstrap sampling distribution of a point estimator of the population parameter}\label{fig:unnamed-chunk-222}
\end{figure}

\hfill\break

\hypertarget{case-study-confidence-interval-of-correlation-coefficient}{%
\subsection{Case Study: Confidence Interval of Correlation Coefficient}\label{case-study-confidence-interval-of-correlation-coefficient}}

There is no exact formula for the correlation coefficient of the two variables. Different approximate confidence intervals are available. One of them is based on Pearson transformation. The explicit form of the interval has the following form.

\[
\left( \frac{e^{2L}-1}{e^{2L}+1},  \frac{e^{2U}-1}{e^{2U}+1}\right)
\]

where
\[
L = z_r - \frac{Z_{0.975}}{\sqrt{n-3}}, \ \ \ U = z_r + \frac{Z_{0.975}}{\sqrt{n-3}}
\]

and
\[
z_r = \frac{\ln(1+r)-\ln(1-r)}{2} \ \ \ \text{and} \ \ \ z_{0.975} = \text{97.5 th percentile of the standard normal distribution.}
\]

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{y }\OtherTok{=}\NormalTok{ Carseats}\SpecialCharTok{$}\NormalTok{Sales}
\NormalTok{x }\OtherTok{=}\NormalTok{ Carseats}\SpecialCharTok{$}\NormalTok{Price}
\FunctionTok{plot}\NormalTok{(Sales }\SpecialCharTok{\textasciitilde{}}\NormalTok{ Price, }\AttributeTok{data =}\NormalTok{ Carseats, }\AttributeTok{pch=}\DecValTok{19}\NormalTok{, }\AttributeTok{cex=}\FloatTok{0.8}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics{STA551EB_files/figure-latex/unnamed-chunk-223-1} 

}

\caption{The scatter plot between sales and price in car seats data set}\label{fig:unnamed-chunk-223}
\end{figure}

\begin{Shaded}
\begin{Highlighting}[]
\DocumentationTok{\#\#  The following is the translation of the given formula}
\NormalTok{n }\OtherTok{=} \FunctionTok{length}\NormalTok{(y)}
\NormalTok{r }\OtherTok{=} \FunctionTok{cor}\NormalTok{(x,y)    }\CommentTok{\# sample Pearson correlation coefficient}
\NormalTok{zr }\OtherTok{=}\NormalTok{ (}\FunctionTok{log}\NormalTok{(}\DecValTok{1}\SpecialCharTok{+}\NormalTok{r)}\SpecialCharTok{{-}}\FunctionTok{log}\NormalTok{(}\DecValTok{1}\SpecialCharTok{{-}}\NormalTok{r))}\SpecialCharTok{/}\DecValTok{2}
\NormalTok{z}\FloatTok{.975} \OtherTok{=} \FunctionTok{qnorm}\NormalTok{(}\FloatTok{0.975}\NormalTok{)}
\NormalTok{L }\OtherTok{=}\NormalTok{ zr }\SpecialCharTok{{-}}\NormalTok{ z}\FloatTok{.975}\SpecialCharTok{/}\FunctionTok{sqrt}\NormalTok{(n}\DecValTok{{-}3}\NormalTok{)}
\NormalTok{U }\OtherTok{=}\NormalTok{ zr }\SpecialCharTok{+}\NormalTok{ z}\FloatTok{.975}\SpecialCharTok{/}\FunctionTok{sqrt}\NormalTok{(n}\DecValTok{{-}3}\NormalTok{)}
\NormalTok{LCI }\OtherTok{=}\NormalTok{ (}\FunctionTok{exp}\NormalTok{(}\DecValTok{2}\SpecialCharTok{*}\NormalTok{L)}\SpecialCharTok{{-}}\DecValTok{1}\NormalTok{)}\SpecialCharTok{/}\NormalTok{(}\FunctionTok{exp}\NormalTok{(}\DecValTok{2}\SpecialCharTok{*}\NormalTok{L)}\SpecialCharTok{+}\DecValTok{1}\NormalTok{)}
\NormalTok{UCI }\OtherTok{=}\NormalTok{ (}\FunctionTok{exp}\NormalTok{(}\DecValTok{2}\SpecialCharTok{*}\NormalTok{U)}\SpecialCharTok{{-}}\DecValTok{1}\NormalTok{)}\SpecialCharTok{/}\NormalTok{(}\FunctionTok{exp}\NormalTok{(}\DecValTok{2}\SpecialCharTok{*}\NormalTok{U)}\SpecialCharTok{+}\DecValTok{1}\NormalTok{)}
\NormalTok{CI }\OtherTok{=} \FunctionTok{cbind}\NormalTok{(}\AttributeTok{LCI =}\NormalTok{ LCI, }\AttributeTok{UCI =}\NormalTok{ UCI)}
\DocumentationTok{\#\#\# Bootstrap CI}
\NormalTok{B }\OtherTok{=} \DecValTok{1000}              \CommentTok{\# Take 1000 bootstrap sample}
\NormalTok{bt.r }\OtherTok{=} \FunctionTok{c}\NormalTok{()            }\CommentTok{\# empty vector to store bootstrap coefficient}
\ControlFlowTok{for}\NormalTok{ ( i }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\NormalTok{B)\{}
\NormalTok{  bt.id }\OtherTok{=} \FunctionTok{sample}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\NormalTok{n, n, }\AttributeTok{replace =} \ConstantTok{TRUE}\NormalTok{)   }\CommentTok{\# bootstrapping observation IDs}
\NormalTok{  bt.x }\OtherTok{=}\NormalTok{ x[bt.id]     }\CommentTok{\# bootstrap x , must use the above bt.id}
\NormalTok{  bt.y }\OtherTok{=}\NormalTok{ y[bt.id]     }\CommentTok{\# bootstrap y , must use the above bt.id}
\NormalTok{  bt.r[i] }\OtherTok{=} \FunctionTok{cor}\NormalTok{(bt.x, bt.y)}
\NormalTok{\}}
\DocumentationTok{\#\# 2.5\% and 97.5\% of the 1000 bootstrap correlation coefficients are the}
\DocumentationTok{\#\# lower and upper limits of the 95\% bootstrap confidence interval}
\NormalTok{bt.CI }\OtherTok{=} \FunctionTok{quantile}\NormalTok{(bt.r, }\FunctionTok{c}\NormalTok{(}\FloatTok{0.025}\NormalTok{, }\FloatTok{0.975}\NormalTok{))}
\FunctionTok{list}\NormalTok{(}\AttributeTok{CI =}\NormalTok{ CI, }\AttributeTok{bt.CI =} \FunctionTok{as.vector}\NormalTok{(bt.CI))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## $CI
##             LCI       UCI
## [1,] -0.5203026 -0.362724
## 
## $bt.CI
## [1] -0.5214968 -0.3649663
\end{verbatim}

\hfill\break

\hypertarget{bootstrap-confidence-interval-of-auc-for-logistic-model}{%
\section{Bootstrap Confidence Interval of AUC for Logistic Model}\label{bootstrap-confidence-interval-of-auc-for-logistic-model}}

In this section, we demonstrate how to find the confidence interval of the area of the ROC curve. This confidence interval is of practical importance.

\hypertarget{bootstrap-sampling-for-logistic-modeling}{%
\section{Bootstrap Sampling for Logistic Modeling}\label{bootstrap-sampling-for-logistic-modeling}}

In logistic regression models, the residuals are not defined as those in the linear regression (i.e., \(e_i = \text{observed }y - \text{fitted }y\)). Bootstrapping residuals does not work for logistic regression. We can use the following two steps to take bootstrap samples:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Take a bootstrap sample from the observation IDs with the same size (and with replacement): \(\{1, 2, 3, \cdots, n \}\). Note that some of the IDs will be sampled multiple times.
\item
  Using the bootstrap IDs to identify the corresponding records in the data set and defined a new data set called \textbf{Bootstrap sample}. Some of the records appear multiple times. These duplicate records with be kept in the sample. In other words, the distinct records in a bootstrap sample are less than the sample size!
\end{enumerate}

Following the above steps, we can take many bootstrap samples. Recall that we can build a logistic regression for a given data set and construct an ROC curve and calculate the area under the curve. This means, if we draw \(B = 1000\) bootstrap samples, we can then build 1000 logistic regression models (also called \textbf{bootstrap logistic regression models}), hence, will have 1000 \textbf{bootstrap AUCs} (one for each bootstrap logistic model).

This further means that the histogram of these \textbf{bootstrap AUCs} can be considered as the sampling distribution of the actual sample \textbf{AUC}. The \textbf{95\% Bootstrap confidence interval} of \textbf{AUC} is defined as the 2.5\% and 97.5\% quantiles of the \textbf{bootstrap AUCs}.

\hfill\break

\hypertarget{case-study---confidence-interval-of-auc}{%
\subsection{Case Study - Confidence Interval of AUC}\label{case-study---confidence-interval-of-auc}}

We present two examples showing how to calculate the bootstrap confidence interval of the AUC using a logistic regression model. One can apply the same steps for neural net and decision tree algorithms.

\hypertarget{predicting-graduate-admission}{%
\subsubsection{Predicting Graduate Admission}\label{predicting-graduate-admission}}

We use the following admission data set to illustrate the steps.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{admitted }\OtherTok{=} \FunctionTok{read.csv}\NormalTok{(}\StringTok{"https://raw.githubusercontent.com/pengdsci/STA551/main/w09/w09{-}admitted.csv"}\NormalTok{)}
\NormalTok{notadmitted }\OtherTok{=} \FunctionTok{read.csv}\NormalTok{(}\StringTok{"https://raw.githubusercontent.com/pengdsci/STA551/main/w09/w09{-}notAdmitted.csv"}\NormalTok{)}
\DocumentationTok{\#\# add an admission status variable to both sub{-}samples.}
\NormalTok{admitted}\SpecialCharTok{$}\NormalTok{status }\OtherTok{=} \StringTok{"yes"}                         \CommentTok{\# admitted}
\NormalTok{notadmitted}\SpecialCharTok{$}\NormalTok{status }\OtherTok{=} \StringTok{"no "}                      \CommentTok{\# not admitted}
\NormalTok{admission }\OtherTok{=} \FunctionTok{rbind}\NormalTok{(admitted, notadmitted)        }\CommentTok{\# combining the two sub{-}samples}
\DocumentationTok{\#\# Define an empty vector to store bootstrap AUCs.}
\NormalTok{btAUC.vec }\OtherTok{=} \FunctionTok{c}\NormalTok{()}
\DocumentationTok{\#\# Select the number of bootstrap samples to be generated}
\NormalTok{B }\OtherTok{=} \DecValTok{1000}
\DocumentationTok{\#\# Size of the original sample}
\NormalTok{sample.size }\OtherTok{=} \FunctionTok{dim}\NormalTok{(admission)[}\DecValTok{1}\NormalTok{]}
\DocumentationTok{\#\# Vector of cut{-}off probabilities for construct ROC}
\NormalTok{cut.off.seq }\OtherTok{=} \FunctionTok{seq}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{1}\NormalTok{, }\AttributeTok{length =} \DecValTok{100}\NormalTok{)}
\CommentTok{\# bootstrap procedure starts here}
\ControlFlowTok{for}\NormalTok{ (k }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\NormalTok{B)\{}
\NormalTok{  boot.id }\OtherTok{=} \FunctionTok{sample}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\NormalTok{sample.size, sample.size, }\AttributeTok{replace =} \ConstantTok{TRUE}\NormalTok{)   }\CommentTok{\# Bootstrap IDs}
\NormalTok{  boot.sample }\OtherTok{=}\NormalTok{ admission[boot.id,]      }\CommentTok{\# Bootstrap samples         }
  \DocumentationTok{\#\# Bootstrap logistic regression model is given below}
\NormalTok{  boot.logistic }\OtherTok{=} \FunctionTok{glm}\NormalTok{(}\FunctionTok{factor}\NormalTok{(status)}\SpecialCharTok{\textasciitilde{}}\NormalTok{., }\AttributeTok{family =}\NormalTok{ binomial, }\AttributeTok{data =}\NormalTok{ boot.sample)}
  \DocumentationTok{\#\#}
\NormalTok{  pred.prob }\OtherTok{=} \FunctionTok{predict.glm}\NormalTok{(boot.logistic, }\AttributeTok{newdata =}\NormalTok{ boot.sample, }\AttributeTok{type =} \StringTok{"response"}\NormalTok{)}
  \DocumentationTok{\#\# vectors to store sensitivity and specificity}
\NormalTok{  sensitivity.vec }\OtherTok{=} \ConstantTok{NULL}
\NormalTok{  specificity.vec }\OtherTok{=} \ConstantTok{NULL}
    \ControlFlowTok{for}\NormalTok{ (i }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\DecValTok{100}\NormalTok{)\{}
\NormalTok{   pred.status }\OtherTok{=} \FunctionTok{as.numeric}\NormalTok{(pred.prob }\SpecialCharTok{\textgreater{}}\NormalTok{ cut.off.seq[i])}
   \DocumentationTok{\#\#\# components for defining various measures}
\NormalTok{   TN }\OtherTok{=} \FunctionTok{sum}\NormalTok{(pred.status }\SpecialCharTok{==} \DecValTok{0} \SpecialCharTok{\&}\NormalTok{ boot.sample}\SpecialCharTok{$}\NormalTok{status }\SpecialCharTok{==} \StringTok{"no "}\NormalTok{)}
\NormalTok{   FN }\OtherTok{=} \FunctionTok{sum}\NormalTok{(pred.status }\SpecialCharTok{==} \DecValTok{0} \SpecialCharTok{\&}\NormalTok{ boot.sample}\SpecialCharTok{$}\NormalTok{status }\SpecialCharTok{==} \StringTok{"yes"}\NormalTok{)}
\NormalTok{   FP }\OtherTok{=} \FunctionTok{sum}\NormalTok{(pred.status }\SpecialCharTok{==} \DecValTok{1} \SpecialCharTok{\&}\NormalTok{ boot.sample}\SpecialCharTok{$}\NormalTok{status }\SpecialCharTok{==} \StringTok{"no "}\NormalTok{)}
\NormalTok{   TP }\OtherTok{=} \FunctionTok{sum}\NormalTok{(pred.status }\SpecialCharTok{==} \DecValTok{1} \SpecialCharTok{\&}\NormalTok{ boot.sample}\SpecialCharTok{$}\NormalTok{status }\SpecialCharTok{==} \StringTok{"yes"}\NormalTok{)}
   \DocumentationTok{\#\#\#}
\NormalTok{   sensitivity.vec[i] }\OtherTok{=}\NormalTok{ TP }\SpecialCharTok{/}\NormalTok{ (TP }\SpecialCharTok{+}\NormalTok{ FN)}
\NormalTok{   specificity.vec[i] }\OtherTok{=}\NormalTok{ TN }\SpecialCharTok{/}\NormalTok{ (TN }\SpecialCharTok{+}\NormalTok{ FP)}
\NormalTok{  \}}
\NormalTok{  one.minus.spec }\OtherTok{=} \DecValTok{1} \SpecialCharTok{{-}}\NormalTok{ specificity.vec}
\NormalTok{  sens.vec }\OtherTok{=}\NormalTok{ sensitivity.vec}
  \DocumentationTok{\#\# A better approx of ROC, need library \{pROC\}}
\NormalTok{  prediction }\OtherTok{=}\NormalTok{ pred.prob}
\NormalTok{  category }\OtherTok{=}\NormalTok{ boot.sample}\SpecialCharTok{$}\NormalTok{status }\SpecialCharTok{==} \StringTok{"yes"}
\NormalTok{  ROCobj }\OtherTok{\textless{}{-}} \FunctionTok{roc}\NormalTok{(category, prediction, }\AttributeTok{quiet =} \ConstantTok{TRUE}\NormalTok{)}
\NormalTok{  btAUC.vec[k] }\OtherTok{=} \FunctionTok{round}\NormalTok{(}\FunctionTok{auc}\NormalTok{(ROCobj),}\DecValTok{4}\NormalTok{)}
\NormalTok{\}}
\FunctionTok{hist}\NormalTok{(btAUC.vec, }\AttributeTok{xlab =} \StringTok{"Bootstrap AUC"}\NormalTok{, }\AttributeTok{main =} \StringTok{"Bootstrap Sampling Distribution }
\StringTok{     of AUCs }\SpecialCharTok{\textbackslash{}n}\StringTok{ (Admission Prediction Model)"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics{STA551EB_files/figure-latex/unnamed-chunk-225-1} 

}

\caption{ The bootstrap sampling distribution of the area under the curve of ROC (graduate admission prediction)}\label{fig:unnamed-chunk-225}
\end{figure}

The 95\% bootstrap confidence interval of the AUC is defined to be 2.5\% and 97.5\% quantiles of the bootstrap AUCs.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{pander}\NormalTok{(}\FunctionTok{quantile}\NormalTok{(btAUC.vec, }\FunctionTok{c}\NormalTok{(}\FloatTok{0.025}\NormalTok{, }\FloatTok{0.975}\NormalTok{)))}
\end{Highlighting}
\end{Shaded}

\begin{longtable}[]{@{}
  >{\centering\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.1250}}
  >{\centering\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.1250}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\centering
2.5\%
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
97.5\%
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
0.6415 & 0.7497 \\
\end{longtable}

The confidence interval of AUC can be used for variable selection: if two confidence intervals of AUC overlapped, the predictive performances of the two corresponding predictive models are not significantly different. A simpler model should be recommended for implementation.

\hfill\break

\hypertarget{fraud-detection-data}{%
\subsubsection{Fraud Detection Data}\label{fraud-detection-data}}

We use the same fraud data that was used in an earlier mote.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fraud.data }\OtherTok{=} \FunctionTok{read.csv}\NormalTok{(}\StringTok{"https://pengdsci.github.io/datasets/FraudIndex/fraudidx.csv"}\NormalTok{)[,}\SpecialCharTok{{-}}\DecValTok{1}\NormalTok{]}
\DocumentationTok{\#\# recode status variable: bad = 1 and good = 0}
\NormalTok{good.id }\OtherTok{=} \FunctionTok{which}\NormalTok{(fraud.data}\SpecialCharTok{$}\NormalTok{status }\SpecialCharTok{==} \StringTok{" good"}\NormalTok{) }
\NormalTok{bad.id }\OtherTok{=} \FunctionTok{which}\NormalTok{(fraud.data}\SpecialCharTok{$}\NormalTok{status }\SpecialCharTok{==} \StringTok{"fraud"}\NormalTok{)}
\DocumentationTok{\#\#}
\NormalTok{fraud.data}\SpecialCharTok{$}\NormalTok{fraud.status }\OtherTok{=} \DecValTok{0}
\NormalTok{fraud.data}\SpecialCharTok{$}\NormalTok{fraud.status[bad.id] }\OtherTok{=} \DecValTok{1}
\end{Highlighting}
\end{Shaded}

Next, we perform bootstrap logistic regression with 1000 bootstrap samples and build 1000 bootstrap logistic regression models and calculate the AUC of the ROC of the corresponding bootstrap logistic regression models.

\begin{verbatim}
## Define an empty vector to store bootstrap AUCs.
btAUC.vec = c()
## select the number of bootstrap samples to be generated
B = 1000
## Size of the original sample
sample.size = dim(fraud.data)[1]
## Vector of cut-off probabilities for construct ROC
cut.off.seq = seq(0,1, length = 100)
# bootstrap procedure starts here
for (k in 1:B){
  boot.id = sample(1:sample.size, sample.size, replace = TRUE)   # Bootstrap IDs
  boot.sample = fraud.data[boot.id,]      # Bootstrap samples         
  ## Bootstrap logistic regression model is given below
  boot.logistic = glm(factor(status) ~ index, family = binomial, data = boot.sample)
  ##
  newdata = data.frame(index= boot.sample$index) 
  pred.prob = predict.glm(boot.logistic, newdata, type = "response")
  ## vectors to store sensitivity and specificity
  sensitivity.vec = NULL
  specificity.vec = NULL
  for (i in 1:100){
   pred.status = as.numeric(pred.prob > cut.off.seq[i])
   ### components for defining various measures
   TN = sum(pred.status == 0 & boot.sample$fraud.status == 0)
   FN = sum(pred.status == 0 & boot.sample$fraud.status == 1)
   FP = sum(pred.status == 1 & boot.sample$fraud.status == 0)
   TP = sum(pred.status == 1 & boot.sample$fraud.status == 1)
   ###
   sensitivity.vec[i] = TP / (TP + FN)
   specificity.vec[i] = TN / (TN + FP)
  }
  one.minus.spec = 1 - specificity.vec
  sens.vec = sensitivity.vec
  ## A better approx of ROC, need library {pROC}
  prediction = pred.prob
  category = boot.sample$fraud.status == 1
  ROCobj <- roc(category, prediction)
  btAUC.vec[k] = round(auc(ROCobj),4)
}
hist(btAUC.vec, xlab = "Bootstrap AUC",
main = "Bootstrap Sampling Distribution of AUCs \n (Fraud Detection Model)")
\end{verbatim}

\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{img09/w09-BootstrapAUCFraudPred} 

}

\caption{Bootstrap sampling distribution of AUC (fraud prediction)}\label{fig:unnamed-chunk-228}
\end{figure}

With the bootstrap AUCs, we can similarly construct the 95\% bootstrap confidence interval for the AUC of the fraud detection model is \((0.9234, 0.9313 )\).

\hfill\break

\hypertarget{concepts-of-ensemble-algorithms}{%
\section{Concepts of Ensemble Algorithms}\label{concepts-of-ensemble-algorithms}}

We have taken many bootstrap samples and built a logistic regression model on each of these bootstrap samples and calculate the area under the ROC curve of associated logistic regression models. Using these bootstrap AUCs, we approximate the find the bootstrap sampling distribution of the AUC and, hence, find the confidence interval of the AUC.

We can follow the same steps to find the bootstrap confidence intervals of the AUCs of decision trees and neural net algorithms. One important application of Bootstrap in machine learning is its ability to aggregate a set of \emph{weak} models and algorithms to make a \emph{stronger} combined model - This is the so-called \textbf{ensemble} learning method in machine learning. One way of improving the performance of a \emph{weak model} through an ensemble approach is to reduce the risk of \textbf{overfitting} and \textbf{underfitting} by balancing the trade-off between \textbf{bias} and \textbf{variance},

\hypertarget{overfitting-v.s.-underfitting}{%
\subsection{Overfitting v.s. Underfitting}\label{overfitting-v.s.-underfitting}}

In predictive modeling, \textbf{bias} is a phenomenon that skews the result of an algorithm in favor of or against the ground truth. It describes how well the model matches the training data set:

\begin{itemize}
\tightlist
\item
  A model with a higher bias would not match the data set closely.
\item
  A low-bias model will closely match the training data set.
\end{itemize}

\textbf{Variance} refers to the changes in the model when using different portions of the training data set. The variance comes from highly complex (but valid) models with a large number of features

\begin{itemize}
\tightlist
\item
  Models with high bias will have low variance.
\item
  Models with high variance will have a low bias.
\end{itemize}

The terms \textbf{underfitting} and \textbf{overfitting} refer to how the model fails to match the data.

\begin{itemize}
\item
  \textbf{Underfitting} occurs when the model is too simple to be able to match the input data to the target data.
\item
  \textbf{Overfitting} occurs when the model is highly complex but perfectly matches almost all the given data points and performs well in training data sets. However, the model would not be able to generalize the data point in the test data set to predict the outcome accurately due to high error (variation).
\end{itemize}

\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{img09/w09-BiasVarianceOverUnderFitting} 

}

\caption{Bias, variance, overfitting and underfitting of predictive models}\label{fig:unnamed-chunk-229}
\end{figure}

\hypertarget{the-logic-ensemble-learning-method}{%
\subsection{The Logic Ensemble Learning Method}\label{the-logic-ensemble-learning-method}}

\textbf{Ensemble learning} attests to the idea of the ``wisdom of crowds,'' which suggests that the decision-making of a larger group of people is typically better than that of an individual expert. Similarly, ensemble learning refers to a group (or ensemble) of base learners (eg., models and algorithms), which work collectively to achieve a better final prediction.

A single model or algorithm, also known as a base or weak learner, may not perform well individually due to high variance or high bias. However, when weak learners are aggregated, they can form a strong learner (with stable performance and low variance) yielding better model performance.

Many different types of ensemble learning methods have been developed in the past few decades, Among them, boosting and Bootstrap aggregation (BAGGING) methods are commonly used in practice.

\hfill\break

\hypertarget{bagging-ensemble-methods}{%
\subsection{BAGGING Ensemble Methods}\label{bagging-ensemble-methods}}

\textbf{Bagging} is an acronym for \emph{Bootstrap Aggregation} and is used to decrease the variance in the prediction model (the idea we adopted when identifying the optimal cut-off scores and the confidence interval of AUC).

\textbf{Bagging} is a \textbf{\color{blue}parallel} method that fits \textbf{\color{blue}different}, considered learners \textbf{\color{blue}independently} from each other, making it possible to train them \textbf{\color{red}simultaneously}.

\textbf{Bagging} generates additional data for training from the data set. This is achieved by \textbf{\color{red}bootstrap sampling} (random sampling with replacement) from the original data set. As discussed in earlier sections, \textbf{Bootstrap Sampling} may repeat some observations in each new training data set. This guarantees that every element in \textbf{Bagging} is equally probable for appearing in a \textbf{bootstrap} data set.

These bootstrap data sets are used to train multiple models in \textbf{parallel}. The average of all the predictions from different ensemble models is calculated. The \textbf{majority vote} gained from the voting mechanism is considered when classification is made. \emph{\color{red}Bagging decreases the variance and tunes the prediction to an expected outcome}.

The following figure explains the BAGGING algorithm applied to the decision tree algorithm.

\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{img09/w09-BAGGING} 

}

\caption{The idea of Gagging ensemble algorithm.}\label{fig:unnamed-chunk-230}
\end{figure}

In summary, the bagging algorithm has three basic steps:

\textbf{Bootstrapping}: Bagging leverages a bootstrapping sampling technique to create diverse samples.

\begin{itemize}
\item
  \textbf{Parallel training}: These bootstrap samples are then trained independently and in parallel with each other using weak or base learners (de).
\item
  \textbf{Aggregation}: Finally, aggregating the outputs from individual algorithms/models. In the case of using a tree algorithm (like the above figure)

  \begin{itemize}
  \tightlist
  \item
    \emph{\color{red}for regression}, an average of all terminal node weights is taken of all the outputs predicted by the individual classifiers; this is known as \textbf{soft voting}.
  \item
    \emph{\color{red}for classification}, the class (defined with default 0.5 in each individual tree) with the highest majority of votes is accepted; this is known as \textbf{hard voting or majority voting}.
  \end{itemize}
\end{itemize}

\hfill\break

\hypertarget{case-study-i-bagging-classification-trees}{%
\subsubsection{Case Study I: BAGGING Classification Trees}\label{case-study-i-bagging-classification-trees}}

\hfill\break

\textbf{BAGGING} is implemented in R libraries \textbf{ipred\{\}} and \textbf{rpart\{\}}. They are usually used with several other libraries such as \textbf{caret\{\}} and \textbf{e1071\{\}} to extract relevant information in the bagging algorithm.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Pima }\OtherTok{=} \FunctionTok{read.csv}\NormalTok{(}\StringTok{"https://pengdsci.github.io/STA551/w03/AnalyticPimaDiabetes.csv"}\NormalTok{)[,}\SpecialCharTok{{-}}\DecValTok{1}\NormalTok{]}
\CommentTok{\# We use a random split approach}
\NormalTok{n }\OtherTok{=} \FunctionTok{dim}\NormalTok{(Pima)[}\DecValTok{1}\NormalTok{]  }\CommentTok{\# sample size}
\CommentTok{\# caution: using without replacement}
\NormalTok{train.id }\OtherTok{=} \FunctionTok{sample}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\NormalTok{n, }\FunctionTok{round}\NormalTok{(}\FloatTok{0.7}\SpecialCharTok{*}\NormalTok{n), }\AttributeTok{replace =} \ConstantTok{FALSE}\NormalTok{)  }
\NormalTok{train }\OtherTok{=}\NormalTok{ Pima[train.id, ]    }\CommentTok{\# training data}
\NormalTok{test }\OtherTok{=}\NormalTok{ Pima[}\SpecialCharTok{{-}}\NormalTok{train.id, ]    }\CommentTok{\# testing data}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\DocumentationTok{\#\#}
\NormalTok{Diabetes.bag.train }\OtherTok{\textless{}{-}} \FunctionTok{bagging}\NormalTok{(}\FunctionTok{as.factor}\NormalTok{(diabetes) }\SpecialCharTok{\textasciitilde{}}\NormalTok{ ., }
                              \AttributeTok{data =}\NormalTok{ train, }
                              \AttributeTok{nbagg =} \DecValTok{150}\NormalTok{,    }\CommentTok{\# number of trees}
                              \AttributeTok{coob =} \ConstantTok{TRUE}\NormalTok{, }
                              \AttributeTok{parms =} \FunctionTok{list}\NormalTok{(}\AttributeTok{loss =} \FunctionTok{matrix}\NormalTok{(}\FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{10}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{), }
                                                          \AttributeTok{ncol =} \DecValTok{20}\NormalTok{, }
                                                          \AttributeTok{byrow =} \ConstantTok{TRUE}\NormalTok{),   }
                                                          \AttributeTok{split =} \StringTok{"gini"}\NormalTok{),  }
                              \AttributeTok{control =} \FunctionTok{rpart.control}\NormalTok{(}\AttributeTok{minsplit =} \DecValTok{10}\NormalTok{,}\AttributeTok{cp =} \FloatTok{0.02}\NormalTok{))}
\DocumentationTok{\#\#\# predict() returns either "class" or "prob" in the classification}
\DocumentationTok{\#\#\# When specifying type = "class", the default cut{-}off of 0.5 is used.}
\NormalTok{pred }\OtherTok{=} \FunctionTok{predict}\NormalTok{(Diabetes.bag.train, train, }\AttributeTok{type =} \StringTok{"prob"}\NormalTok{)}
\DocumentationTok{\#\#\# Optimal cut{-}off probability identification: no cross{-}validation is needed}
\NormalTok{cut.prob }\OtherTok{=} \FunctionTok{seq}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{1}\NormalTok{, }\AttributeTok{length =} \DecValTok{20}\NormalTok{)}
\NormalTok{  senspe.mtx }\OtherTok{=} \FunctionTok{matrix}\NormalTok{(}\DecValTok{0}\NormalTok{, }\AttributeTok{ncol =} \FunctionTok{length}\NormalTok{(cut.prob), }\AttributeTok{nrow=} \DecValTok{3}\NormalTok{, }\AttributeTok{byrow =} \ConstantTok{FALSE}\NormalTok{)}
  \ControlFlowTok{for}\NormalTok{ (i }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\FunctionTok{length}\NormalTok{(cut.prob))\{}
  \CommentTok{\# }\AlertTok{CAUTION}\CommentTok{: "pos" and "neg" are values of the label in this data set!}
  \CommentTok{\# The following line uses only "pos" probability: pred[, "pos"] !!!!}
\NormalTok{  pred.out }\OtherTok{=}  \FunctionTok{ifelse}\NormalTok{(pred[,}\StringTok{"pos"}\NormalTok{] }\SpecialCharTok{\textgreater{}=}\NormalTok{ cut.prob[i], }\StringTok{"pos"}\NormalTok{, }\StringTok{"neg"}\NormalTok{)  }
\NormalTok{  TP }\OtherTok{=} \FunctionTok{sum}\NormalTok{(pred.out }\SpecialCharTok{==}\StringTok{"pos"} \SpecialCharTok{\&}\NormalTok{ train}\SpecialCharTok{$}\NormalTok{diabetes }\SpecialCharTok{==} \StringTok{"pos"}\NormalTok{)}
\NormalTok{  TN }\OtherTok{=} \FunctionTok{sum}\NormalTok{(pred.out }\SpecialCharTok{==}\StringTok{"neg"} \SpecialCharTok{\&}\NormalTok{ train}\SpecialCharTok{$}\NormalTok{diabetes }\SpecialCharTok{==} \StringTok{"neg"}\NormalTok{)}
\NormalTok{  FP }\OtherTok{=} \FunctionTok{sum}\NormalTok{(pred.out }\SpecialCharTok{==}\StringTok{"pos"} \SpecialCharTok{\&}\NormalTok{ train}\SpecialCharTok{$}\NormalTok{diabetes }\SpecialCharTok{==} \StringTok{"neg"}\NormalTok{)}
\NormalTok{  FN }\OtherTok{=} \FunctionTok{sum}\NormalTok{(pred.out }\SpecialCharTok{==}\StringTok{"neg"} \SpecialCharTok{\&}\NormalTok{ train}\SpecialCharTok{$}\NormalTok{diabetes }\SpecialCharTok{==} \StringTok{"pos"}\NormalTok{)}
\NormalTok{  senspe.mtx[}\DecValTok{1}\NormalTok{,i] }\OtherTok{=}\NormalTok{ TP}\SpecialCharTok{/}\NormalTok{(TP }\SpecialCharTok{+}\NormalTok{ FN)                  }\CommentTok{\# sensitivity}
\NormalTok{  senspe.mtx[}\DecValTok{2}\NormalTok{,i] }\OtherTok{=}\NormalTok{ TN}\SpecialCharTok{/}\NormalTok{(TN }\SpecialCharTok{+}\NormalTok{ FP)                  }\CommentTok{\# specificity}
\NormalTok{  senspe.mtx[}\DecValTok{3}\NormalTok{,i] }\OtherTok{=}\NormalTok{ (TP}\SpecialCharTok{+}\NormalTok{TN)}\SpecialCharTok{/}\NormalTok{(TP }\SpecialCharTok{+}\NormalTok{ FN }\SpecialCharTok{+}\NormalTok{ TN }\SpecialCharTok{+}\NormalTok{ FP)   }\CommentTok{\# accuracy}
\NormalTok{  \}}
  \DocumentationTok{\#\# A better approx of ROC, need library \{pROC\}}
\NormalTok{  prediction }\OtherTok{=}\NormalTok{ pred[, }\StringTok{"pos"}\NormalTok{]}
\NormalTok{  category }\OtherTok{=}\NormalTok{ train}\SpecialCharTok{$}\NormalTok{diabetes }\SpecialCharTok{==} \StringTok{"pos"}
\NormalTok{  ROCobj }\OtherTok{\textless{}{-}} \FunctionTok{roc}\NormalTok{(category, prediction)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Setting levels: control = FALSE, case = TRUE
\end{verbatim}

\begin{verbatim}
## Setting direction: controls < cases
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{  AUC }\OtherTok{=} \FunctionTok{auc}\NormalTok{(ROCobj)}
\NormalTok{  AUC }\OtherTok{=} \FunctionTok{round}\NormalTok{(}\FunctionTok{as.vector}\NormalTok{(AUC[}\DecValTok{1}\NormalTok{]),}\DecValTok{3}\NormalTok{)}
  \DocumentationTok{\#\#\#}
\NormalTok{  n }\OtherTok{=} \FunctionTok{length}\NormalTok{(senspe.mtx[}\DecValTok{3}\NormalTok{,])}
\NormalTok{  idx }\OtherTok{=} \FunctionTok{which}\NormalTok{(senspe.mtx[}\DecValTok{3}\NormalTok{,] }\SpecialCharTok{==} \FunctionTok{max}\NormalTok{(senspe.mtx[}\DecValTok{3}\NormalTok{,]))}
\NormalTok{  tick.label }\OtherTok{=} \FunctionTok{as.character}\NormalTok{(}\FunctionTok{round}\NormalTok{(cut.prob,}\DecValTok{2}\NormalTok{))}
  \DocumentationTok{\#\#\#}
  \FunctionTok{par}\NormalTok{(}\AttributeTok{mfrow =} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{))}
  \FunctionTok{plot}\NormalTok{(}\DecValTok{1}\SpecialCharTok{{-}}\NormalTok{senspe.mtx[}\DecValTok{2}\NormalTok{,], senspe.mtx[}\DecValTok{1}\NormalTok{,], }\AttributeTok{xlim=}\FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{1}\NormalTok{), }\AttributeTok{ylim =} \FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{1}\NormalTok{),}
       \AttributeTok{xlab =} \StringTok{"1 {-} specificity"}\NormalTok{, }
       \AttributeTok{ylab =} \StringTok{"Sensitivity"}\NormalTok{, }\AttributeTok{main =} \StringTok{"ROC (Taining Data)"}\NormalTok{, }\AttributeTok{type=}\StringTok{"l"}\NormalTok{,}\AttributeTok{cex.main =} \FloatTok{0.8}\NormalTok{)}
  \FunctionTok{legend}\NormalTok{(}\StringTok{"bottomright"}\NormalTok{,}\FunctionTok{c}\NormalTok{(}\StringTok{"fn = 10"}\NormalTok{, }\StringTok{"fp = 1"}\NormalTok{, }\StringTok{"cp = 0.02"}\NormalTok{, }\FunctionTok{paste}\NormalTok{(}\StringTok{"AUC ="}\NormalTok{, AUC)),}
         \AttributeTok{bty=}\StringTok{"n"}\NormalTok{, }\AttributeTok{cex =} \FloatTok{0.8}\NormalTok{)       }
  
  \FunctionTok{plot}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\FunctionTok{length}\NormalTok{(cut.prob), senspe.mtx[}\DecValTok{3}\NormalTok{,], }\AttributeTok{xlab=}\StringTok{"cut{-}off probability"}\NormalTok{,}
       \AttributeTok{ylab =} \StringTok{"accuracy"}\NormalTok{, }\AttributeTok{ylim=}\FunctionTok{c}\NormalTok{(}\FunctionTok{min}\NormalTok{(senspe.mtx[}\DecValTok{3}\NormalTok{,]),}\DecValTok{1}\NormalTok{),}
               \AttributeTok{axes =} \ConstantTok{FALSE}\NormalTok{,}
        \AttributeTok{main=}\StringTok{"cut{-}off vs accuracy"}\NormalTok{,}
        \AttributeTok{cex.main =} \FloatTok{0.9}\NormalTok{,}
        \AttributeTok{col.main =} \StringTok{"navy"}\NormalTok{)}
        \FunctionTok{axis}\NormalTok{(}\DecValTok{1}\NormalTok{, }\AttributeTok{at=}\DecValTok{1}\SpecialCharTok{:}\DecValTok{20}\NormalTok{, }\AttributeTok{label =}\NormalTok{ tick.label, }\AttributeTok{las =} \DecValTok{2}\NormalTok{)}
        \FunctionTok{axis}\NormalTok{(}\DecValTok{2}\NormalTok{)}
        \FunctionTok{points}\NormalTok{(idx, senspe.mtx[}\DecValTok{3}\NormalTok{,][idx], }\AttributeTok{pch=}\DecValTok{19}\NormalTok{, }\AttributeTok{col =} \StringTok{"red"}\NormalTok{)}
        \FunctionTok{segments}\NormalTok{(idx , }\FunctionTok{min}\NormalTok{(senspe.mtx[}\DecValTok{3}\NormalTok{,]), idx , senspe.mtx[}\DecValTok{3}\NormalTok{,][idx ], }\AttributeTok{col =} \StringTok{"red"}\NormalTok{)}
       \FunctionTok{text}\NormalTok{(idx, senspe.mtx[}\DecValTok{3}\NormalTok{,][idx]}\SpecialCharTok{+}\FloatTok{0.03}\NormalTok{, }\FunctionTok{as.character}\NormalTok{(}\FunctionTok{round}\NormalTok{(senspe.mtx[}\DecValTok{3}\NormalTok{,][idx],}\DecValTok{4}\NormalTok{)), }
            \AttributeTok{col =} \StringTok{"red"}\NormalTok{, }\AttributeTok{cex =} \FloatTok{0.8}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{STA551EB_files/figure-latex/unnamed-chunk-232-1} 

}

\caption{The ROC curve and the plot for optimal cut-off determination.}\label{fig:unnamed-chunk-232}
\end{figure}

Similarly, there are several \textbf{\color{red}hyperparameters} one can \textbf{tune} to find the best-bootstrapped decision tree. For those who are interested in exploring more in \textbf{tunning hyperparameters}, you can write a wrapper of \textbf{bagging()} and pass hyperparameters such as \textbf{fp, fn, cp, minisplit} in the list of arguments in \textbf{rpart()} to find the best bootstrap decision tree.

\hypertarget{boosting-ensemble-methods}{%
\subsection{Boosting Ensemble Methods}\label{boosting-ensemble-methods}}

Boosting is a sequential ensemble method that iteratively \textbf{adjusts the weight of data points} as per the last classification. If a data point is incorrectly classified, it increases the weight of that data point. It decreases the bias and variance (hence the predictive error) and builds strong predictive models.

Data points misclassified in each iteration are spotted, and their weights are increased. The Boosting algorithm allocates weights to each resulting model during training. A model (also commonly called learner) with good training data prediction results will be assigned a higher weight. When evaluating a new learner, Boosting keeps track of the learner's errors.

The steps of boosting algorithm are summarized in the following:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Data points in the initial training data set are equally weighted.
\item
  A based model is created for the initial training data set.
\item
  classification Errors are counted using actual and predicted values. The data point that was incorrectly predicted is provided a higher weight.
\item
  a model is built on the modified data (re-weighted data points)
\item
  the process is iterated for multiple models and each of them corrects the previous model's errors.
\item
  The final model works as a strong learner and shows the weighted mean of all the models.
\end{enumerate}

The following figure demonstrates the rough idea of boosting decision trees in a conceptual approach.

\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{img09/w09-DemoBoostedTreeAlgorithm} 

}

\caption{Graphical representation of boosting tree method}\label{fig:unnamed-chunk-233}
\end{figure}

\hypertarget{bagging-versus-boosting}{%
\subsection{Bagging Versus Boosting}\label{bagging-versus-boosting}}

\textbf{Bagging} and \textbf{Boosting} have a universal similarity of being classified as ensemble methods. In addition, Bagging and Boosting

\begin{itemize}
\item
  are ensemble methods focused on getting \(N\) learners from a single learner.
\item
  make random sampling and generate several training data sets.
\item
  arrive upon the end decision by making an average of \(N\) learners or taking the voting rank done by most of them.
\item
  reduce variance and provide higher stability by minimizing errors.
\end{itemize}

However, the two ensemble algorithms are very different from the technical perspective. The following table shows these structural differences.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.5135}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.4865}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Bagging
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Boosting
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Merging the same type of predictions. & Merging different types of predictions. \\
Decreases variance, not bias, and solves over-fitting issues. & Decreases bias, not variance. \\
Each model receives an equal weight. & Models are weighed based on their performance. \\
Models are built independently. & New models are affected by a previously built model's performance. \\
Training data subsets are drawn randomly with a bootstrap sample. & Every new subset comprises the elements that were misclassified by previous models. \\
Usually applied where the classifier is unstable and has a high variance. & Usually applied where the classifier is stable and simple and has a high bias. \\
\end{longtable}

The boosting ensemble algorithms have various new development in recent years. In general, they are more mathematically and pragmatically demanded in implementation. We will not go into details about any of boosting algorithms and implement them.

To conclude, we use the following figure to show the role of mathematical tools and thinking in the evolution of tree-based algorithms: the more mathematical tools you have, the more powerful models you are capable of developing!

\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{img09/w09-FromBagging2Boosting} 

}

\caption{Evolution of tree-based algorithms with various level of technical tools.}\label{fig:unnamed-chunk-234}
\end{figure}

\hfill\break

\hypertarget{algorithms-for-anomaly-detection}{%
\chapter{Algorithms for Anomaly Detection}\label{algorithms-for-anomaly-detection}}

Credit card fraud is a type of identity theft that occurs when someone illegally uses another person's credit card or account information for an unauthorized transaction. Fraud can happen as a result of a stolen, misplaced, or counterfeit credit card.

According to Nilson Report {[}\url{https://nilsonreport.com/content_promo.php?id_promo=16}{]}, card fraud losses worldwide reached \$28.65 billion in 2020. By 2025, the United States is projected to reach \$12.5 billion in card fraud losses.

\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{img09/w09-CardFraudLoss-NilsonReport} 

}

\caption{The trend of worldwide card fraud loss.}\label{fig:unnamed-chunk-236}
\end{figure}

Among all card fraud, credit card fraud contributes a significant portion. Unlike the process of identifying credit default in which we have a lot of information about cardholders, in the fraud domain, we have no information about fraudsters except for the amount, location, and time of the fraudulent transactions. This makes the detection and identification of credit card fraud more challenging.

There are different types of credit card fraud. The most frequently occurring types of credit card fraud include lost and stolen, counterfeit, card-not-present (CNP), account takeover, application fraud, card-never-received, etc. The different strategies of prevention for different types of fraud have been developed by financial industries over the years. These include educating cardholders on how to protect their personal information, increasing security measures such as using chip-enabled cards and enhanced web portals for online transactions, monitoring card shipment, etc.

Fraud detection, on the other hand, is a more complex and difficult process. This is where analytics come into play.

In the rest of this note, we will

\begin{itemize}
\tightlist
\item
  overview of the statistical models and machine learning algorithms that are used in credit card fraud detection;
\item
  introduce the statistical foundation of the proposed algorithm;
\item
  detail the proposed new algorithms based on sequential data;
\item
  evaluate the performance of the proposed algorithms
\item
  introduce the potential improvement and generalization of the proposed algorithms.
\end{itemize}

\hypertarget{anomaly-detection-use-case-fraud-operation}{%
\section{Anomaly Detection Use Case: Fraud Operation}\label{anomaly-detection-use-case-fraud-operation}}

In essence, identifying fraud is an anomaly detection process that identifies unusual patterns that do not fit normal behavior in the data generation process. In addition to fraud detection, anomaly detection has many applications in business such as intrusion detection (identifying strange patterns in network traffic), health monitoring (spotting a malignant tumor in an MRI scan), fault detection in operating environments, etc.

Three different types of anomalies can be found in credit card fraud.

\textbf{Global Anomalies} -- if a data point is too far from the rest, it falls into the category of point anomalies (outliers). For example, if all historical fuel costs are below \$75, but the most recent transaction is \$149.50. The most recent transaction is abnormal.

\textbf{Contextual Anomalies} -- If the event is anomalous in specific circumstances (context), then we have contextual anomalies. As data becomes more and more complex, it is vital to use anomaly detection methods for the context. This anomaly type is common in sequence data. For example, - a card was used for fuel roughly 4-5 times a month historically, but the same card was used at gas pumps 5 times within the last 3 hours.

\textbf{Collective Anomalies}. The collective anomaly denotes a collection of anomalous concerning the multiple feature variables, but not individual objects. For example: if one card was skimmed at a pump, the fraudsters then make many duplicate cards and sell them to card other fraudsters. The counterfeit cards may be used in very different geographic locations at approximately the ``same time''. Each transaction associated with this card may be normal. However, we will see the abnormality if we look at these transactions collectively.

Sometimes, people break anomaly detection algorithms down into two subclasses: outlier detection and novelty detection.

\begin{itemize}
\item
  \textbf{Outlier detection}: the input data set contains examples of both standard events and anomaly events. These algorithms seek to fit regions of the training data where the standard events are most concentrated, disregarding, and therefore isolating the anomaly events. Such algorithms are often trained in an unsupervised fashion (i.e., without labels). We sometimes use these methods to help clean and pre-process datasets before applying additional machine-learning techniques.
\item
  \textbf{Novelty detection}: Unlike outlier detection, which includes examples of both standard and anomaly events, novelty detection algorithms have only the standard event data points (i.e., no anomaly events) during training time. During training, these algorithms use only labeled examples of standard events (supervised learning). At the time of testing/prediction, novelty detection algorithms must detect when an input data point is an outlier.
\end{itemize}

Various machine learning algorithms and statistical models could be used for credit card fraud detection dependent on the amount of information on the types of fraud. But they are broadly classified into supervised and unsupervised methods. Some of the classical methods are outlined in the following subsections.

\hypertarget{supervised-and-unsupervised-anomaly-detection}{%
\section{Supervised and Unsupervised Anomaly Detection}\label{supervised-and-unsupervised-anomaly-detection}}

Depending on whether the labels are available, anomaly detection techniques can be categorized into one of the following three modes:

\hypertarget{supervised-anomaly-detection}{%
\subsection{Supervised Anomaly Detection}\label{supervised-anomaly-detection}}

Techniques trained in supervised mode assume the availability of a training data set that has labeled instances for normal as well as anomaly classes. A typical approach in such cases is to build a predictive model for normal vs.~anomaly classes. Any unseen data instance is compared against the model to determine which class it belongs to.

Two major issues arise in supervised anomaly detection.

\begin{itemize}
\item
  \textbf{Imbalance Labels}: The anomalous instances are far fewer compared to the normal instances in the training data. Issues that arise
  due to imbalanced class distributions have been addressed in the data mining and
  machine-learning literature
\item
  \textbf{Inaccuracy of Label}: obtaining accurate and representative labels, especially for the anomaly class is usually challenging. This is particularly true in the world of credit card fraud operations.
\end{itemize}

Other than these two issues, the supervised anomaly detection problem is similar to building predictive models such as logistic regression and decision tree algorithms.

\hypertarget{semi-supervised-anomaly-detection.}{%
\subsection{Semi-Supervised Anomaly Detection.}\label{semi-supervised-anomaly-detection.}}

These types of methods usually assume that the training data has labeled instances for only the normal class. Since they do not require labels for the anomaly class, they are more widely applicable than supervised techniques. The typical approach used in such techniques is to build a model for the class corresponding to normal behavior and use the model to identify anomalies in the test data.

A limited set of anomaly detection techniques exist that assume the availability of
only the anomaly instances for training. Such techniques are not commonly used,
primarily because it is difficult to obtain a training data set that covers every
possible anomalous behavior that can occur in the data.

Sometimes, we have to use supervised and unsupervised methods to label some unlabeled examples to reach the minimum sample size for using supervised anomaly detection methods.

\hypertarget{unsupervised-anomaly-detection.}{%
\subsection{Unsupervised Anomaly Detection.}\label{unsupervised-anomaly-detection.}}

These types of methods operate in unsupervised mode and do not require training data, and thus are most widely applicable. The techniques in this category make the implicit assumption that normal instances are far more frequent than anomalies in the test data. If this assumption is not true then such techniques suffer from a high false alarm rate.

Many semi-supervised techniques can be adapted and used in an unsupervised
mode by using a sample of the unlabeled data set as training data. Such adaptation
assumes that the test data contains very few anomalies and the model learned during
training is robust to these few anomalies.

In the following sections, we only select two representative anomaly detection algorithms: outlier detection and novelty detection.

\hypertarget{local-outlier-factor-method-outlier-detection}{%
\section{Local Outlier Factor Method: Outlier Detection}\label{local-outlier-factor-method-outlier-detection}}

The LOF {[}proposed by Markus M. Breunig, Hans-Peter Kriegel, Raymond T. Ng, and Jorg Sander in 2000{]} is the most well-known local anomaly detection algorithm whose idea is carried out in many nearest-neighbor-based algorithms.

\hypertarget{definitions-of-some-distances}{%
\subsection{\texorpdfstring{Definitions of Some \texttt{Distances}}{Definitions of Some Distances}}\label{definitions-of-some-distances}}

Before introducing the steps for calculating the LOF score, we define the following technical terms

\textbf{Normal Distance (ND)} - any of the valid ``statistical distances'' such as Euclid, Minkowski, Manhattan, etc.

\textbf{k-distance (kD)}: For the pre-selected k, k-distance is defined to be the distance of a (new) point to its kth neighbor. For example, if k was 3, the k-distance of A, denoted by \texttt{k-distance(A)}, would be the distance of point A to its \texttt{third\ closest} point which is D (B is the first closest neighbor, C is the second closest neighbor), see the following Figure 3.

\textbf{Reachability Distance (RD)} is defined to be the maximum distance of two points and the k-distance of the \texttt{second} point. For example, the reachability distance between A and D is given by

\texttt{Reachability-Distance(A,\ D)\ =\ max\{k-distance\ (D),\ normal\_distance\ (A,\ D)\}}

Where \texttt{normal-distance(A,\ D)} could be any ``statistical distance'' that is used in \texttt{k-distance(D)}.

\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{img09/w09-calc-LOF-score} 

}

\caption{Illustration of calculating LOF score.}\label{fig:unnamed-chunk-237}
\end{figure}

\textbf{Local Reachability Density (LRD)} refers to how far we need to go from the point we are at to reach the next point or set of points. For example, For all k =3 closest neighbors of A, the reachability distances are calculated and the values found are summed and divided by the value of k =3. When the inverse of this value is taken, we calculate the density we are looking for.

\texttt{Local-reachability-density(A)\ =\ 1\ /\ (\ sum\ (\ Reachability\_Distance\ (A\ ,\ n\ )\ )\ /\ k)}

n is the number of points/neighbors centered at point A.

\textbf{Local Outlier Factor (LOF) Score}

The local reachability densities found are compared to the local reachability densities of A's nearest k neighbors. The density of each neighbor is summed up and divided by the density of A. The value found is divided by the number of neighbors i.e.~k.

\texttt{LOF(A)\ ={[}(LRD(1st.\ neighbor)\ +\ LRD(2nd.\ neighbor)\ +\ ...\ +\ LRD(kth.\ neighbor))/LRD(A){]}/k}

\textbf{Use of LOF Score} - A value of approximately 1 indicates that the object is comparable to its neighbors. A value below 1 indicates a denser region (which would be an inlier), while values significantly larger than 1 indicate outliers.

\begin{itemize}
\item
  \textbf{LOF(k) \textasciitilde{} 1} means Similar density as neighbors,
\item
  \textbf{LOF(k) \textless{} 1} means Higher density than neighbors (Inlier),
\item
  \textbf{LOF(k) \textgreater{} 1} means Lower density than neighbors (Outlier)
\end{itemize}

\hfill\break

\hypertarget{a-toy-example}{%
\subsection{A Toy Example}\label{a-toy-example}}

The calculation of the LOF score is not difficult. However, it involves multiple steps and several ``distance''-based measures. In this section, we use a toy example with 4 points depicted in the following figure.

\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{img09/w09-lof-4point-example} 

}

\caption{Toy data set.}\label{fig:unnamed-chunk-238}
\end{figure}

The following figure shows the steps for calculating the LOF score for each of the four points in the toy data set.

\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{img09/w09-manual-calculation-LOF-score} 

}

\caption{Steps for calculating LOF scores.}\label{fig:unnamed-chunk-239}
\end{figure}

We can see that the LOF score of point c is 2. Point c is a potential outlier.

\hypertarget{implementation-of-lof-in-r}{%
\subsection{Implementation of LOF in R}\label{implementation-of-lof-in-r}}

Several R packages have a function to calculate LOF scores. We use \textbf{lof()} in \textbf{\{dbscan\}} to calculate LOF scores of data points in the well-known \textbf{iris} data.

\textbf{lof()} can calculate the LOF score in a high dimensional space. The original iris data has 4 numerical variables (sepal and petal widths and lengths). We will calculate the LOF scores based on these variables (in 4-dimensional space). To visualize the LOF score, we also perform a PCA and use the first two PCs (which account for about 95\% of the total variation) to calculate LOF scores.

\textbf{A Cautionary Note on LOF Scores with PCA} - The purpose of calculating LOF scores based on the first two-component analysis is to visualize the outliers in a 2-dimensional plot. The original variables must not be scaled in the PCA to obtain comparable LOF scores. The translation of the original variable will give the same LOF score.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{log.iris }\OtherTok{=} \FunctionTok{log}\NormalTok{(iris[,}\SpecialCharTok{{-}}\DecValTok{5}\NormalTok{])   }\CommentTok{\# drop the categorical variable in the original }
                            \CommentTok{\# data set and transform all numerical to the}
                            \CommentTok{\# log{-}scale}
\NormalTok{ir.pca }\OtherTok{\textless{}{-}} \FunctionTok{prcomp}\NormalTok{(log.iris, }\AttributeTok{center =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{scale =} \ConstantTok{FALSE}\NormalTok{)}
\CommentTok{\# use the first two PCs to define a data frame for LOF}
\NormalTok{pca.iris }\OtherTok{=} \FunctionTok{data.frame}\NormalTok{(ir.pca}\SpecialCharTok{$}\NormalTok{x[, }\DecValTok{1}\SpecialCharTok{:}\DecValTok{2}\NormalTok{])}
\DocumentationTok{\#\#\# Calculate the two LOF scores with the original variables and PCs respectively.}
\NormalTok{lof.pca }\OtherTok{\textless{}{-}} \FunctionTok{lof}\NormalTok{(pca.iris, }\AttributeTok{minPts =} \DecValTok{30}\NormalTok{)  }\CommentTok{\#minPts = k value}
\NormalTok{lof.orig }\OtherTok{\textless{}{-}} \FunctionTok{lof}\NormalTok{(log.iris, }\AttributeTok{minPts =} \DecValTok{30}\NormalTok{)}
\DocumentationTok{\#\# 2D plot of LOF score based on PCs}
\FunctionTok{plot}\NormalTok{(pca.iris, }\AttributeTok{pch =} \StringTok{"x"}\NormalTok{,    }\CommentTok{\# point symbol}
     \AttributeTok{main =} \StringTok{"LOF Based on PCA"}\NormalTok{, }
     \CommentTok{\#asp = 1,}
     \AttributeTok{cex =} \FloatTok{0.5}\NormalTok{)                }\CommentTok{\# aspect ratio {-} ratio of \textquotesingle{}y/x\textquotesingle{}}
\FunctionTok{points}\NormalTok{(pca.iris, }
       \AttributeTok{cex =}\NormalTok{ (lof.pca}\DecValTok{{-}1}\NormalTok{)}\SpecialCharTok{*}\FloatTok{1.5}\NormalTok{,      }\CommentTok{\# point size according to the LOF score}
       \AttributeTok{pch =} \DecValTok{21}\NormalTok{, }
       \AttributeTok{col =} \StringTok{"purple"}\NormalTok{)}
\FunctionTok{text}\NormalTok{(pca.iris[lof.pca }\SpecialCharTok{\textgreater{}} \FloatTok{1.8}\NormalTok{,], }
     \AttributeTok{labels =} \FunctionTok{round}\NormalTok{(lof.pca, }\DecValTok{1}\NormalTok{)[lof.pca }\SpecialCharTok{\textgreater{}} \FloatTok{1.8}\NormalTok{], }
     \AttributeTok{pos =} \DecValTok{1}\NormalTok{,   }\CommentTok{\# 1, 2, 3 and 4 =\textgreater{} below, left , above, and right }
     \AttributeTok{cex =} \FloatTok{0.7}\NormalTok{,}
     \AttributeTok{col =} \StringTok{"blue"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics{STA551EB_files/figure-latex/unnamed-chunk-240-1} 

}

\caption{LOF scores based on the first 2 principal components.}\label{fig:unnamed-chunk-240}
\end{figure}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{plot}\NormalTok{(lof.pca, lof.orig, }\AttributeTok{pch =}\StringTok{"x"}\NormalTok{, }
     \AttributeTok{main =} \StringTok{"LOF Scores Comparison: PCA vs Original Variable"}\NormalTok{,}
     \AttributeTok{xlab =} \StringTok{"PCA LOF Score"}\NormalTok{,}
     \AttributeTok{ylab =} \StringTok{"Original LOF Score"}\NormalTok{,}
     \AttributeTok{cex =} \FloatTok{0.6}\NormalTok{)}
\FunctionTok{points}\NormalTok{(lof.pca[lof.pca }\SpecialCharTok{\textgreater{}} \FloatTok{1.8}\NormalTok{], lof.orig[lof.pca }\SpecialCharTok{\textgreater{}} \FloatTok{1.8}\NormalTok{],}
       \AttributeTok{pch =} \DecValTok{21}\NormalTok{, }
       \AttributeTok{cex =}\NormalTok{ lof.pca}\SpecialCharTok{*}\FloatTok{1.5}\NormalTok{,}
       \AttributeTok{col =} \StringTok{"purple"}\NormalTok{)}
\FunctionTok{text}\NormalTok{(lof.pca[lof.pca }\SpecialCharTok{\textgreater{}} \FloatTok{1.8}\NormalTok{], lof.orig[lof.pca }\SpecialCharTok{\textgreater{}} \FloatTok{1.8}\NormalTok{], }
     \AttributeTok{labels =} \FunctionTok{round}\NormalTok{(lof.pca, }\DecValTok{1}\NormalTok{)[lof.pca }\SpecialCharTok{\textgreater{}} \FloatTok{1.8}\NormalTok{], }
     \AttributeTok{pos =} \DecValTok{1}\NormalTok{,   }\CommentTok{\# 1, 2, 3 and 4 =\textgreater{} below, left , above, and right }
     \AttributeTok{cex =} \FloatTok{0.7}\NormalTok{,}
     \AttributeTok{col =} \StringTok{"blue"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics{STA551EB_files/figure-latex/unnamed-chunk-241-1} 

}

\caption{Scatter plot of LOF scores based on the original variables and the first 2 PCs.}\label{fig:unnamed-chunk-241}
\end{figure}

The above two figures show that the LOF method identifies the same outliers based on the original four variables (in the 4-dimensional feature space) and the first two PCs (2-dimensional space). No variable scaling was used in the PCA.

\hypertarget{one-class-svm-novelty-detection}{%
\section{One-class SVM: Novelty Detection}\label{one-class-svm-novelty-detection}}

Many machine learning models throw inaccuracies in the modeling of outlier elements. So it becomes a most basic requirement for us to determine if the new observation comes under the same existing distribution or if the new observation should be determined as different. In other words, there is only one class in the practical problem. The methods introduced in the following subsections are based on the non-linear kernel function. We will not introduce the mathematics behind these methods. But I will give a brief description of how the kernel function works.

\hypertarget{kernel-function}{%
\subsection{Kernel Function}\label{kernel-function}}

A kernel function is a weighing function. It defines an inner product (dot product) based on transformed feature variables. It can map a lower-dimensional nonlinear classification problem to a higher-dimensional space to obtain a linear classification problem.

\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{img09/w09-kernel-function} 

}

\caption{Kernel function.}\label{fig:unnamed-chunk-242}
\end{figure}

There are different kernel functions used in various applications. The commonly used kernel functions are linear, quadratic, exponential, sigmoid, Gaussian radial-based function (RBF), etc.

\hypertarget{description-of-support-vector-data-description-svdd}{%
\subsection{Description of Support Vector Data Description (SVDD)}\label{description-of-support-vector-data-description-svdd}}

Support Vector Data Description (SVDD, developed by Tax and Duin in 2004) is a one-class classification technique that is useful in applications where data that belong to one class are abundant but data about any other class are scarce or missing. SVDD can

\begin{itemize}
\tightlist
\item
  Identifies minimum radius hyper-sphere around ``normal'' data
\item
  Works on multivariate data
\item
  Does not require the assumption of normality
\item
  Fits flexible surfaces using a kernel function
\item
  Minimizes the chance of accepting outliers
\end{itemize}

It creates a minimum-radius hyper-sphere around the training data set and scores new observations by calculating the distance to the hyper-sphere center. Observations with distances greater than the minimum radius are flagged as anomalies. It has been used in the areas such as fraud detection, equipment health monitoring, and process control where the majority of the data belong to one class.

SVDD's foundation is built based on the primal-dual algorithms (a method for solving linear programs inspired by the Ford--Fulkerson). It uses a kernel-based approach to define a score function for each observation and obtain a decision boundary on whether new incoming data is an outlier.

To avoid mathematical expression, the following figure illustrates the

\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{img09/w09-SVDD} 

}

\caption{Illustration of SVDD.}\label{fig:unnamed-chunk-243}
\end{figure}

\hypertarget{one-class-support-vector-machine-oc-svm}{%
\subsection{One-Class Support Vector Machine (OC-SVM)}\label{one-class-support-vector-machine-oc-svm}}

The OC-SVM formulation is equivalent to the SVDD formulation when the \textbf{RBF kernel} function is used.

In this subsection, we use the R function \textbf{svm()} in package \textbf{\{e1071\}} to perform one-class support vector machine analysis using the iris data. We need to define a training data set. For simplicity, we use all setosa records to train the OC-SVM and then use the entire iris data to calculate the accuracy metrics.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{data}\NormalTok{(iris)}
\NormalTok{df }\OtherTok{\textless{}{-}}\NormalTok{ iris}
\DocumentationTok{\#\#}
\NormalTok{df }\OtherTok{\textless{}{-}} \FunctionTok{subset}\NormalTok{(df ,  Species}\SpecialCharTok{==}\StringTok{\textquotesingle{}setosa\textquotesingle{}}\NormalTok{)          }\CommentTok{\# choose only one of the classes}
\NormalTok{x }\OtherTok{\textless{}{-}} \FunctionTok{subset}\NormalTok{(df, }\AttributeTok{select =} \SpecialCharTok{{-}}\NormalTok{Species)             }\CommentTok{\# make x variables}
\NormalTok{y }\OtherTok{\textless{}{-}}\NormalTok{ df}\SpecialCharTok{$}\NormalTok{Species                                }\CommentTok{\# make y variable(dependent)}
\NormalTok{model }\OtherTok{\textless{}{-}} \FunctionTok{svm}\NormalTok{(x, y, }\AttributeTok{type =}\StringTok{\textquotesingle{}one{-}classification\textquotesingle{}}\NormalTok{) }\CommentTok{\# train an one{-}classification model }
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Warning in Ops.factor(yorig, ret$fitted): '-' not meaningful for factors
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# print(model)}
\FunctionTok{summary}\NormalTok{(model) }\CommentTok{\#print summary}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## Call:
## svm.default(x = x, y = y, type = "one-classification")
## 
## 
## Parameters:
##    SVM-Type:  one-classification 
##  SVM-Kernel:  radial 
##       gamma:  0.25 
##          nu:  0.5 
## 
## Number of Support Vectors:  27
## 
## 
## 
## 
## Number of Classes: 1
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Test on the whole set}
\NormalTok{pred }\OtherTok{\textless{}{-}} \FunctionTok{predict}\NormalTok{(model, }\FunctionTok{subset}\NormalTok{(iris, }\AttributeTok{select =} \SpecialCharTok{{-}}\NormalTok{Species)) }\CommentTok{\#create predictions}
\NormalTok{actual }\OtherTok{=}\NormalTok{ (iris[,}\DecValTok{5}\NormalTok{]}\SpecialCharTok{==} \StringTok{\textquotesingle{}setosa\textquotesingle{}}\NormalTok{)   }\CommentTok{\# actual labels}
\NormalTok{confusion.matrix }\OtherTok{=} \FunctionTok{table}\NormalTok{(pred, actual)}
\NormalTok{confusion.matrix}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##        actual
## pred    FALSE TRUE
##   FALSE   100   25
##   TRUE      0   25
\end{verbatim}

\textbf{Question}: Can we mimic the case study in LOF to use the first two principal components to perform OC-SVM?

  \bibliography{book.bib,packages.bib}

\end{document}
