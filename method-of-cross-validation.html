<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Topic 9 Method of Cross-Validation | STA551 E-Pack: Foundations of Data Science</title>
  <meta name="description" content="The output format for this example is bookdown::gitbook." />
  <meta name="generator" content="bookdown 0.35 and GitBook 2.6.7" />

  <meta property="og:title" content="Topic 9 Method of Cross-Validation | STA551 E-Pack: Foundations of Data Science" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="The output format for this example is bookdown::gitbook." />
  <meta name="github-repo" content="rstudio/STA551EB" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Topic 9 Method of Cross-Validation | STA551 E-Pack: Foundations of Data Science" />
  
  <meta name="twitter:description" content="The output format for this example is bookdown::gitbook." />
  

<meta name="author" content="Cheng Peng" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="basics-of-bootstrap-method.html"/>
<link rel="next" href="perfomance-measures-of-algorithms-and-models.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<center><li><a href="./"><font color = "darkred"><b>STA 551 E-Pack: Foundations of Data Science</b></font></a><br><font color = "navy">Cheng Peng</font></li></center>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Introduction</a>
<ul>
<li class="chapter" data-level="1.1" data-path="index.html"><a href="index.html#the-origin-of-data-science"><i class="fa fa-check"></i><b>1.1</b> The Origin of Data Science</a></li>
<li class="chapter" data-level="1.2" data-path="index.html"><a href="index.html#the-coverage-of-the-first-data-science-course"><i class="fa fa-check"></i><b>1.2</b> The Coverage of the First Data Science Course</a></li>
<li class="chapter" data-level="1.3" data-path="index.html"><a href="index.html#tentative-topics"><i class="fa fa-check"></i><b>1.3</b> Tentative Topics</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="data-science---a-big-picture.html"><a href="data-science---a-big-picture.html"><i class="fa fa-check"></i><b>2</b> Data Science - A Big Picture</a>
<ul>
<li class="chapter" data-level="2.1" data-path="data-science---a-big-picture.html"><a href="data-science---a-big-picture.html#data-science-process"><i class="fa fa-check"></i><b>2.1</b> Data Science Process</a>
<ul>
<li class="chapter" data-level="2.1.1" data-path="data-science---a-big-picture.html"><a href="data-science---a-big-picture.html#what-is-data"><i class="fa fa-check"></i><b>2.1.1</b> What is Data?</a></li>
<li class="chapter" data-level="2.1.2" data-path="data-science---a-big-picture.html"><a href="data-science---a-big-picture.html#data-storage-and-retrieval"><i class="fa fa-check"></i><b>2.1.2</b> Data Storage and Retrieval</a></li>
<li class="chapter" data-level="2.1.3" data-path="data-science---a-big-picture.html"><a href="data-science---a-big-picture.html#different-da-roles-in-industry"><i class="fa fa-check"></i><b>2.1.3</b> Different DA Roles in Industry</a></li>
<li class="chapter" data-level="2.1.4" data-path="data-science---a-big-picture.html"><a href="data-science---a-big-picture.html#cloud-computing"><i class="fa fa-check"></i><b>2.1.4</b> Cloud Computing</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="data-science---a-big-picture.html"><a href="data-science---a-big-picture.html#from-business-questions-to-analytic-question"><i class="fa fa-check"></i><b>2.2</b> From Business Questions to Analytic Question</a>
<ul>
<li class="chapter" data-level="2.2.1" data-path="data-science---a-big-picture.html"><a href="data-science---a-big-picture.html#business-goal"><i class="fa fa-check"></i><b>2.2.1</b> Business Goal</a></li>
<li class="chapter" data-level="2.2.2" data-path="data-science---a-big-picture.html"><a href="data-science---a-big-picture.html#analytic-question"><i class="fa fa-check"></i><b>2.2.2</b> Analytic Question</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="data-science---a-big-picture.html"><a href="data-science---a-big-picture.html#concepts-of-relational-databases-and-sql"><i class="fa fa-check"></i><b>2.3</b> Concepts of Relational Databases and SQL</a>
<ul>
<li class="chapter" data-level="2.3.1" data-path="data-science---a-big-picture.html"><a href="data-science---a-big-picture.html#what-is-database"><i class="fa fa-check"></i><b>2.3.1</b> What is Database?</a></li>
<li class="chapter" data-level="2.3.2" data-path="data-science---a-big-picture.html"><a href="data-science---a-big-picture.html#what-is-a-data-warehouse"><i class="fa fa-check"></i><b>2.3.2</b> What is a Data Warehouse?</a></li>
<li class="chapter" data-level="2.3.3" data-path="data-science---a-big-picture.html"><a href="data-science---a-big-picture.html#difference-between-database-and-data-warehouse"><i class="fa fa-check"></i><b>2.3.3</b> Difference between Database and Data Warehouse</a></li>
<li class="chapter" data-level="2.3.4" data-path="data-science---a-big-picture.html"><a href="data-science---a-big-picture.html#database-management-system-dbms"><i class="fa fa-check"></i><b>2.3.4</b> Database Management System (DBMS)</a></li>
<li class="chapter" data-level="2.3.5" data-path="data-science---a-big-picture.html"><a href="data-science---a-big-picture.html#some-definitions-and-notations-of-relational-tables"><i class="fa fa-check"></i><b>2.3.5</b> Some Definitions and Notations of Relational Tables</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="running-sql-in-sas-and-r.html"><a href="running-sql-in-sas-and-r.html"><i class="fa fa-check"></i><b>3</b> Running SQL in SAS and R</a>
<ul>
<li class="chapter" data-level="3.1" data-path="running-sql-in-sas-and-r.html"><a href="running-sql-in-sas-and-r.html#running-sql-in-sas"><i class="fa fa-check"></i><b>3.1</b> Running SQL in SAS</a>
<ul>
<li class="chapter" data-level="3.1.1" data-path="running-sql-in-sas-and-r.html"><a href="running-sql-in-sas-and-r.html#loading-data"><i class="fa fa-check"></i><b>3.1.1</b> Loading Data</a></li>
<li class="chapter" data-level="3.1.2" data-path="running-sql-in-sas-and-r.html"><a href="running-sql-in-sas-and-r.html#basic-sql-syntax-and-clauses"><i class="fa fa-check"></i><b>3.1.2</b> Basic SQL Syntax and Clauses</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="running-sql-in-sas-and-r.html"><a href="running-sql-in-sas-and-r.html#running-sas-in-r"><i class="fa fa-check"></i><b>3.2</b> Running SAS in R</a>
<ul>
<li class="chapter" data-level="3.2.1" data-path="running-sql-in-sas-and-r.html"><a href="running-sql-in-sas-and-r.html#connect-r-to-existing-database"><i class="fa fa-check"></i><b>3.2.1</b> Connect R to Existing Database</a></li>
<li class="chapter" data-level="3.2.2" data-path="running-sql-in-sas-and-r.html"><a href="running-sql-in-sas-and-r.html#create-sqlite-database-with-r"><i class="fa fa-check"></i><b>3.2.2</b> Create SQLite Database with R</a></li>
<li class="chapter" data-level="3.2.3" data-path="running-sql-in-sas-and-r.html"><a href="running-sql-in-sas-and-r.html#running-sql-queries-in-r-code-chunks"><i class="fa fa-check"></i><b>3.2.3</b> Running SQL Queries in R Code chunks</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="exploratory-data-analysis-eda.html"><a href="exploratory-data-analysis-eda.html"><i class="fa fa-check"></i><b>4</b> Exploratory Data Analysis (EDA)</a>
<ul>
<li class="chapter" data-level="4.1" data-path="exploratory-data-analysis-eda.html"><a href="exploratory-data-analysis-eda.html#tools-of-eda-and-applications"><i class="fa fa-check"></i><b>4.1</b> Tools of EDA and Applications</a>
<ul>
<li class="chapter" data-level="4.1.1" data-path="exploratory-data-analysis-eda.html"><a href="exploratory-data-analysis-eda.html#descriptive-statistics-approach"><i class="fa fa-check"></i><b>4.1.1</b> Descriptive Statistics Approach</a></li>
<li class="chapter" data-level="4.1.2" data-path="exploratory-data-analysis-eda.html"><a href="exploratory-data-analysis-eda.html#graphical-approach"><i class="fa fa-check"></i><b>4.1.2</b> Graphical Approach</a></li>
<li class="chapter" data-level="4.1.3" data-path="exploratory-data-analysis-eda.html"><a href="exploratory-data-analysis-eda.html#algorithm-based-method"><i class="fa fa-check"></i><b>4.1.3</b> Algorithm-based Method</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="exploratory-data-analysis-eda.html"><a href="exploratory-data-analysis-eda.html#visual-techniques-of-eda"><i class="fa fa-check"></i><b>4.2</b> Visual Techniques of EDA</a>
<ul>
<li class="chapter" data-level="4.2.1" data-path="exploratory-data-analysis-eda.html"><a href="exploratory-data-analysis-eda.html#univariate-eda"><i class="fa fa-check"></i><b>4.2.1</b> Univariate EDA</a></li>
<li class="chapter" data-level="4.2.2" data-path="exploratory-data-analysis-eda.html"><a href="exploratory-data-analysis-eda.html#two-variables"><i class="fa fa-check"></i><b>4.2.2</b> Two Variables</a></li>
<li class="chapter" data-level="4.2.3" data-path="exploratory-data-analysis-eda.html"><a href="exploratory-data-analysis-eda.html#three-or-more-variables"><i class="fa fa-check"></i><b>4.2.3</b> Three or More Variables</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="exploratory-data-analysis-eda.html"><a href="exploratory-data-analysis-eda.html#roles-of-visualization-in-eda"><i class="fa fa-check"></i><b>4.3</b> Roles of Visualization in EDA</a>
<ul>
<li class="chapter" data-level="4.3.1" data-path="exploratory-data-analysis-eda.html"><a href="exploratory-data-analysis-eda.html#data-visualization"><i class="fa fa-check"></i><b>4.3.1</b> Data Visualization</a></li>
<li class="chapter" data-level="4.3.2" data-path="exploratory-data-analysis-eda.html"><a href="exploratory-data-analysis-eda.html#visual-analytics"><i class="fa fa-check"></i><b>4.3.2</b> Visual Analytics</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="eda-for-feature-engineering.html"><a href="eda-for-feature-engineering.html"><i class="fa fa-check"></i><b>5</b> EDA for Feature Engineering</a>
<ul>
<li class="chapter" data-level="5.1" data-path="eda-for-feature-engineering.html"><a href="eda-for-feature-engineering.html#description-of-data"><i class="fa fa-check"></i><b>5.1</b> Description of Data</a></li>
<li class="chapter" data-level="5.2" data-path="eda-for-feature-engineering.html"><a href="eda-for-feature-engineering.html#eda-for-feature-engineering-1"><i class="fa fa-check"></i><b>5.2</b> EDA for Feature Engineering</a>
<ul>
<li class="chapter" data-level="5.2.1" data-path="eda-for-feature-engineering.html"><a href="eda-for-feature-engineering.html#missing-values---imputation"><i class="fa fa-check"></i><b>5.2.1</b> Missing Values - Imputation</a></li>
<li class="chapter" data-level="5.2.2" data-path="eda-for-feature-engineering.html"><a href="eda-for-feature-engineering.html#assess-distributions"><i class="fa fa-check"></i><b>5.2.2</b> Assess Distributions</a></li>
<li class="chapter" data-level="5.2.3" data-path="eda-for-feature-engineering.html"><a href="eda-for-feature-engineering.html#pairwise-association"><i class="fa fa-check"></i><b>5.2.3</b> Pairwise Association</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="eda-for-feature-engineering.html"><a href="eda-for-feature-engineering.html#concluding-remarks"><i class="fa fa-check"></i><b>5.3</b> Concluding Remarks</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html"><i class="fa fa-check"></i><b>6</b> Multiple Linear Regression</a>
<ul>
<li class="chapter" data-level="6.1" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#the-practical-question"><i class="fa fa-check"></i><b>6.1</b> The Practical Question</a></li>
<li class="chapter" data-level="6.2" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#the-process-of-building-a-multiple-linear-regression-model"><i class="fa fa-check"></i><b>6.2</b> The Process of Building A Multiple Linear Regression Model</a>
<ul>
<li class="chapter" data-level="6.2.1" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#assumptions-of-mlr"><i class="fa fa-check"></i><b>6.2.1</b> Assumptions of MLR</a></li>
<li class="chapter" data-level="6.2.2" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#the-structure-of-mlr"><i class="fa fa-check"></i><b>6.2.2</b> The Structure of MLR</a></li>
<li class="chapter" data-level="6.2.3" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#more-on-model-specifications"><i class="fa fa-check"></i><b>6.2.3</b> More on Model Specifications</a></li>
<li class="chapter" data-level="6.2.4" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#estimation-of-regression-coefficients"><i class="fa fa-check"></i><b>6.2.4</b> Estimation of Regression Coefficients</a></li>
<li class="chapter" data-level="6.2.5" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#model-diagnostics"><i class="fa fa-check"></i><b>6.2.5</b> Model Diagnostics</a></li>
<li class="chapter" data-level="6.2.6" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#goodness-of-fit-and-variable-selection"><i class="fa fa-check"></i><b>6.2.6</b> Goodness-of-fit and Variable Selection</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#case-study-1"><i class="fa fa-check"></i><b>6.3</b> Case Study 1</a></li>
<li class="chapter" data-level="6.4" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#case-study-2"><i class="fa fa-check"></i><b>6.4</b> Case Study 2</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="logistic-regression-models.html"><a href="logistic-regression-models.html"><i class="fa fa-check"></i><b>7</b> Logistic Regression Models</a>
<ul>
<li class="chapter" data-level="7.1" data-path="logistic-regression-models.html"><a href="logistic-regression-models.html#motivational-example-and-practical-question"><i class="fa fa-check"></i><b>7.1</b> Motivational Example and Practical Question</a></li>
<li class="chapter" data-level="7.2" data-path="logistic-regression-models.html"><a href="logistic-regression-models.html#logistic-regression-models-and-applications"><i class="fa fa-check"></i><b>7.2</b> Logistic Regression Models and Applications</a>
<ul>
<li class="chapter" data-level="7.2.1" data-path="logistic-regression-models.html"><a href="logistic-regression-models.html#the-structure-of-the-logistic-regression-model"><i class="fa fa-check"></i><b>7.2.1</b> The Structure of the Logistic Regression Model</a></li>
<li class="chapter" data-level="7.2.2" data-path="logistic-regression-models.html"><a href="logistic-regression-models.html#assumptions-and-diagnostics"><i class="fa fa-check"></i><b>7.2.2</b> Assumptions and Diagnostics</a></li>
<li class="chapter" data-level="7.2.3" data-path="logistic-regression-models.html"><a href="logistic-regression-models.html#coefficient-estimation-and-interpretation"><i class="fa fa-check"></i><b>7.2.3</b> Coefficient Estimation and Interpretation</a></li>
<li class="chapter" data-level="7.2.4" data-path="logistic-regression-models.html"><a href="logistic-regression-models.html#use-of-glm-and-annotations"><i class="fa fa-check"></i><b>7.2.4</b> Use of <strong>glm()</strong> and Annotations</a></li>
<li class="chapter" data-level="7.2.5" data-path="logistic-regression-models.html"><a href="logistic-regression-models.html#applications-of-logistic-regression-models"><i class="fa fa-check"></i><b>7.2.5</b> Applications of Logistic Regression Models</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="logistic-regression-models.html"><a href="logistic-regression-models.html#case-studies"><i class="fa fa-check"></i><b>7.3</b> Case Studies</a>
<ul>
<li class="chapter" data-level="7.3.1" data-path="logistic-regression-models.html"><a href="logistic-regression-models.html#the-simple-logistic-regression-model"><i class="fa fa-check"></i><b>7.3.1</b> The simple logistic regression model</a></li>
<li class="chapter" data-level="7.3.2" data-path="logistic-regression-models.html"><a href="logistic-regression-models.html#multiple-logistic-regression-model"><i class="fa fa-check"></i><b>7.3.2</b> Multiple Logistic Regression Model</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="8" data-path="basics-of-bootstrap-method.html"><a href="basics-of-bootstrap-method.html"><i class="fa fa-check"></i><b>8</b> Basics of Bootstrap Method</a>
<ul>
<li class="chapter" data-level="8.1" data-path="basics-of-bootstrap-method.html"><a href="basics-of-bootstrap-method.html#basic-idea-of-bootstrap-method."><i class="fa fa-check"></i><b>8.1</b> Basic Idea of Bootstrap Method.</a>
<ul>
<li class="chapter" data-level="8.1.1" data-path="basics-of-bootstrap-method.html"><a href="basics-of-bootstrap-method.html#random-sample-from-population"><i class="fa fa-check"></i><b>8.1.1</b> Random Sample from Population</a></li>
<li class="chapter" data-level="8.1.2" data-path="basics-of-bootstrap-method.html"><a href="basics-of-bootstrap-method.html#bootstrap-sampling-and-bootstrap-sampling-distribution"><i class="fa fa-check"></i><b>8.1.2</b> Bootstrap Sampling and Bootstrap Sampling Distribution</a></li>
<li class="chapter" data-level="8.1.3" data-path="basics-of-bootstrap-method.html"><a href="basics-of-bootstrap-method.html#relationship-between-two-estimated-sampling-distributions"><i class="fa fa-check"></i><b>8.1.3</b> Relationship between Two Estimated Sampling Distributions</a></li>
</ul></li>
<li class="chapter" data-level="8.2" data-path="basics-of-bootstrap-method.html"><a href="basics-of-bootstrap-method.html#bootstrap-confidence-intervals"><i class="fa fa-check"></i><b>8.2</b> Bootstrap Confidence Intervals</a></li>
<li class="chapter" data-level="8.3" data-path="basics-of-bootstrap-method.html"><a href="basics-of-bootstrap-method.html#bootstrap-confidence-interval-of-correlation-coefficient"><i class="fa fa-check"></i><b>8.3</b> Bootstrap Confidence Interval of Correlation Coefficient</a>
<ul>
<li class="chapter" data-level="8.3.1" data-path="basics-of-bootstrap-method.html"><a href="basics-of-bootstrap-method.html#bootstrapping-data-set"><i class="fa fa-check"></i><b>8.3.1</b> Bootstrapping Data Set</a></li>
<li class="chapter" data-level="8.3.2" data-path="basics-of-bootstrap-method.html"><a href="basics-of-bootstrap-method.html#confidence-interval-of-coefficient-correlation"><i class="fa fa-check"></i><b>8.3.2</b> Confidence Interval of Coefficient Correlation</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="9" data-path="method-of-cross-validation.html"><a href="method-of-cross-validation.html"><i class="fa fa-check"></i><b>9</b> Method of Cross-Validation</a>
<ul>
<li class="chapter" data-level="9.1" data-path="method-of-cross-validation.html"><a href="method-of-cross-validation.html#regression-models"><i class="fa fa-check"></i><b>9.1</b> Regression Models</a>
<ul>
<li class="chapter" data-level="9.1.1" data-path="method-of-cross-validation.html"><a href="method-of-cross-validation.html#linear-regression-model"><i class="fa fa-check"></i><b>9.1.1</b> Linear Regression Model</a></li>
<li class="chapter" data-level="9.1.2" data-path="method-of-cross-validation.html"><a href="method-of-cross-validation.html#logistic-regression-model"><i class="fa fa-check"></i><b>9.1.2</b> Logistic Regression Model</a></li>
<li class="chapter" data-level="9.1.3" data-path="method-of-cross-validation.html"><a href="method-of-cross-validation.html#inference-about-association-and-prediction"><i class="fa fa-check"></i><b>9.1.3</b> Inference about Association and Prediction</a></li>
</ul></li>
<li class="chapter" data-level="9.2" data-path="method-of-cross-validation.html"><a href="method-of-cross-validation.html#data-splitting-methods"><i class="fa fa-check"></i><b>9.2</b> Data Splitting Methods</a>
<ul>
<li class="chapter" data-level="9.2.1" data-path="method-of-cross-validation.html"><a href="method-of-cross-validation.html#training-data"><i class="fa fa-check"></i><b>9.2.1</b> Training Data</a></li>
<li class="chapter" data-level="9.2.2" data-path="method-of-cross-validation.html"><a href="method-of-cross-validation.html#validation-data"><i class="fa fa-check"></i><b>9.2.2</b> Validation Data</a></li>
<li class="chapter" data-level="9.2.3" data-path="method-of-cross-validation.html"><a href="method-of-cross-validation.html#test-data"><i class="fa fa-check"></i><b>9.2.3</b> Test data</a></li>
</ul></li>
<li class="chapter" data-level="9.3" data-path="method-of-cross-validation.html"><a href="method-of-cross-validation.html#cross-validation"><i class="fa fa-check"></i><b>9.3</b> Cross-validation</a>
<ul>
<li class="chapter" data-level="9.3.1" data-path="method-of-cross-validation.html"><a href="method-of-cross-validation.html#k-fold-cross-validation"><i class="fa fa-check"></i><b>9.3.1</b> K-fold Cross-validation</a></li>
<li class="chapter" data-level="9.3.2" data-path="method-of-cross-validation.html"><a href="method-of-cross-validation.html#other-cross-validation-methods"><i class="fa fa-check"></i><b>9.3.2</b> Other Cross-Validation Methods</a></li>
</ul></li>
<li class="chapter" data-level="9.4" data-path="method-of-cross-validation.html"><a href="method-of-cross-validation.html#case-study-using-fraud-data"><i class="fa fa-check"></i><b>9.4</b> Case Study Using Fraud Data</a>
<ul>
<li class="chapter" data-level="9.4.1" data-path="method-of-cross-validation.html"><a href="method-of-cross-validation.html#data-partition"><i class="fa fa-check"></i><b>9.4.1</b> Data Partition</a></li>
<li class="chapter" data-level="9.4.2" data-path="method-of-cross-validation.html"><a href="method-of-cross-validation.html#finding-optimal-cut-off-probability"><i class="fa fa-check"></i><b>9.4.2</b> Finding Optimal Cut-off Probability</a></li>
<li class="chapter" data-level="9.4.3" data-path="method-of-cross-validation.html"><a href="method-of-cross-validation.html#reporting-test"><i class="fa fa-check"></i><b>9.4.3</b> Reporting Test</a></li>
</ul></li>
<li class="chapter" data-level="9.5" data-path="method-of-cross-validation.html"><a href="method-of-cross-validation.html#concluding-remarks-1"><i class="fa fa-check"></i><b>9.5</b> Concluding Remarks</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="perfomance-measures-of-algorithms-and-models.html"><a href="perfomance-measures-of-algorithms-and-models.html"><i class="fa fa-check"></i><b>10</b> Perfomance Measures of Algorithms and Models</a></li>
<li class="chapter" data-level="11" data-path="classification-performance-metrics.html"><a href="classification-performance-metrics.html"><i class="fa fa-check"></i><b>11</b> Classification Performance Metrics</a>
<ul>
<li class="chapter" data-level="11.1" data-path="classification-performance-metrics.html"><a href="classification-performance-metrics.html#confusion-matrix-for-binary-decision"><i class="fa fa-check"></i><b>11.1</b> Confusion Matrix for Binary Decision</a></li>
<li class="chapter" data-level="11.2" data-path="classification-performance-metrics.html"><a href="classification-performance-metrics.html#local-performance-measures-for-model-development"><i class="fa fa-check"></i><b>11.2</b> Local Performance Measures (for Model Development)</a></li>
<li class="chapter" data-level="11.3" data-path="classification-performance-metrics.html"><a href="classification-performance-metrics.html#global-performance-measures"><i class="fa fa-check"></i><b>11.3</b> Global Performance Measures</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="case-study---logistic-regression-model-with-the-fraud-data.html"><a href="case-study---logistic-regression-model-with-the-fraud-data.html"><i class="fa fa-check"></i><b>12</b> Case Study - Logistic regression model with the fraud data</a>
<ul>
<li class="chapter" data-level="12.1" data-path="case-study---logistic-regression-model-with-the-fraud-data.html"><a href="case-study---logistic-regression-model-with-the-fraud-data.html#local-performance-meaures"><i class="fa fa-check"></i><b>12.1</b> Local Performance Meaures</a></li>
<li class="chapter" data-level="12.2" data-path="case-study---logistic-regression-model-with-the-fraud-data.html"><a href="case-study---logistic-regression-model-with-the-fraud-data.html#global-measure-roc-and-auc"><i class="fa fa-check"></i><b>12.2</b> Global Measure: ROC and AUC</a></li>
</ul></li>
<li class="chapter" data-level="13" data-path="from-statistics-models-to-machine-learning-algorithms.html"><a href="from-statistics-models-to-machine-learning-algorithms.html"><i class="fa fa-check"></i><b>13</b> From Statistics Models to Machine Learning Algorithms</a>
<ul>
<li class="chapter" data-level="13.1" data-path="from-statistics-models-to-machine-learning-algorithms.html"><a href="from-statistics-models-to-machine-learning-algorithms.html#some-technical-terms-and-ml-types"><i class="fa fa-check"></i><b>13.1</b> Some Technical Terms and ML Types</a>
<ul>
<li class="chapter" data-level="13.1.1" data-path="from-statistics-models-to-machine-learning-algorithms.html"><a href="from-statistics-models-to-machine-learning-algorithms.html#machine-learning-problems-and-jargon"><i class="fa fa-check"></i><b>13.1.1</b> Machine Learning Problems and Jargon</a></li>
<li class="chapter" data-level="13.1.2" data-path="from-statistics-models-to-machine-learning-algorithms.html"><a href="from-statistics-models-to-machine-learning-algorithms.html#types-of-machine-learning-problems"><i class="fa fa-check"></i><b>13.1.2</b> Types of Machine Learning Problems</a></li>
</ul></li>
<li class="chapter" data-level="13.2" data-path="from-statistics-models-to-machine-learning-algorithms.html"><a href="from-statistics-models-to-machine-learning-algorithms.html#categories-of-machine-learning"><i class="fa fa-check"></i><b>13.2</b> Categories of Machine Learning</a>
<ul>
<li class="chapter" data-level="13.2.1" data-path="from-statistics-models-to-machine-learning-algorithms.html"><a href="from-statistics-models-to-machine-learning-algorithms.html#supervised-learning"><i class="fa fa-check"></i><b>13.2.1</b> Supervised Learning</a></li>
<li class="chapter" data-level="13.2.2" data-path="from-statistics-models-to-machine-learning-algorithms.html"><a href="from-statistics-models-to-machine-learning-algorithms.html#unsupervised-learning"><i class="fa fa-check"></i><b>13.2.2</b> Unsupervised Learning</a></li>
<li class="chapter" data-level="13.2.3" data-path="from-statistics-models-to-machine-learning-algorithms.html"><a href="from-statistics-models-to-machine-learning-algorithms.html#semi-supervised-learning"><i class="fa fa-check"></i><b>13.2.3</b> Semi-Supervised Learning</a></li>
</ul></li>
<li class="chapter" data-level="13.3" data-path="from-statistics-models-to-machine-learning-algorithms.html"><a href="from-statistics-models-to-machine-learning-algorithms.html#from-statistics-to-machine-learning"><i class="fa fa-check"></i><b>13.3</b> From Statistics to Machine Learning</a>
<ul>
<li class="chapter" data-level="13.3.1" data-path="from-statistics-models-to-machine-learning-algorithms.html"><a href="from-statistics-models-to-machine-learning-algorithms.html#logistic-regression-model-revisited"><i class="fa fa-check"></i><b>13.3.1</b> Logistic Regression Model Revisited</a></li>
<li class="chapter" data-level="13.3.2" data-path="from-statistics-models-to-machine-learning-algorithms.html"><a href="from-statistics-models-to-machine-learning-algorithms.html#single-layer-neural-network---perceptron"><i class="fa fa-check"></i><b>13.3.2</b> Single Layer Neural Network - Perceptron</a></li>
<li class="chapter" data-level="13.3.3" data-path="from-statistics-models-to-machine-learning-algorithms.html"><a href="from-statistics-models-to-machine-learning-algorithms.html#multi-layer-perceptron"><i class="fa fa-check"></i><b>13.3.3</b> Multi-layer Perceptron</a></li>
<li class="chapter" data-level="13.3.4" data-path="from-statistics-models-to-machine-learning-algorithms.html"><a href="from-statistics-models-to-machine-learning-algorithms.html#commonly-used-activation-functions"><i class="fa fa-check"></i><b>13.3.4</b> Commonly Used Activation Functions</a></li>
<li class="chapter" data-level="13.3.5" data-path="from-statistics-models-to-machine-learning-algorithms.html"><a href="from-statistics-models-to-machine-learning-algorithms.html#algorithms-for-estimating-weights"><i class="fa fa-check"></i><b>13.3.5</b> Algorithms for Estimating Weights</a></li>
</ul></li>
<li class="chapter" data-level="13.4" data-path="from-statistics-models-to-machine-learning-algorithms.html"><a href="from-statistics-models-to-machine-learning-algorithms.html#implementing-nn-with-r"><i class="fa fa-check"></i><b>13.4</b> Implementing NN with R</a>
<ul>
<li class="chapter" data-level="13.4.1" data-path="from-statistics-models-to-machine-learning-algorithms.html"><a href="from-statistics-models-to-machine-learning-algorithms.html#syntax-of-neuralnet"><i class="fa fa-check"></i><b>13.4.1</b> Syntax of <code>neuralnet</code></a></li>
<li class="chapter" data-level="13.4.2" data-path="from-statistics-models-to-machine-learning-algorithms.html"><a href="from-statistics-models-to-machine-learning-algorithms.html#feature-conversion-for-neuralnet"><i class="fa fa-check"></i><b>13.4.2</b> Feature Conversion for <code>neuralnet</code></a></li>
<li class="chapter" data-level="13.4.3" data-path="from-statistics-models-to-machine-learning-algorithms.html"><a href="from-statistics-models-to-machine-learning-algorithms.html#numeric-feature-scaling"><i class="fa fa-check"></i><b>13.4.3</b> Numeric Feature Scaling</a></li>
<li class="chapter" data-level="13.4.4" data-path="from-statistics-models-to-machine-learning-algorithms.html"><a href="from-statistics-models-to-machine-learning-algorithms.html#extract-all-feature-names"><i class="fa fa-check"></i><b>13.4.4</b> Extract All Feature Names</a></li>
<li class="chapter" data-level="13.4.5" data-path="from-statistics-models-to-machine-learning-algorithms.html"><a href="from-statistics-models-to-machine-learning-algorithms.html#define-model-formula"><i class="fa fa-check"></i><b>13.4.5</b> Define Model Formula</a></li>
<li class="chapter" data-level="13.4.6" data-path="from-statistics-models-to-machine-learning-algorithms.html"><a href="from-statistics-models-to-machine-learning-algorithms.html#training-and-testing-nn-model"><i class="fa fa-check"></i><b>13.4.6</b> Training and Testing NN Model</a></li>
<li class="chapter" data-level="13.4.7" data-path="from-statistics-models-to-machine-learning-algorithms.html"><a href="from-statistics-models-to-machine-learning-algorithms.html#about-deep-learning"><i class="fa fa-check"></i><b>13.4.7</b> About Deep Learning</a></li>
</ul></li>
<li class="chapter" data-level="13.5" data-path="from-statistics-models-to-machine-learning-algorithms.html"><a href="from-statistics-models-to-machine-learning-algorithms.html#clustering-algorithms"><i class="fa fa-check"></i><b>13.5</b> Clustering Algorithms</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">STA551 E-Pack: Foundations of Data Science</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="method-of-cross-validation" class="section level1 hasAnchor" number="9">
<h1><span class="header-section-number">Topic 9</span> Method of Cross-Validation<a href="method-of-cross-validation.html#method-of-cross-validation" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>In classical statistical modeling, we select candidate models based on exploratory data analysis and visual analysis. The candidate models are then fit to the analytic data set. Since all models have some sort of assumptions (think about linear regression and logistic and Poisson regression models), and then carry out model diagnostic analyses to identify potential violations of the model assumption. Of cause, we assume that the data represent the population. This modeling process is depicted in the following diagram.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:unnamed-chunk-117"></span>
<img src="img06/w06-StatModelProcess.jpg" alt="Statistical modeling process." width="80%" />
<p class="caption">
Figure 9.1: Statistical modeling process.
</p>
</div>
<div id="regression-models" class="section level2 hasAnchor" number="9.1">
<h2><span class="header-section-number">9.1</span> Regression Models<a href="method-of-cross-validation.html#regression-models" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Recall the process of the following two representative statistical models</p>
<div id="linear-regression-model" class="section level3 hasAnchor" number="9.1.1">
<h3><span class="header-section-number">9.1.1</span> Linear Regression Model<a href="method-of-cross-validation.html#linear-regression-model" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>First, we look at the process of how to build a linear regression model for a data set.</p>
<ul>
<li><p><strong>Explicit and Implicit Assumptions</strong>. The following are some of the assumptions about a normal-based linear regression model:</p>
<ul>
<li>Predictor (feature) variables are (linearly or curve-linearly) correlated to the response variable (also called label in machine learning terms);</li>
<li>Predictor variables themselves are NOT highly linearly correlated (i.e., no serious collinearity);</li>
<li>The response variable is normally distributed;</li>
<li>The variance of the response variable is constant;</li>
<li>There are no outlier (influential) observations;</li>
<li>information on all relevant predictor variables is available in the data (i.e., no important predictor is missing);</li>
</ul></li>
<li><p><strong>Model Fitting (Parameter Estimating)</strong>. The least-square estimator (LSE), which is equivalent to the maximum likelihood estimator (MLE) when the response variable is assumed to be a normal distribution, can be used to estimate the regression coefficients.</p></li>
<li><p><strong>Model Selection and Diagnostics</strong>. Since several implicit and explicit assumptions have been assumed underlying the linear regression, different sets of diagnostic measures were developed to detect different potential violations of the model.</p>
<ul>
<li><b><em>global goodness-of-fit</em></b>: R<sup>2</sup>, AIC and SBC (requires normality assumption of the response variable), MSE, etc.</li>
<li><b><em>local goodness-of-fit/model selection</em></b>: R, F-test (need normality and equal variance of the response variable), t-test, likelihood ratio test, Mallow’s C<sub>p</sub>, etc.</li>
<li><b><em>normality</em></b>: QQ-plot and probability plot for a visual check, goodness-of-fit tests such as Anderson-Darling, Cramer-von Miss, Kolmogorov-Smirnov, Shapiro-Wilks, and Pearson’s chi-square tests, etc.</li>
<li><b><em>detecting outliers/influential observations</em></b>: leverage point-hat matrix, DFITT - defined based on leave-one-out resampling method, cook’s distance, (scaled) residual plots, etc.</li>
<li><b><em>verifying constant variance</em></b>: F-test (requires normality), Brown-Forsythe test (nonparametric), Breusch-Pagan Test (also nonparametric), Bartlett’s Test (requires normality), etc.</li>
<li><b><em>detecting collinearity</em></b>: Variance inflation factor (VIF) for data-based and structural collinearity.</li>
<li><b><em>detecting mission of determinant variable</em></b>:</li>
</ul></li>
</ul>
</div>
<div id="logistic-regression-model" class="section level3 hasAnchor" number="9.1.2">
<h3><span class="header-section-number">9.1.2</span> Logistic Regression Model<a href="method-of-cross-validation.html#logistic-regression-model" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>For the binary logistic model, we also follow the same steps to identify the final model. For example, the well-known binary logistic regression modeling follows similar steps:</p>
<ul>
<li><p><strong>Assumptions</strong>: Unlike the linear regression model which has a strong assumption of normality, the logistic regression model assumes the following</p>
<ul>
<li><b><em>binomial distribution</em></b>: The dependent variable is binary.</li>
<li><b><em>independence</em></b>: The logistic regression requires the observations to be independent of each other.</li>
<li><b><em>collinearity</em></b>: The logistic regression requires there to be little or no multicollinearity among the independent variables.</li>
<li><b><em>linearity</em></b>: The logistic regression assumes linearity of independent variables and the log odds of the event of interest.</li>
<li><b><em>large sample size</em></b>: The logistic regression typically requires a large sample size. A general guideline is that you need a minimum of 10 observations with the least frequent outcome for each independent variable in your model.</li>
<li><b><em>mis-specification</em></b>: No important variables are omitted. No extraneous variables are included.</li>
<li><b><em>measurement error</em></b>: The independent variables are measured without error.</li>
</ul></li>
<li><p><strong>Model Fitting</strong>: The coefficients of the logistic regression model are estimated using a maximum likelihood estimator. Note LSE cannot be estimated for the logistic regression model.</p></li>
<li><p><strong>Model Selection and Diagnostics</strong>: Unlike the normal linear regression model, there are a few diagnostic methods one can use in logistic regression models.</p>
<ul>
<li><b><em>misspecification</em></b>: link test (large sample test);</li>
<li><b><em>goodness-of-fit</em></b>: log-likelihood chi-square and pseudo-R-square; Hosmer-Lemeshow’s lack of fit test AIC and SBC.</li>
<li><b><em>multi-collinearity</em></b>: VIF</li>
<li><b><em>influential points</em></b>: Cook’s distance, DBETA, deviance residuals</li>
</ul></li>
</ul>
</div>
<div id="inference-about-association-and-prediction" class="section level3 hasAnchor" number="9.1.3">
<h3><span class="header-section-number">9.1.3</span> Inference about Association and Prediction<a href="method-of-cross-validation.html#inference-about-association-and-prediction" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>In classical statistics, most problems are related to the significance of the regression coefficients. The p-values associated with corresponding regression coefficients are calculated based on certain assumptions. Prediction intervals are also constructed based on certain assumptions.</p>
<p>We can also use resampling methods to relax the assumptions about the distribution of the response variable to make inferences about the regression coefficients and construct prediction intervals.</p>
<p>In practice, the prediction problems usually involve both classical statistical models and machine learning algorithms. The model selection process and performance evaluation in classical statistical modeling is dependent on model assumptions, while in machine learning algorithms, model selection and evaluation use data-driven methods that are also valid for modern statistical modeling.</p>
<p>In the next few sections, we will introduce the data-driven methods for machine learning algorithms and statistical models.</p>
</div>
</div>
<div id="data-splitting-methods" class="section level2 hasAnchor" number="9.2">
<h2><span class="header-section-number">9.2</span> Data Splitting Methods<a href="method-of-cross-validation.html#data-splitting-methods" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>When training and testing machine learning and statistical models without assuming strong assumptions (particularly the distributional information), we break down the data into three separate and distinct data sets: training data, validation data, and testing data. The basic idea is depicted in the following chart.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:unnamed-chunk-118"></span>
<img src="img06/w06-ThreeWaySplitting.jpg" alt="The-way data splitting method." width="80%" />
<p class="caption">
Figure 9.2: The-way data splitting method.
</p>
</div>
<div id="training-data" class="section level3 hasAnchor" number="9.2.1">
<h3><span class="header-section-number">9.2.1</span> Training Data<a href="method-of-cross-validation.html#training-data" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The training data set is the sample of data used to fit the model. In other words, the training data teaches the model how it’s supposed to learn and think. To do this, training data is often presented in pairs: predictor variables (feature variables) and a response variable (also called a label).</p>
<p>Training data is the first set of data the model/algorithm is exposed to. During each stage of training, the model will be fit to the training data and estimate the parameters (also called weight in some machine learning algorithms such as neural networks).</p>
<p>Because the training data set is used to estimate the parameters (i.e., teaching the algorithm), it requires a certain amount of information to make the algorithms and models reliable. It makes up between 60% and 80% of the total data.</p>
</div>
<div id="validation-data" class="section level3 hasAnchor" number="9.2.2">
<h3><span class="header-section-number">9.2.2</span> Validation Data<a href="method-of-cross-validation.html#validation-data" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The validation data set is a sample of data held back from training the model. This data set provides an unbiased evaluation of a model fit on the training data set while tuning model hyperparameters. In more basic terms, validation data is an unused portion of your training data and helps determine if the initial model is accurate.</p>
<p>A model <strong>hyperparameter</strong> is a configuration that is external to the model and whose value cannot be estimated from data.</p>
<ul>
<li>They are often used in processes to help estimate model parameters.</li>
<li>They are often specified by the practitioner.</li>
<li>They can often be set using heuristics.</li>
<li>They are often tuned for a given predictive modeling problem.</li>
</ul>
<p>We cannot know the best value for a model hyperparameter on a given problem. We may use rules of thumb, copy values used on other problems, or search for the best value by trial and error.</p>
<p>When a machine learning algorithm is tuned for a specific problem, such as the cut-off probability determination in the logistic prediction, then we are tuning the hyperparameters of the model or order to discover the parameters of the model that result in the most skillful predictions. The validation data helps tune a machine-learning model while checking for overfitting.</p>
<p>Overfitting happens when the model/algorithm is too closely fitted to the training data — producing results tied to the specifics of that first data set. After validation, the team will often return to the training data and run it again, making adjustments to values and parameters to improve the model.</p>
</div>
<div id="test-data" class="section level3 hasAnchor" number="9.2.3">
<h3><span class="header-section-number">9.2.3</span> Test data<a href="method-of-cross-validation.html#test-data" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The test data set is a sample of data used to provide an unbiased evaluation of a final model fit on the training data set or to test the model. Put more simply, test data is a set of <strong>unlabeled inputs</strong> (i.e., the response value is removed from the data) that test whether the model is producing the correct outputs in the real world.</p>
<p>The key difference between a validation data set and a test data set is that the validation data set is used during model configuration, while the test data set is reserved to evaluate the final model.</p>
<p>Test data is about 20% of the total data and should be completely separate from the training data — which our model should know very well by this point.</p>
<p>In summary, the following chart gives an example of three-way splitting data with a sample configuration of the sub-set sizes.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:unnamed-chunk-119"></span>
<img src="img06/w06-ThreeWaySplittingSize.jpg" alt="The-way data splitting method with an example of sub-sample sizes." width="80%" />
<p class="caption">
Figure 9.3: The-way data splitting method with an example of sub-sample sizes.
</p>
</div>
</div>
</div>
<div id="cross-validation" class="section level2 hasAnchor" number="9.3">
<h2><span class="header-section-number">9.3</span> Cross-validation<a href="method-of-cross-validation.html#cross-validation" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Partitioning the available data into three sets drastically reduces the number of samples that can be used for training the model, and the results can depend on a particular random choice for the pair of (train, validation) sets. To address this reliability and stability, a solution to this problem is a procedure called cross-validation (CV for short). A test set should still be held out for final evaluation, but the validation set is no longer needed when doing a CV.</p>
<div id="k-fold-cross-validation" class="section level3 hasAnchor" number="9.3.1">
<h3><span class="header-section-number">9.3.1</span> K-fold Cross-validation<a href="method-of-cross-validation.html#k-fold-cross-validation" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Many different cross-validation methods are defined based on the following basic k-fold CV, the <strong>training set</strong> is split into k smaller sets (other approaches are described below, but generally follow the same principles). The following is the procedure that uses 5 “folds”:</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:unnamed-chunk-120"></span>
<img src="img06/w06-5-fold-CV.jpg" alt="Figure 4.  Demonstration of a five-fold cross-validation." width="80%" />
<p class="caption">
Figure 9.4: Figure 4. Demonstration of a five-fold cross-validation.
</p>
</div>
<p>As an example, we explain the five-fold cross-validation for determining the optional cut-off probability in logistic prediction problems. Without loss of generality, we consider possible cut-off probabilities. For each cut-off probability, we calculate the prediction accuracy of the model based on the confusion matrix using the one-fold of validation data (see the following figure).</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:unnamed-chunk-121"></span>
<img src="img06/w06-CalculateConfusionMatrix.jpg" alt="Confusion matrix and accuracy metrics." width="80%" />
<p class="caption">
Figure 9.5: Confusion matrix and accuracy metrics.
</p>
</div>
<p>With the above process of calculation of the confusion matrix and the accuracy metrics based on the first iteration of the 5-fold CV with a given cut-off probability, we next will explain the logic of finding the optimal cut-off probability (also called hyperparameter) based on a set of given cut-off probabilities.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:unnamed-chunk-122"></span>
<img src="img06/w06-cut-off-prob-CV.jpg" alt="The pseudo-program of 5-fold CV for tuning hyperparameter - cut-off probability." width="80%" />
<p class="caption">
Figure 9.6: The pseudo-program of 5-fold CV for tuning hyperparameter - cut-off probability.
</p>
</div>
<p>From the above flow chart, we see that the performance measure reported by 5-fold cross-validation is the average of the values of accuracy computed in the loop. This approach can be computationally expensive but does not waste too much data (as is the case when fixing an arbitrary validation set), which is a major advantage in problems such as inverse inference where the number of samples is very small.</p>
</div>
<div id="other-cross-validation-methods" class="section level3 hasAnchor" number="9.3.2">
<h3><span class="header-section-number">9.3.2</span> Other Cross-Validation Methods<a href="method-of-cross-validation.html#other-cross-validation-methods" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>There are many other cross-validation methods defined using the same logic in k-fold cross-validation. We will not detail these CV methods. Instead, we describe some of them that are occasionally used in practice.</p>
<ul>
<li><p><strong>Leave-one-out (LOO)</strong> cross-validation is a simple cross-validation. Each training set is created by taking all the records (also called <strong>samples</strong> in machine learning) except one, the validating set being the record (also called <strong>sample</strong> in machine learning) left out. Therefore, this cross-validation procedure does not waste much data as only one sample is removed from the training set.</p></li>
<li><p><strong>Leave-P-Out</strong> is very similar to Leave-One-Out as it creates all the possible training/test sets by removing samples (i.e., records) from the complete set. Unlike Leave-One-Out and K-Fold, the test sets will overlap for <span class="math inline">\(p &gt; 1\)</span>.</p></li>
<li><p><strong>Stratified K-Fold</strong> is a variation of k-fold that returns stratified folds: each set contains approximately the same percentage of samples of each target class as the complete set.</p></li>
<li><p><strong>Leave-One-Group-Out</strong> is a cross-validation scheme that holds out the samples according to a third-party-provided array of integer groups. This group information can be used to encode arbitrary domain-specific pre-defined cross-validation folds. Each training set is thus constituted by all the samples except the ones related to a specific group.</p></li>
</ul>
</div>
</div>
<div id="case-study-using-fraud-data" class="section level2 hasAnchor" number="9.4">
<h2><span class="header-section-number">9.4</span> Case Study Using Fraud Data<a href="method-of-cross-validation.html#case-study-using-fraud-data" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>In this section, we use the fraud data at <a href="https://pengdsci.github.io/datasets/FraudIndex/fraudidx.csv" class="uri">https://pengdsci.github.io/datasets/FraudIndex/fraudidx.csv</a>. The data set has only two variables. The response variable is a binary fraud status variable with character values “good” and “bad”. The predictor variable is an index variable. The sample size is 33236. The fraud proportion is about 23.34%. The objective is to build a logistic regression model to predict fraud for future transactions.</p>
<div id="data-partition" class="section level3 hasAnchor" number="9.4.1">
<h3><span class="header-section-number">9.4.1</span> Data Partition<a href="method-of-cross-validation.html#data-partition" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Since the same size is large, we split the sample by 70%:30% with 70% data for training and validating models and 30% for testing purposes. The labels (value of the fraud status) of testing and validation data will be removed when calculating the accuracy measures.</p>
<div class="sourceCode" id="cb118"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb118-1"><a href="method-of-cross-validation.html#cb118-1" tabindex="-1"></a>fraud.data <span class="ot">=</span> <span class="fu">read.csv</span>(<span class="st">&quot;https://pengdsci.github.io/datasets/FraudIndex/fraudidx.csv&quot;</span>)[,<span class="sc">-</span><span class="dv">1</span>]</span>
<span id="cb118-2"><a href="method-of-cross-validation.html#cb118-2" tabindex="-1"></a><span class="do">## recode status variable: bad = 1 and good = 0</span></span>
<span id="cb118-3"><a href="method-of-cross-validation.html#cb118-3" tabindex="-1"></a>good.id <span class="ot">=</span> <span class="fu">which</span>(fraud.data<span class="sc">$</span>status <span class="sc">==</span> <span class="st">&quot; good&quot;</span>) </span>
<span id="cb118-4"><a href="method-of-cross-validation.html#cb118-4" tabindex="-1"></a>bad.id <span class="ot">=</span> <span class="fu">which</span>(fraud.data<span class="sc">$</span>status <span class="sc">==</span> <span class="st">&quot;fraud&quot;</span>)</span>
<span id="cb118-5"><a href="method-of-cross-validation.html#cb118-5" tabindex="-1"></a><span class="do">##</span></span>
<span id="cb118-6"><a href="method-of-cross-validation.html#cb118-6" tabindex="-1"></a>fraud.data<span class="sc">$</span>fraud.status <span class="ot">=</span> <span class="dv">0</span></span>
<span id="cb118-7"><a href="method-of-cross-validation.html#cb118-7" tabindex="-1"></a>fraud.data<span class="sc">$</span>fraud.status[bad.id] <span class="ot">=</span> <span class="dv">1</span></span>
<span id="cb118-8"><a href="method-of-cross-validation.html#cb118-8" tabindex="-1"></a>nn <span class="ot">=</span> <span class="fu">dim</span>(fraud.data)[<span class="dv">1</span>]</span>
<span id="cb118-9"><a href="method-of-cross-validation.html#cb118-9" tabindex="-1"></a>train.id <span class="ot">=</span> <span class="fu">sample</span>(<span class="dv">1</span><span class="sc">:</span>nn, <span class="fu">round</span>(nn<span class="sc">*</span><span class="fl">0.7</span>), <span class="at">replace =</span> <span class="cn">FALSE</span>) </span>
<span id="cb118-10"><a href="method-of-cross-validation.html#cb118-10" tabindex="-1"></a>training <span class="ot">=</span> fraud.data[train.id,]</span>
<span id="cb118-11"><a href="method-of-cross-validation.html#cb118-11" tabindex="-1"></a>testing <span class="ot">=</span> fraud.data[<span class="sc">-</span>train.id,]</span></code></pre></div>
</div>
<div id="finding-optimal-cut-off-probability" class="section level3 hasAnchor" number="9.4.2">
<h3><span class="header-section-number">9.4.2</span> Finding Optimal Cut-off Probability<a href="method-of-cross-validation.html#finding-optimal-cut-off-probability" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>We define a sequence of 20 candidate cut-off probabilities and then use 5-fold cross-validation to identify the optimal cut-off probability for the final detection model.</p>
<div class="sourceCode" id="cb119"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb119-1"><a href="method-of-cross-validation.html#cb119-1" tabindex="-1"></a>n0 <span class="ot">=</span> <span class="fu">dim</span>(training)[<span class="dv">1</span>]<span class="sc">/</span><span class="dv">5</span></span>
<span id="cb119-2"><a href="method-of-cross-validation.html#cb119-2" tabindex="-1"></a>cut<span class="fl">.0</span>ff.prob <span class="ot">=</span> <span class="fu">seq</span>(<span class="dv">0</span>,<span class="dv">1</span>, <span class="at">length =</span> <span class="dv">22</span>)[<span class="sc">-</span><span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">22</span>)]        <span class="co"># candidate cut off prob</span></span>
<span id="cb119-3"><a href="method-of-cross-validation.html#cb119-3" tabindex="-1"></a>pred.accuracy <span class="ot">=</span> <span class="fu">matrix</span>(<span class="dv">0</span>,<span class="at">ncol=</span><span class="dv">20</span>, <span class="at">nrow=</span><span class="dv">5</span>, <span class="at">byrow =</span> T)  <span class="co"># null vector for storing prediction accuracy</span></span>
<span id="cb119-4"><a href="method-of-cross-validation.html#cb119-4" tabindex="-1"></a><span class="do">## 5-fold CV</span></span>
<span id="cb119-5"><a href="method-of-cross-validation.html#cb119-5" tabindex="-1"></a><span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">5</span>){</span>
<span id="cb119-6"><a href="method-of-cross-validation.html#cb119-6" tabindex="-1"></a>  valid.id <span class="ot">=</span> ((i<span class="dv">-1</span>)<span class="sc">*</span>n0 <span class="sc">+</span> <span class="dv">1</span>)<span class="sc">:</span>(i<span class="sc">*</span>n0)</span>
<span id="cb119-7"><a href="method-of-cross-validation.html#cb119-7" tabindex="-1"></a>  valid.data <span class="ot">=</span> training[valid.id,]</span>
<span id="cb119-8"><a href="method-of-cross-validation.html#cb119-8" tabindex="-1"></a>  train.data <span class="ot">=</span> training[<span class="sc">-</span>valid.id,]</span>
<span id="cb119-9"><a href="method-of-cross-validation.html#cb119-9" tabindex="-1"></a>  train.model <span class="ot">=</span> <span class="fu">glm</span>(fraud.status <span class="sc">~</span> index, <span class="at">family =</span> <span class="fu">binomial</span>(<span class="at">link =</span> logit), <span class="at">data =</span> train.data)</span>
<span id="cb119-10"><a href="method-of-cross-validation.html#cb119-10" tabindex="-1"></a>  newdata <span class="ot">=</span> <span class="fu">data.frame</span>(<span class="at">index=</span> valid.data<span class="sc">$</span>index)</span>
<span id="cb119-11"><a href="method-of-cross-validation.html#cb119-11" tabindex="-1"></a>  pred.prob <span class="ot">=</span> <span class="fu">predict.glm</span>(train.model, newdata, <span class="at">type =</span> <span class="st">&quot;response&quot;</span>)</span>
<span id="cb119-12"><a href="method-of-cross-validation.html#cb119-12" tabindex="-1"></a>  <span class="co"># define confusion matrix and accuracy</span></span>
<span id="cb119-13"><a href="method-of-cross-validation.html#cb119-13" tabindex="-1"></a>  <span class="cf">for</span>(j <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">20</span>){</span>
<span id="cb119-14"><a href="method-of-cross-validation.html#cb119-14" tabindex="-1"></a>    pred.status <span class="ot">=</span> <span class="fu">rep</span>(<span class="dv">0</span>,<span class="fu">length</span>(pred.prob))</span>
<span id="cb119-15"><a href="method-of-cross-validation.html#cb119-15" tabindex="-1"></a>    valid.data<span class="sc">$</span>pred.status <span class="ot">=</span> <span class="fu">as.numeric</span>(pred.prob <span class="sc">&gt;</span>cut<span class="fl">.0</span>ff.prob[j])</span>
<span id="cb119-16"><a href="method-of-cross-validation.html#cb119-16" tabindex="-1"></a>    a11 <span class="ot">=</span> <span class="fu">sum</span>(valid.data<span class="sc">$</span>pred.status <span class="sc">==</span> valid.data<span class="sc">$</span>fraud.status)</span>
<span id="cb119-17"><a href="method-of-cross-validation.html#cb119-17" tabindex="-1"></a>    pred.accuracy[i,j] <span class="ot">=</span> a11<span class="sc">/</span><span class="fu">length</span>(pred.prob)</span>
<span id="cb119-18"><a href="method-of-cross-validation.html#cb119-18" tabindex="-1"></a>  }</span>
<span id="cb119-19"><a href="method-of-cross-validation.html#cb119-19" tabindex="-1"></a>}</span></code></pre></div>
<pre><code>## Warning: glm.fit:拟合機率算出来是数值零或一

## Warning: glm.fit:拟合機率算出来是数值零或一

## Warning: glm.fit:拟合機率算出来是数值零或一

## Warning: glm.fit:拟合機率算出来是数值零或一

## Warning: glm.fit:拟合機率算出来是数值零或一</code></pre>
<div class="sourceCode" id="cb121"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb121-1"><a href="method-of-cross-validation.html#cb121-1" tabindex="-1"></a><span class="do">###  </span></span>
<span id="cb121-2"><a href="method-of-cross-validation.html#cb121-2" tabindex="-1"></a>avg.accuracy <span class="ot">=</span> <span class="fu">apply</span>(pred.accuracy, <span class="dv">2</span>, mean)</span>
<span id="cb121-3"><a href="method-of-cross-validation.html#cb121-3" tabindex="-1"></a>max.id <span class="ot">=</span> <span class="fu">which</span>(avg.accuracy <span class="sc">==</span><span class="fu">max</span>(avg.accuracy ))</span>
<span id="cb121-4"><a href="method-of-cross-validation.html#cb121-4" tabindex="-1"></a><span class="do">### visual representation</span></span>
<span id="cb121-5"><a href="method-of-cross-validation.html#cb121-5" tabindex="-1"></a>tick.label <span class="ot">=</span> <span class="fu">as.character</span>(<span class="fu">round</span>(cut<span class="fl">.0</span>ff.prob,<span class="dv">2</span>))</span>
<span id="cb121-6"><a href="method-of-cross-validation.html#cb121-6" tabindex="-1"></a><span class="fu">plot</span>(<span class="dv">1</span><span class="sc">:</span><span class="dv">20</span>, avg.accuracy, <span class="at">type =</span> <span class="st">&quot;b&quot;</span>,</span>
<span id="cb121-7"><a href="method-of-cross-validation.html#cb121-7" tabindex="-1"></a>     <span class="at">xlim=</span><span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">20</span>), </span>
<span id="cb121-8"><a href="method-of-cross-validation.html#cb121-8" tabindex="-1"></a>     <span class="at">ylim=</span><span class="fu">c</span>(<span class="fl">0.5</span>,<span class="dv">1</span>), </span>
<span id="cb121-9"><a href="method-of-cross-validation.html#cb121-9" tabindex="-1"></a>     <span class="at">axes =</span> <span class="cn">FALSE</span>,</span>
<span id="cb121-10"><a href="method-of-cross-validation.html#cb121-10" tabindex="-1"></a>     <span class="at">xlab =</span> <span class="st">&quot;Cut-off Probability&quot;</span>,</span>
<span id="cb121-11"><a href="method-of-cross-validation.html#cb121-11" tabindex="-1"></a>     <span class="at">ylab =</span> <span class="st">&quot;Accuracy&quot;</span>,</span>
<span id="cb121-12"><a href="method-of-cross-validation.html#cb121-12" tabindex="-1"></a>     <span class="at">main =</span> <span class="st">&quot;5-fold CV performance&quot;</span></span>
<span id="cb121-13"><a href="method-of-cross-validation.html#cb121-13" tabindex="-1"></a>     )</span>
<span id="cb121-14"><a href="method-of-cross-validation.html#cb121-14" tabindex="-1"></a><span class="fu">axis</span>(<span class="dv">1</span>, <span class="at">at=</span><span class="dv">1</span><span class="sc">:</span><span class="dv">20</span>, <span class="at">label =</span> tick.label, <span class="at">las =</span> <span class="dv">2</span>)</span>
<span id="cb121-15"><a href="method-of-cross-validation.html#cb121-15" tabindex="-1"></a><span class="fu">axis</span>(<span class="dv">2</span>)</span>
<span id="cb121-16"><a href="method-of-cross-validation.html#cb121-16" tabindex="-1"></a><span class="fu">segments</span>(max.id, <span class="fl">0.5</span>, max.id, avg.accuracy[max.id], <span class="at">col =</span> <span class="st">&quot;red&quot;</span>)</span>
<span id="cb121-17"><a href="method-of-cross-validation.html#cb121-17" tabindex="-1"></a><span class="fu">text</span>(max.id, avg.accuracy[max.id]<span class="sc">+</span><span class="fl">0.03</span>, <span class="fu">as.character</span>(<span class="fu">round</span>(avg.accuracy[max.id],<span class="dv">4</span>)), <span class="at">col =</span> <span class="st">&quot;red&quot;</span>, <span class="at">cex =</span> <span class="fl">0.8</span>)</span></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:unnamed-chunk-124"></span>
<img src="STA551EB_files/figure-html/unnamed-chunk-124-1.png" alt="Figure 7. 5-fold CV performance plot" width="672" />
<p class="caption">
Figure 9.7: Figure 7. 5-fold CV performance plot
</p>
</div>
<p>The above figure indicates that the optimal cut-off probability that yields the best accuracy is 0.57.</p>
</div>
<div id="reporting-test" class="section level3 hasAnchor" number="9.4.3">
<h3><span class="header-section-number">9.4.3</span> Reporting Test<a href="method-of-cross-validation.html#reporting-test" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>This subsection reports the performance of the model using the test data set. Note that the model needs to be fit to the original training data to find the regression coefficients and then use the holdout testing sample to find the accuracy.</p>
<div class="sourceCode" id="cb122"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb122-1"><a href="method-of-cross-validation.html#cb122-1" tabindex="-1"></a>test.model <span class="ot">=</span> <span class="fu">glm</span>(fraud.status <span class="sc">~</span> index, <span class="at">family =</span> <span class="fu">binomial</span>(<span class="at">link =</span> logit), <span class="at">data =</span> training)</span></code></pre></div>
<pre><code>## Warning: glm.fit:拟合機率算出来是数值零或一</code></pre>
<div class="sourceCode" id="cb124"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb124-1"><a href="method-of-cross-validation.html#cb124-1" tabindex="-1"></a>newdata <span class="ot">=</span> <span class="fu">data.frame</span>(<span class="at">index=</span> testing<span class="sc">$</span>index)</span>
<span id="cb124-2"><a href="method-of-cross-validation.html#cb124-2" tabindex="-1"></a>pred.prob.test <span class="ot">=</span> <span class="fu">predict.glm</span>(test.model, newdata, <span class="at">type =</span> <span class="st">&quot;response&quot;</span>)</span>
<span id="cb124-3"><a href="method-of-cross-validation.html#cb124-3" tabindex="-1"></a>testing<span class="sc">$</span>test.status <span class="ot">=</span> <span class="fu">as.numeric</span>(pred.prob.test <span class="sc">&gt;</span> <span class="fl">0.57</span>)</span>
<span id="cb124-4"><a href="method-of-cross-validation.html#cb124-4" tabindex="-1"></a>a11 <span class="ot">=</span> <span class="fu">sum</span>(testing<span class="sc">$</span>test.status <span class="sc">==</span> testing<span class="sc">$</span>fraud.status)</span>
<span id="cb124-5"><a href="method-of-cross-validation.html#cb124-5" tabindex="-1"></a>test.accuracy <span class="ot">=</span> a11<span class="sc">/</span><span class="fu">length</span>(pred.prob.test)</span>
<span id="cb124-6"><a href="method-of-cross-validation.html#cb124-6" tabindex="-1"></a><span class="fu">kable</span>(<span class="fu">as.data.frame</span>(test.accuracy), <span class="at">align=</span><span class="st">&#39;c&#39;</span>)</span></code></pre></div>
<table>
<thead>
<tr class="header">
<th align="center">test.accuracy</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">0.9173603</td>
</tr>
</tbody>
</table>
<p>The accuracy is 92.18%. This accuracy indicates that there is no under-fitting. In models and algorithms with multiple feature variables, CV can help detect (and, hence, avoid )over and underfitting.</p>
</div>
</div>
<div id="concluding-remarks-1" class="section level2 hasAnchor" number="9.5">
<h2><span class="header-section-number">9.5</span> Concluding Remarks<a href="method-of-cross-validation.html#concluding-remarks-1" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>We have used 5-fold cross-validation to find the hyperparameter (optimal cut-off probability). This is only one application for estimating a hyperparameter. In practice, we can use cross-validation to select the best models in different model families. For example, we have candidate models such as decision tree-based models, support vector machines, neural networks, and logistic regression models. Cross-validation is a powerful tool for selecting the best model.</p>

<p><br />
</p>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="basics-of-bootstrap-method.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="perfomance-measures-of-algorithms-and-models.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/pengdsci/STA551EB/edit/master/08-TrainigValidationTesting.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["STA551EB.pdf", "STA551EB.epub"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
